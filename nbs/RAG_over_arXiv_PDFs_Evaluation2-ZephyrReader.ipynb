{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YErqpfH9jVI"
      },
      "source": [
        "# RAG Evaluation\n",
        "_Authored by: [Aymeric Roucher](https://huggingface.co/m-ric)_\n",
        "\n",
        "This notebook demonstrates how you can evaluate your RAG (Retrieval Augmented Generation), by building a synthetic evaluation dataset and using LLM-as-a-judge to compute the accuracy of your system.\n",
        "\n",
        "For an introduction to RAG, you can check [this other cookbook](rag_zephyr_langchain)!\n",
        "\n",
        "RAG systems are complex: here a RAG diagram, where we noted in blue all possibilities for system enhancement:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_workflow.png\" height=\"700\">\n",
        "\n",
        "Implementing any of these improvements can bring a huge performance boost; but changing anything is useless if you cannot monitor the impact of your changes on the system's performance!\n",
        "So let's see how to evaluate our RAG system.\n",
        "\n",
        "### Evaluating RAG performance\n",
        "\n",
        "Since there are so many moving parts to tune with a big impact on performance, benchmarking the RAG system is crucial.\n",
        "\n",
        "For our evaluation pipeline, we will need:\n",
        "1. An evaluation dataset with question - answer couples (QA couples)\n",
        "2. An evaluator to compute the accuracy of our system on the above evaluation dataset.\n",
        "\n",
        "â¡ï¸ It turns out, we can use LLMs to help us all along the way!\n",
        "1. The evaluation dataset will be synthetically generated by an LLM ğŸ¤–, and questions will be filtered out by other LLMs ğŸ¤–\n",
        "2. An [LLM-as-a-judge](https://huggingface.co/papers/2306.05685) agent ğŸ¤– will then perform the evaluation on this synthetic dataset.\n",
        "\n",
        "__Let's dig into it and start building our evaluation pipeline!__ First, we install the required model dependancies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCKBvOcp9jVK"
      },
      "outputs": [],
      "source": [
        "#!pip install -q torch transformers transformers langchain sentence-transformers tqdm openpyxl openai pandas datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_lJFbYm9jVL"
      },
      "outputs": [],
      "source": [
        "# %reload_ext autoreload\n",
        "# %autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oIlNZ1Mn9jVL"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "import json\n",
        "import datasets\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BOwG1Nuxtkj-"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5e696edc6214d3ba6e019223df53236",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeW8P62J9jVM"
      },
      "source": [
        "### Load your knowledge base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "YRbm5tNF9jVM"
      },
      "outputs": [],
      "source": [
        "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy9CKj0M9jVM"
      },
      "source": [
        "# 1. Build a synthetic dataset for evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkoEgiDg9jVM"
      },
      "source": [
        "### 1.1. Prepare source documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Drop REFERENCES onward, respecting previous discoveries that these just introduce noise to retrieval.  Must process new docs the same way!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "from pathlib import Path\n",
        "PDFS_PATH = Path('/home/mainuser/Desktop/LLMs/RagOverArXiv/clusterofstars')\n",
        "PDFS = list(PDFS_PATH.glob('*.pdf'))\n",
        "PDFS[0], len(PDFS)\n",
        "\n",
        "reader = PdfReader(os.path.expanduser(PDFS[0]))\n",
        "pages = reader.pages\n",
        "documents = []\n",
        "for page in pages:\n",
        "  documents.append(page.extract_text())\n",
        "\n",
        "\n",
        "def load_pdf_to_string(pdf_path):\n",
        "    # Open the PDF file in binary mode\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        # Create a PDF file reader object\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        # Initialize an empty string to hold the text\n",
        "        text = ''\n",
        "\n",
        "        # Loop through each page and extract the text\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            page_text = page.extract_text()\n",
        "            references_index= page_text.upper().find('\\nREFERENCES\\n')\n",
        "            if references_index != -1:\n",
        "              page_text = page_text[:references_index]\n",
        "              text += page_text\n",
        "              return text\n",
        "            text += page_text\n",
        "    return text\n",
        "\n",
        "# Use the function to load a PDF into a string\n",
        "text = load_pdf_to_string(os.path.expanduser(PDFS[0]))\n",
        "def get_title(pdf_path): return os.path.expanduser(pdf_path).split('/')[-1]\n",
        "\n",
        "all_docs_and_titles = [(load_pdf_to_string(os.path.expanduser(pdf_path)),get_title(pdf_path)) for pdf_path in PDFS]\n",
        "\n",
        "all_docs = [doc[0] for doc in all_docs_and_titles]\n",
        "all_titles = [doc[1] for doc in all_docs_and_titles]\n",
        "\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document \n",
        "\n",
        "# CHUNK_SIZE = 1000 #try 2000 next\n",
        "# CHUNK_OVERLAP = 30 #try 200 next\n",
        "CHUNK_SIZE = 2000 #try 2000 next\n",
        "CHUNK_OVERLAP = 200 #try 200 next\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap = CHUNK_OVERLAP,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "docs_processed  = [text_splitter.split_documents([Document(page_content=doc, metadata={'source':all_titles[idx]})]) \n",
        "         for idx,doc in enumerate(all_docs)]\n",
        "\n",
        "# text_splitter = RecursiveCharacterTextSplitter(\n",
        "#     chunk_size=2000,\n",
        "#     chunk_overlap=200,\n",
        "#     add_start_index=True,\n",
        "#     #separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        "# )\n",
        "\n",
        "# docs_processed = []\n",
        "# for idx,doc in enumerate(all_docs):\n",
        "#     docs_processed += text_splitter.split_documents([Document(page_content=doc, metadata={'source':all_titles[idx]})])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs_processed = [txt for doc in docs_processed for txt in doc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "700"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='CHAIN -OF-VERIFICATION REDUCES HALLUCINATION\\nINLARGE LANGUAGE MODELS\\nShehzaad Dhuliawala\\nMeta AI & ETH Z Â¨urichMojtaba Komeili\\nMeta AIJing Xu\\nMeta AIRoberta Raileanu\\nMeta AI\\nXian Li\\nMeta AIAsli Celikyilmaz\\nMeta AIJason Weston\\nMeta AI\\nABSTRACT\\nGeneration of plausible yet incorrect factual information, termed hallucination,\\nis an unsolved issue in large language models. We study the ability of language\\nmodels to deliberate on the responses they give in order to correct their mistakes.\\nWe develop the Chain-of-Verification (C OVE) method whereby the model first (i)\\ndrafts an initial response; then (ii) plans verification questions to fact-check its\\ndraft; (iii) answers those questions independently so the answers are not biased\\nby other responses; and (iv) generates its final verified response. In experiments,\\nwe show COVEdecreases hallucinations across a variety of tasks, from list-based\\nquestions from Wikidata, closed book MultiSpanQA and longform text generation.\\n1 I NTRODUCTION', metadata={'source': 'Chain-of-Verification Reduces Hallucination in Large Language Models.pdf'})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs_processed[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Make embeddings and save to vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "435ca99174dc4413b5c17603b3a4b72a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.faiss.FAISS at 0x70c28e0842f0>"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.embeddings import CacheBackedEmbeddings, HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.storage import LocalFileStore\n",
        "\n",
        "store = LocalFileStore(\"./cache/\")\n",
        "\n",
        "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "core_embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=embed_model_id\n",
        ")\n",
        "\n",
        "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    core_embeddings_model, store, namespace=embed_model_id\n",
        ")\n",
        "\n",
        "\n",
        "from langchain.schema.document import Document\n",
        "\n",
        "#docs = [Document(page_content=doc[i],metadata={'source':all_titles[j]}) for j,doc in enumerate(docs) for i in range(len(doc))]\n",
        "for index, pdf in enumerate(docs_processed):\n",
        "   content = docs_processed[index]\n",
        "   if index == 0:\n",
        "       vector_store = FAISS.from_documents([content], embedder)\n",
        "   else:\n",
        "      vector_store_i = FAISS.from_documents([content], embedder)\n",
        "      vector_store.merge_from(vector_store_i)\n",
        "\n",
        "vector_store\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TODO: Once have the pipeline working and have a baseline, look into https://huggingface.co/spaces/mteb/leaderboard.  SFR-Embedding-Mistral or something along those lines may work much better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_store.save_local('../rag_index_dir')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjrNhcCh9jVN"
      },
      "source": [
        "### 1.2. Setup agents for question generation\n",
        "\n",
        "- HF used [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) for QA couple generation because it it has excellent performance in leaderboards such as [Chatbot Arena](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard).  Used Mixtral-8x7b-4bit exl2, and it did not appear significantly better than Mistral, so using Mistral for speed but may come back to this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "GoRySj3Q9jVN",
        "outputId": "47dcbedc-a1fe-4b3e-82a4-5db3d49f2909"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import InferenceClient\n",
        "\n",
        "\n",
        "# repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "\n",
        "# llm_client = InferenceClient(\n",
        "#     model=repo_id,\n",
        "#     timeout=120,\n",
        "# )\n",
        "\n",
        "\n",
        "# def call_llm(inference_client: InferenceClient, prompt: str):\n",
        "#     response = inference_client.post(\n",
        "#         json={\n",
        "#             \"inputs\": prompt,\n",
        "#             \"parameters\": {\"max_new_tokens\": 1000},\n",
        "#             \"task\": \"text-generation\",\n",
        "#         },\n",
        "#     )\n",
        "#     return json.loads(response.decode())[0][\"generated_text\"]\n",
        "\n",
        "\n",
        "# call_llm(llm_client, \"This is a test context\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TODO: Tried Mixtral4Bit, visually perhaps a bit better, but overfits on 'deep question'. Perhaps return to this after looking at embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n"
          ]
        }
      ],
      "source": [
        "from exllamav2 import *\n",
        "from exllamav2.generator import *\n",
        "import sys, torch\n",
        "\n",
        "\n",
        "generator_config = ExLlamaV2Config()\n",
        "generator_config.model_dir = \"/home/mainuser/Desktop/LLMs/MiStralInference\"\n",
        "#generator_config.model_dir = '/home/mainuser/Desktop/LLMs/Mixtral4bit'\n",
        "generator_config.prepare()\n",
        "\n",
        "generator_model = ExLlamaV2(generator_config)\n",
        "cache = ExLlamaV2Cache(generator_model, lazy = True)\n",
        "\n",
        "print(\"Loading model...\")\n",
        "generator_model.load_autosplit(cache)\n",
        "\n",
        "generator_tokenizer = ExLlamaV2Tokenizer(generator_config)\n",
        "generator_llm = ExLlamaV2StreamingGenerator(generator_model, cache, generator_tokenizer)\n",
        "generator_llm.set_stop_conditions([generator_tokenizer.eos_token_id])\n",
        "generator_settings = ExLlamaV2Sampler.Settings()\n",
        "generator_settings.temperature = 0.85\n",
        "generator_settings.top_k = 50\n",
        "generator_settings.top_p = 0.8\n",
        "generator_settings.token_repetition_penalty = 1.01\n",
        "#generator_settings.disallow_tokens(generator_tokenizer, [generator_tokenizer.eos_token_id])\n",
        "# see if commenting out the above solved the endless generation issue (did not have with stream generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "#help(generator_llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "#help(generator_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "#help(generator_llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "#help(generator_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from transformers import Pipeline\n",
        "# from ragatouille import RAGPretrainedModel\n",
        "# from typing import Optional, List, Tuple\n",
        "# from langchain.docstore.document import Document\n",
        "# import time\n",
        "# # RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
        "\n",
        "# #     prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
        "\n",
        "# # )\n",
        "# RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "# from langchain.docstore.document import Document as LangchainDocument\n",
        "# def call_llm(\n",
        "#     question: str,\n",
        "#     generator_model: ExLlamaV2,\n",
        "#     generator_llm: ExLlamaV2StreamingGenerator,\n",
        "#     tokenizer: ExLlamaV2Tokenizer,\n",
        "#     settings:ExLlamaV2Sampler.Settings,\n",
        "#     max_new_tokens = 512\n",
        "# ) -> Tuple[str, List[LangchainDocument]]:\n",
        "\n",
        "\n",
        "#     instruction_ids = tokenizer.encode(f\"<s>[INST] {question} [/INST]\", add_bos = True)\n",
        "#     context_ids = instruction_ids if generator_llm.sequence_ids is None \\\n",
        "#             else torch.cat([generator_llm.sequence_ids, instruction_ids], dim = -1)\n",
        "\n",
        "#     max_new_tokens = max_new_tokens\n",
        "\n",
        "#     generator_llm.warmup()\n",
        "#     time_begin = time.time()\n",
        "\n",
        "#     output = generator_model.forward(f\"<s>[INST] {question} [/INST]\", settings, max_new_tokens)#, seed = 1234)#,add_eos=True)\n",
        "    \n",
        "#     time_end = time.time()\n",
        "#     time_total = time_end - time_begin\n",
        "\n",
        "#     print(output)\n",
        "#     print()\n",
        "#     print(f\"Response generated in {time_total:.2f} seconds, {max_new_tokens} tokens, {max_new_tokens / time_total:.2f} tokens/second\")\n",
        "#     return output\n",
        "# #     generator.begin_stream(context_ids, settings)\n",
        "\n",
        "# # #    return generator.generate_simple(context_ids, settings,num_tokens=512)\n",
        "\n",
        "# #     while True:\n",
        "# #         chunk, eos, _ = generator.stream()\n",
        "# #         if eos: break\n",
        "# #         print(chunk, end = \"\")\n",
        "# #         sys.stdout.flush()\n",
        "# #     #####\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # def call_llm(inference_client: InferenceClient, prompt: str):\n",
        "# #     response = inference_client.post(\n",
        "# #         json={\n",
        "# #             \"inputs\": prompt,\n",
        "# #             \"parameters\": {\"max_new_tokens\": 1000},\n",
        "# #             \"task\": \"text-generation\",\n",
        "# #         },\n",
        "# #     )\n",
        "# #     return json.loads(response.decode())[0][\"generated_text\"]\n",
        "\n",
        "\n",
        "# call_llm(question=\"How can I get my cat to like me?\", generator_model=generator_model,generator_llm=generator_llm,tokenizer=generator_tokenizer,settings=generator_settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<s>[INST] How can I get my cat to like me? [/INST] 1. Spend time with your cat: Cats enjoy spending time with their owners, so take some time out of your day to play with them or simply sit with them.\\n\\n2. Provide food and shelter: Ensure that your cat has access to good food and a comfortable place to sleep.\\n\\n3. Show affection: Cats love physical touch, so try petting them or giving them a gentle scratch behind their ears.\\n\\n4. Play with toys: Cats enjoy playing with toys, so try introducing them to some new ones.\\n\\n5. Be patient: Cats can be slow to warm up to new people, so be patient and give them time to get used to you.\\n\\n6. Use positive reinforcement: Reward your cat with treats or praise when they interact with you positively.\\n\\n7. Avoid loud noises: Cats can be easily startled by loud noises, so try to keep your voice and movements calm around them.\\n\\n8. Be consistent: Consistency is key when it comes to building a relationship with your cat. Try to spend the same amount of time with them every day and be consistent with your behavior.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Working except eos\n",
        "from transformers import Pipeline\n",
        "from ragatouille import RAGPretrainedModel\n",
        "from typing import Optional, List, Tuple\n",
        "from langchain.docstore.document import Document\n",
        "import time\n",
        "# RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
        "\n",
        "#     prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
        "\n",
        "# )\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "def call_llm(\n",
        "    question: str,\n",
        "    generator: ExLlamaV2StreamingGenerator,\n",
        "    tokenizer: ExLlamaV2Tokenizer,\n",
        "    settings:ExLlamaV2Sampler.Settings,\n",
        "    max_new_tokens = 512\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "\n",
        "\n",
        "    # instruction_ids = tokenizer.encode(f\"<s>[INST] {question} [/INST]\", add_bos = True)\n",
        "    # context_ids = instruction_ids if generator.sequence_ids is None \\\n",
        "    #         else torch.cat([generator.sequence_ids, instruction_ids], dim = -1)\n",
        "\n",
        "    max_new_tokens = max_new_tokens\n",
        "\n",
        "    generator.warmup()\n",
        "    #time_begin = time.time()\n",
        "    output = generator.generate_simple(f\"<s>[INST] {question} [/INST]\", settings, max_new_tokens, seed = 1234)\n",
        "\n",
        "    #output = generator.generate_simple(f\"<s>[INST] {question} [/INST]\", settings, max_new_tokens, seed = 1234)\n",
        "    #output = generator.generate_simple(question, settings, max_new_tokens, seed = 1234)\n",
        "\n",
        "    # time_end = time.time()\n",
        "    # time_total = time_end - time_begin\n",
        "\n",
        "    #print(output)\n",
        "    #print()\n",
        "    #print(f\"Response generated in {time_total:.2f} seconds, {max_new_tokens} tokens, {max_new_tokens / time_total:.2f} tokens/second\")\n",
        "    return output\n",
        "#     generator.begin_stream(context_ids, settings)\n",
        "\n",
        "# #    return generator.generate_simple(context_ids, settings,num_tokens=512)\n",
        "\n",
        "#     while True:\n",
        "#         chunk, eos, _ = generator.stream()\n",
        "#         if eos: break\n",
        "#         print(chunk, end = \"\")\n",
        "#         sys.stdout.flush()\n",
        "#     #####\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def call_llm(inference_client: InferenceClient, prompt: str):\n",
        "#     response = inference_client.post(\n",
        "#         json={\n",
        "#             \"inputs\": prompt,\n",
        "#             \"parameters\": {\"max_new_tokens\": 1000},\n",
        "#             \"task\": \"text-generation\",\n",
        "#         },\n",
        "#     )\n",
        "#     return json.loads(response.decode())[0][\"generated_text\"]\n",
        "\n",
        "\n",
        "call_llm(question=\"How can I get my cat to like me?\", generator=generator_llm,tokenizer=generator_tokenizer,settings=generator_settings,max_new_tokens=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "hIM_DJRo9jVN"
      },
      "outputs": [],
      "source": [
        "# QA_generation_prompt = \"\"\"\n",
        "# Your task is to write a factoid question and an answer given a context.\n",
        "# Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
        "# Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
        "# This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
        "\n",
        "# Provide your answer as follows:\n",
        "\n",
        "# Output:::\n",
        "# Factoid question: (your factoid question)\n",
        "# Answer: (your answer to the factoid question)\n",
        "\n",
        "# Now here is the context.\n",
        "\n",
        "# Context: {context}\\n\n",
        "# Output:::\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "QA_generation_prompt = \"\"\"\n",
        "Your task is to write a deep factual or conceptual question and an answer given a context.\n",
        "Your deep question should be unambigiously answerable from the context.\n",
        "Your deep question should be formulated in the same style as questions people reading advanced LLM papers would ask.\n",
        "This means that your question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Output:::\n",
        "Deep question: (your deep question)\n",
        "Answer: (your answer to the deep question)\n",
        "\n",
        "Now here is the context.\n",
        "\n",
        "Context: {context}\\n\n",
        "Output:::\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVFc-lVy9jVN"
      },
      "source": [
        "Now let's generate our QA couples.\n",
        "For this example, we generate only 10 QA couples and will load the rest from the Hub.\n",
        "\n",
        "But for your specific knowledge base, given that you want to get at least ~100 test samples, and accounting for the fact that we will filter out around half of these with our critique agents later on, you should generate much more, in the >200 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8fteqDDD9jVN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating 300 QA couples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [07:15<00:00,  1.45s/it]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "N_GENERATIONS = 300  # We intentionally generate only 10 QA couples here for cost and time considerations\n",
        "\n",
        "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
        "\n",
        "outputs = []\n",
        "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
        "    # Generate QA couple\n",
        "    # output_QA_couple = call_llm(\n",
        "    #     llm_client, QA_generation_prompt.format(context=sampled_context.page_content)\n",
        "    # )\n",
        "    output_QA_couple = call_llm(question=QA_generation_prompt.format(context=sampled_context.page_content), generator=generator_llm,tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "                                max_new_tokens=1024)\n",
        "    try:\n",
        "        question = output_QA_couple.split(\"Deep question: \")[-1].split(\"Answer: \")[0]\n",
        "        answer = output_QA_couple.split(\"Answer: \")[-1]\n",
        "        #assert len(answer) < 300, \"Answer is too long\"\n",
        "        outputs.append(\n",
        "            {\n",
        "                \"context\": sampled_context.page_content,\n",
        "                \"question\": question,\n",
        "                \"answer\": answer,\n",
        "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
        "            }\n",
        "        )\n",
        "    except:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'context': 'Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding\\nSKELETON -OF-THOUGHT : L ARGE LANGUAGE MOD-\\nELSCANDOPARALLEL DECODING\\nXuefei Ning1âˆ—\\nfoxdoraame@gmail.comZinan Lin2âˆ—\\nlinzinan1995@gmail.com\\nZixuan Zhou1âˆ—\\nzhouzx21@mails.tsinghua.edu.cnZifu Wang3\\nzifu.wang@kuleuven.be\\nHuazhong Yang1\\nyanghz@tsinghua.edu.cnYu Wang1\\nyu-wang@tsinghua.edu.cn\\n1Department of Electronic Engineering, Tsinghua University, Beijing, China\\n2Microsoft Research, Redmond, Washington, USA\\n3ESAT-PSI, KU Leuven, Leuven, Belgium\\nWebsite: https://sites.google.com/view/sot-llm\\nABSTRACT\\nThis work aims at decreasing the end-to-end generation latency of large language\\nmodels (LLMs). One of the major causes of the high generation latency is the\\nsequential decoding approach adopted by almost all state-of-the-art LLMs. In\\nthis work, motivated by the thinking and writing process of humans, we propose\\nSkeleton-of-Thought (SoT) , which first guides LLMs to generate the skeleton of\\nthe answer, and then conducts parallel API calls or batched decoding to com-\\nplete the contents of each skeleton point in parallel . Not only does SoT provide\\nconsiderable speed-ups across 12 LLMs, but it can also potentially improve the\\nanswer quality on several question categories. SoT is an initial attempt at data-\\ncentric optimization for inference efficiency, and further underscores the potential\\nof pushing LLMs to think more like a human for answer quality.\\n1 I NTRODUCTION\\nLarge language models (LLMs) (Brown et al., 2020; Touvron et al., 2023a; Du et al., 2022; OpenAI,\\n2023; Zheng et al., 2023) have shown exceptional performance in natural language processing and\\nchatbot systems. However, the inference process of the state-of-the-art LLMs is slow, hindering their\\ninteractive use. For example, it takes 22 seconds for Claude (Anthropic, 2023) (accessed through\\nSlack API) and 43 seconds for Vicuna-33B V1.3 (a 33B LLaMA-based model, running locally on\\none NVIDIA A100 GPU) to answer the question in Fig. 1.',\n",
              "  'question': 'Can a large language model with parallel decoding be designed to think more like a human for answer quality?\\n\\n',\n",
              "  'answer': 'Yes, the proposed Skeleton-of-Thought (SoT) method in the context is an attempt to optimize the inference efficiency of large language models (LLMs) for data-centric optimization, which can potentially improve the answer quality on several question categories. This is achieved by guiding the LLMs to generate the skeleton of the answer first and then conducting parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. The method is an initial attempt at data-centric optimization for inference efficiency and underscores the potential of pushing LLMs to think more like a human for answer quality.',\n",
              "  'source_doc': 'Skeleton-of-Thought_ Large Language Models Can Do Parallel Decoding.pdf'},\n",
              " {'context': 'scratchpad models on an dataset augmented with samples from the model signiï¬cantly improves\\nperformance for the scratchpad model, while it harms the direct execution model. Combining tracing\\ntraining data from several sources further improves scratchpad model performance.\\nDirect execution Scratchpad\\nMBPP MBPP-aug MBPP MBPP-aug MBPP-aug MBPP-aug MBPP-aug\\n+CodeNet +CodeNet +single line\\n+single line\\n(Â§5.2.1) (Â§5.2.2) (Â§5.2.1) (Â§5.2.2) (Â§5.3) (Â§5.3) (Â§5.3)\\nper-task execution acc: 10.3 5.1 5.1 17.3 26.6 25.2 23.4\\nper-task trace acc: n/a n/a 0.9 13.1 24.6 22.0 21.5\\nper-example execution acc: 22.0 12.3 24.6 35.5 46.0 45.3 43.5\\nper-example trace acc: n/a n/a 6.7 26.8 41.9 42.1 40.2\\nstate: b = 15; code: b = b // 2; output: b = 7;\\nstate: g = 100; i = 1; l = [100, 100, 0, 0, -100, -100];\\ncode: g += l[i]; output: g = 200; i = 1; l = [100, 100, 0, 0, -100, -100];\\nstate: s = /quotesingle.Varaabbcd /quotesingle.Var; code: o = set(s); output: o = { /quotesingle.Vara/quotesingle.Var,/quotesingle.Varb/quotesingle.Var,/quotesingle.Varc/quotesingle.Var,/quotesingle.Vard/quotesingle.Var}; s = /quotesingle.Varaabbcd /quotesingle.Var;\\nstate: f = 63; i = 11; j = 53; code: f = i Ë† j; output: f = 62; i = 11; j = 53;\\na, b, x = map(int, input().split())\\nifa // 2 < b:\\nifx % 1000 == 0:\\nprint (a*(x//1000))\\nelse :\\nif(x % 1000) / 500 > 1:\\nprint (min(a*(x//1000 + 1), a*(x//1000) + b*2))\\nelse :\\nprint (min(a*(x//1000 + 1), a*(x//1000) + b))\\nelse :\\nifx % 500 == 0:\\nprint (b*(x//500))\\nelse :\\nprint (b*(x//500 + 1))\\nFigure 6: Top: examples of single line data. Bottom: example CodeNet submission.\\nSingle-line programs This dataset consists of roughly 9 million examples of single-line Python\\ntransformations. Figure 6 (Top) shows examples of these transformations. Each transformation\\nconsists of an initial set of variables and corresponding values, a single line of Python (together\\nthese form the input), and the new set of variables and values which results from running the line',\n",
              "  'question': 'Does the dataset augmentation with samples from the model improve the performance of the scratchpad model, while it harms the direct execution model?\\n\\n',\n",
              "  'answer': 'Yes, the dataset augmentation with samples from the model improves the performance of the scratchpad model, while it harms the direct execution model.',\n",
              "  'source_doc': 'Show Your Work_  Scratchpads for Intermediate Computation with Language Models.pdf'},\n",
              " {'context': 'Results. As shown in Table 3, IO and CoT prompting methods perform poorly with a word-level\\nsuccess rate less than 16%, while ToT signiï¬cantly improves all metrics, achieving a word-level\\nsuccess rate of 60% and solving 4 out of 20 games. Such an improvement is not surprising, given IO\\nand CoT lack mechanisms to try different clues, make changes to decisions, or backtrack.\\nOracle and ablation studies. When outputting from the oracle best DFS state (instead of the\\nheuristically determined best state) per task, ToT performance is even higher and actually solves\\n7/20 games (Table 3, â€œ+best stateâ€), indicating our simple output heuristics can be readily improved.\\nInterestingly, sometimes when the crosswords game is actually solved, the state evaluator might still\\ndeem some words as â€œimpossibleâ€ and prune â€” possibly because 5\\x025crosswords by design have\\nsome rare or obselete words that GPT-4 cannot recognize2. Given the state evaluation as a pruning\\nheuristic is imperfect, we also explore ablating the pruning, and ï¬nd the performance generally worse\\n(Table 3, â€œ-pruneâ€). However, it could actually ï¬nd the correct solution for 4/20 games (though only\\noutputting 1 via heuristic), 3 of which are games ToT+pruning cannot solve within 100 steps. Thus,\\nbetter heuristics for DFS pruning are critical for problem solving in this case. Lastly, we conï¬rm the\\nimportance of backtracking by running an ablation that keeps ï¬lling the most promising clue for at\\nmost 20 steps, allowing overwrites. This is similar to a â€œgreedyâ€ BFS search with breadth limit of\\nb= 1, and performs poorly with a word level success of only 20% (Table 3, â€œ-backtrackâ€).\\n5 Related Work\\nPlanning and decision making. Smart planning and decision making are critical to achieving\\npredeï¬ned goals. As they are trained on vast amount of world knowledge and human examples, LMs\\nare known to have already absorbed rich commonsense that makes it possible to propose reasonable',\n",
              "  'question': 'In relation to the results presented in Table 3, what is the significance of the differences in word-level success rates between IO, CoT, Oracle, and ToT prompting methods, and how do these differences impact the performance of the models in solving crossword games?\\n\\n',\n",
              "  'answer': 'The results in Table 3 show that IO and CoT prompting methods have a word-level success rate of less than 16%, while ToT significantly improves all metrics, achieving a word-level success rate of 60% and solving 4 out of 20 games. This indicates that ToT has mechanisms in place to try different clues, make changes to decisions, and backtrack, which are lacking in IO and CoT. Additionally, when outputting from the oracle best DFS state, ToT performance is even higher and actually solves 7/20 games, indicating that the simple output heuristics used can be improved. The importance of backtracking is also confirmed by an ablation that keeps filling the most promising clue for at most 20 steps, which performs poorly with a word-level success rate of only 20%.',\n",
              "  'source_doc': 'Tree of Thoughts_  Deliberate Problem Solving with Large Language Models.pdf'},\n",
              " {'context': '\\x0f\\x03\\x13\\x11\\x14\\x0c@\\nY\\x16\\x11\\x03HORSH\\n0XOWLSOH\\x03UXQV3DUVH\\x0f\\x03ILOWHU\\x03RXW\\x03QRQ\\x10ILYH\\x10OHWWHU\\x0f\\x03VFRUH\\x0f\\x03DJJUHJDWH\\n&KRRVH\\x03\\x0bVRIW\\x03VHOI\\x10FRQVLVWHQF\\\\\"\\x0c\\x14\\x110D[\\x15\\x110D[\\x03ZLWKRXW\\x03YLRODWH\\x16\\x11\\')6\\nGÄ®Å”Å©ÆœË¤\\x1dÄ¦Å©Ã²ÅÄÉ¿Ê›Ä­ÄµÅ¤ÄµÅ—ÄÉ¾Ê›Æ˜ÃŠÅÄ£ÅÄÊÊ›ÅÃŠÄ¦ÄµÄ®Æ˜ÃŠÅÄ£ÅÆ˜ÃŠÅÄ£Å\\nÄÊÊ›Ë¤ÅÃŠÄ¦ÄµÄ®Ë¤Ê±ÅÅ©Å—Ã²Ê²Å¿Ê‚Ê›Ë¤ÅÅ—Ã­Å—Æ†Ë¤Ê±Ä¦ÄµÆ€Ê²Å¿Ê€Ê›Ë¤ÅÆœÅ—Ä‘Ä®ÄˆË¤Ê±ÄÆ“ÄˆÄÊ²ÊŸÊŸ7KRXJKW\\x033URSRVDOVÃŠÄˆÄˆÅ—Ã²ÄˆÃŠÅ¤Ã²Å¿Ê€Ê›Ë¤\\x89Å—Ã²Å¤Ã²Ä®ÆœÆ“ÄµÅ©ÅÊË¤ÆšÄ¦ÄµÆ€Ã²Å—Æ†ÊË¤ËˆËˆËˆËˆËˆË¤ÅÅ©Å—Ã²6WDWH\\x03(YDOXDWRU\\x03\\x0bRYHU\\x03HDFK\\x03FOXH\\x0cÅ¿É¾Ê›Ë¤Ã‰ÄµË¤ÄÃ²ÃŠÅ”ÊË¤Æ›Ä­ËˆÅËˆË¤Ê³Ê›Ê›Ê›Ê´Ë¤Ä‘Ä­Å”ÄµÅÅÄ‘Ã¦Ä¦Ã²Å¿Ê‚Ê›Ë¤#Ã²ÅÆ“Ã§Ã§ÃŠÅ¤ÄµÅ—ÊË¤Ä­ÄµÅ—Ã²Ë¤Ã­Å—Æ†ÊË¤ÅÅ—ËˆÄ®ËˆË¤Ê³Ê›Ê›Ê›Ê´Ë¤Ä­ÃŠÆ†Ã¦Ã²ÊŸÊŸÊ±Ã¦ÃŠÃ§Ä£ÆœÅ—ÃŠÃ§Ä£Ê²ÄÊ€Ê›ÄˆÅ—ÃŠÄ®Ã­ÊŸÊŸÊ±ÅÅ©Ã¦ÆœÅ—Ã²Ã²Ë¤Å”Å—Å©Ä®Ã²Ã­Ê²ÄÊÊ›Ë¤ÅÃŠÄ¦ÄµÄ®ÄÊ€Ê›Ë¤ÄˆÅ—ÃŠÄ®Ã­Å¿Ê€Ê›Ë¤ÅÆœÅ—Ä‘Ä®ÄˆÊŸÊŸ\\')6\\x032UGHUÄÊÊ›Ë¤ÅÃŠÄ¦ÄµÄ®Ë¤Ê±ÅÅ©Å—Ã²Ê²Å¿Ê‚Ê›Ë¤ÅÅ—Ã­Å—Æ†Ë¤Ê±Ä¦ÄµÆ€Ê²Å¿Ê€Ê›Ë¤ÅÆœÅ—Ä‘Ä®ÄˆË¤Ê±ÄÆ“ÄˆÄÊ²ÊŸÊŸ7KRXJKW\\x033URSRVDOVÄÊÊ›Ë¤ÅÃŠÄ¦ÄµÄ®Ë¤Ê±ÅÅ©Å—Ã²Ê²Å¿Ê‚Ê›Ë¤ÅÅ—Ã­Å—Æ†Ë¤Ê±Ä¦ÄµÆ€Ê²Å¿Ê€Ê›Ë¤ÅÆœÅ—Ä‘Ä®ÄˆË¤Ê±ÄÆ“ÄˆÄÊ²ÊŸÊŸ7KRXJKW\\x033URSRVDOVÄÊÊ›Ë¤ÅÃŠÄ¦ÄµÄ®Ë¤Ê±ÅÅ©Å—Ã²Ê²Å¿Ê‚Ê›Ë¤ÅÅ—Ã­Å—Æ†Ë¤Ê±Ä¦ÄµÆ€Ê²Å¿Ê€Ê›Ë¤ÅÆœÅ—Ä‘Ä®ÄˆË¤Ê±ÄÆ“ÄˆÄÊ²ÊŸÊŸ7KRXJKW\\x033URSRVDOVÄÊÊ›Ë¤ÅÃŠÄ¦ÄµÄ®Ë¤Ê±ÅÅ©Å—Ã²Ê²Å¿Ê‚Ê›Ë¤ÅÅ—Ã­Å—Æ†Ë¤Ê±Ä¦ÄµÆ€Ê²Å¿Ê€Ê›Ë¤ÅÆœÅ—Ä‘Ä®ÄˆË¤Ê±ÄÆ“ÄˆÄÊ²ÊŸÊŸ7KRXJKW\\x033URSRVDOVÊ±ÃŠÊ²Ê±Ã¦Ê²Æ›Ë¤ÃŠË¤ÅË¤Ä£Ë¤ÅÄ­Ë¤ÄµË¤Æ›Ë¤ÄµË¤Å—ËˆË¤ËˆË¤ËˆË¤ËˆË¤ËˆÅË¤ÃŠË¤Ä¦Ë¤ÄµË¤Ä®ËˆË¤ËˆË¤ËˆË¤ËˆË¤ËˆFigure 6: In Mini Crosswords, (a) how thoughts are proposed and aggregated in a priority queue\\nfor depth-ï¬rst search (DFS), and (b) how a state is evaluated based on the possibility of ï¬lling in\\neach remaining word clue, and pruned if any remaining clue is deemed not possible to ï¬ll by the LM.\\nThen DFS backtracks to the parent state and explore the next promising thought for clue.\\nthese across proposals to obtain a sorted list of next thoughts to explore (Figure 6(a)). For state\\nevaluations, we similarly translate each state into letter constraints for remaining clues, then evaluate\\nfor each clue if it is possible to ï¬ll given the constraints. If any remaining clue is deemed â€œimpossibleâ€\\nto ï¬ll in (e.g. â€œv1. To heap: tm sâ€), then the exploration of the stateâ€™s subtree is pruned and DFS\\nbacktracks to its parent to explore the next promising thought. We limit DFS search steps to 100, and\\nsimply render the deepest explored state (the ï¬rst explored one if multiple) into the ï¬nal output.\\nResults. As shown in Table 3, IO and CoT prompting methods perform poorly with a word-level\\nsuccess rate less than 16%, while ToT signiï¬cantly improves all metrics, achieving a word-level',\n",
              "  'question': 'Given the context, how does the Mini Crosswords algorithm propose and aggregate thoughts in a priority queue during a depth-first search (DFS) for solving crossword puzzles?\\n\\n',\n",
              "  'answer': 'The Mini Crosswords algorithm proposes and aggregates thoughts in a priority queue during a depth-first search (DFS) for solving crossword puzzles by first generating all possible thoughts for each given clue. These thoughts are then evaluated based on the possibility of filling in each remaining word clue, and pruned if any remaining clue is deemed not possible to fill by the LM. The remaining thoughts are then sorted in a priority queue based on their evaluation scores, and the algorithm explores the next promising thought for clue in the priority queue. This process continues until the deepest explored state is rendered into the final output.',\n",
              "  'source_doc': 'Tree of Thoughts_  Deliberate Problem Solving with Large Language Models.pdf'},\n",
              " {'context': '[20] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, and D. Song. Koala: A\\ndialogue model for academic research. Blog post, April 2023. URL https://bair.berkeley.\\nedu/blog/2023/04/03/koala/ .\\n[21] A. Glaese, N. McAleese, M. TrË› ebacz, J. Aslanides, V . Firoiu, T. Ewalds, M. Rauh, L. Weidinger,\\nM. Chadwick, P. Thacker, et al. Improving alignment of dialogue agents via targeted human\\njudgements. arXiv preprint arXiv:2209.14375 , 2022.\\n[22] S. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz, S. R. Bowman, and N. A. Smith.\\nAnnotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324 , 2018.\\n[23] J. Henderson, S. Ruder, et al. Compacter: Efficient low-rank hypercomplex adapter layers. In\\nAdvances in Neural Information Processing Systems , 2021.\\n[24] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Mea-\\nsuring massive multitask language understanding. In International Conference on Learning\\nRepresentations , 2020.\\n[25] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y . Choi. The curious case of neural text\\ndegeneration. In International Conference on Learning Representations , 2020.\\n[26] O. Honovich, T. Scialom, O. Levy, and T. Schick. Unnatural instructions: Tuning language\\nmodels with (almost) no human labor. arXiv preprint arXiv:2212.09689 , 2022.\\n[27] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. At-\\ntariyan, and S. Gelly. Parameter-efficient transfer learning for nlp. In International Conference\\non Machine Learning , pages 2790â€“2799. PMLR, 2019.\\n[28] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. Lora:\\nLow-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 , 2021.\\n[29] S. Iyer, X. V . Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S.\\nKoura, et al. Opt-iml: Scaling language model instruction meta learning through the lens of',\n",
              "  'question': 'How can dialogue models be improved to better align with human judgements?\\n\\n',\n",
              "  'answer': 'Dialogue models can be improved to better align with human judgements by incorporating targeted human judgements into the training process. This can be done through methods such as active learning and human-in-the-loop approaches. Additionally, improving the quality of the training data and using more advanced techniques such as transfer learning and low-rank hypercomplex adapter layers can also help improve alignment.',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': 'data?\\nLearn by analogy. Though many of the rationales gener-\\nated by TeacherLM are not always correct, in this paper,\\nno measures were taken to filter the augmented content,\\nand the student model still produces significant benefits.\\nThe correctness of the augmented content is not the only\\nimportant factor. The relevance of the content and the con-sistency of the reasoning logic are also significant (Wang\\net al., 2022a), which allows the student models to learn to\\nthink critically and truly enhance the modelâ€™s generalization\\nability on unseen tasks.\\nIn comparison to human annotation and text-davinci-\\n003, what are the characteristics and deficiencies of the\\ngeneration content of TeacherLM-7.1B?TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise\\nAs shown in Figure 4 and Appendix C, TeacherLM-7.1Bâ€™s\\nexplanations are generally more comprehensive and detailed\\nthan human annotations and text-davinci-003â€™s explanations.\\nHowever, it falls behind text-davinci-003 in solving mathe-\\nmatical problems, probably related to the modelâ€™s size.',\n",
              "  'question': 'In the context of TeacherLM-7.1B, what are the characteristics and deficiencies of the generation content compared to human annotation and text-davinci-003?\\n\\n',\n",
              "  'answer': \"TeacherLM-7.1B's explanations are generally more comprehensive and detailed than human annotations and text-davinci-003's explanations. However, it falls behind text-davinci-003 in solving mathematical problems, probably related to the model's size.\",\n",
              "  'source_doc': 'TeacherLM_ Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise.pdf'},\n",
              " {'context': 'Table 5: Validation accuracy on WikiSQL and MultiNLI after applying LoRA to different types of\\nattention weights in GPT-3, given the same number of trainable parameters. Adapting both Wqand\\nWvgives the best performance overall. We ï¬nd the standard deviation across random seeds to be\\nconsistent for a given dataset, which we report in the ï¬rst column.\\nNote that putting all the parameters in \\x01Wqor\\x01Wkresults in signiï¬cantly lower performance,\\nwhile adapting both WqandWvyields the best result. This suggests that even a rank of four\\ncaptures enough information in \\x01Wsuch that it is preferable to adapt more weight matrices than\\nadapting a single type of weights with a larger rank.\\n7.2 W HAT IS THE OPTIMAL RANKrFOR LORA?\\nWe turn our attention to the effect of rank ron model performance. We adapt fWq;Wvg,\\nfWq;Wk;Wv;Wcg, and justWqfor a comparison.\\nWeight Type r= 1r= 2r= 4r= 8r= 64\\nWikiSQL(\\x060:5%)Wq 68.8 69.6 70.5 70.4 70.0\\nWq;Wv 73.4 73.3 73.7 73.8 73.5\\nWq;Wk;Wv;Wo 74.1 73.7 74.0 74.0 73.9\\nMultiNLI (\\x060:1%)Wq 90.7 90.9 91.1 90.7 90.7\\nWq;Wv 91.3 91.4 91.3 91.6 91.4\\nWq;Wk;Wv;Wo 91.2 91.7 91.7 91.5 91.4\\nTable 6: Validation accuracy on WikiSQL and MultiNLI with different rank r. To our surprise, a\\nrank as small as one sufï¬ces for adapting both WqandWvon these datasets while training Wqalone\\nneeds a larger r. We conduct a similar experiment on GPT-2 in Section H.2.\\nTable 6 shows that, surprisingly, LoRA already performs competitively with a very small r(more\\nso forfWq;Wvgthan justWq). This suggests the update matrix \\x01Wcould have a very small\\nâ€œintrinsic rankâ€.6To further support this ï¬nding, we check the overlap of the subspaces learned by\\ndifferent choices of rand by different random seeds. We argue that increasing rdoes not cover a\\nmore meaningful subspace, which suggests that a low-rank adaptation matrix is sufï¬cient.\\n6However, we do not expect a small rto work for every task or dataset. Consider the following thought',\n",
              "  'question': 'What is the optimal rank for LoRA?\\n\\n',\n",
              "  'answer': 'The optimal rank for LoRA is not specified in the given context. However, the study found that a rank as small as one is sufficient for adapting both Wq and Wv on WikiSQL and MultiNLI datasets. The study also found that LoRA already performs competitively with a very small rank, suggesting that the update matrix \\x01W could have a very small \"intrinsic rank\". However, the study cautions that a small rank may not work for every task or dataset, and further research is needed to determine the optimal rank for LoRA.',\n",
              "  'source_doc': 'LoRA_ Low-Rank Adaptation of Large Language Models.pdf'},\n",
              " {'context': 'or rationale instead of just an output. This approach has been shown to elicit LLMsâ€™ strong reasoning\\ncapabilities on various kinds of tasks.\\n3Published in Transactions on Machine Learning Research (10/2023)\\nFigure 3: Left: Few-shot PoT prompting, Right: Zero-shot PoT prompting.\\n2.2 Program of Thoughts\\nBesides natural language, programs can also be used to express our thought processes. By using semantically\\nmeaningfulvariablenames,aprogramcanalsobeanaturalrepresentationtoconveyhumanthoughts. Forex-\\nample, inthelowerexampleinFigure1, wefirstcreateanunknownvariablenamed interest_rate . Thenwe\\nbind â€˜summation in two years with ... interest rateâ€™ to the variable sum_in_two_years_with_XXX_interest\\nand write down the equation expressing their mathematical relations with interest_rate . These equations\\nare packaged into the â€˜solveâ€™ function provided by â€˜SymPyâ€™. The program is executed with Python to solve\\nthe equations to derive the answer variable interest_rate .\\nUnlike CoT, PoT relegates some computation to an external process (a Python interpreter). The LLMs\\nare only responsible for expressing the â€˜reasoning processâ€™ in the programming language. In contrast, CoT\\naims to use LLMs to perform both reasoning and computation. We argue that such an approach is more\\nexpressive and accurate in terms of numerical reasoning.\\nThe â€˜program of thoughtsâ€™ is different from generating equations directly, where the generation target would\\nbesolve (20000 âˆ—(1 +x)3âˆ’2000âˆ’xâˆ—20000 âˆ—3âˆ’1000, x). As observed by Wei et al. (2022) for CoT, directly\\ngenerating such equations is challenging for LLMs. PoT differs from equation generation in two aspects: (1)\\nPoT breaks down the equation into a multi-step â€˜thoughtâ€™ process, and (2) PoT binds semantic meanings\\nto variables to help ground the model in language. We found that this sort of â€˜thoughtfulâ€™ process can\\nelicit language modelsâ€™ reasoning capabilities and generate more accurate programs. We provide a detailed',\n",
              "  'question': 'In contrast to CoT, PoT aims to use LLMs to perform both reasoning and computation. How does this approach differ from CoT in terms of expressiveness and accuracy in numerical reasoning?\\n\\n',\n",
              "  'answer': \"PoT relies on LLMs to express the reasoning process in a programming language, while CoT aims to use LLMs to perform both reasoning and computation. PoT differs from CoT in that it breaks down the equation into a multi-step thought process and binds semantic meanings to variables to help ground the model in language. This approach has been shown to elicit LLMs' reasoning capabilities and generate more accurate programs.\",\n",
              "  'source_doc': 'Program of Thoughts Prompting_  Disentangling Computation from Reasoning for Numerical Reasoning Tasks.pdf'},\n",
              " {'context': 'datasets. We will round the predicted number to a specific precision and then compare it with the reference\\nnumber. For the AQuA dataset, we use PoT to compute the intermediate answer and then prompt the\\nLLM again to output the closest option to measure the accuracy. For TabMWP, ConvFinQA, and TATQA\\ndatasets, we use the official evaluation scripts provided on Github. For FinQA, we relax the evaluation for\\nCoT because LLMs cannot perform the computation precisely (especially with high-precision floats and large\\nnumbers), so we adopt â€˜math.iscloseâ€™ with relative tolerance of 0.001 to compare answers.\\nBaselines WereportresultsforthreedifferentmodelsincludingCodex(Chenetal.,2021a), GPT-3(Brown\\net al., 2020), PaLM (Chowdhery et al., 2022) and LaMDA (Thoppilan et al., 2022). We consider two types\\nof prediction strategies including direct answer output and chain of thought to derive the answer. Since\\nPaLM API is not public, we only list PaLM results reported from previous work (Wei et al., 2022; Wang\\net al., 2022b). We also leverage an external calculator as suggested in Wei et al. (2022) for all the equations\\ngenerated by CoT, which is denoted as CoT + calc. Besides greedy decoding, we use self-consistency (Wang\\net al., 2022b) with CoT, taking the majority vote over 40 different completions as the prediction.\\n3.2 Main Results\\nFew-shot Results We give our few-shot results in Table 2. On MWP datasets, PoT with greedy decoding\\nimproves on GSM8K/AQuA/TabMWP by more than 8%. On SVAMP, the improvement is 4% mainly due\\nto its simplicity. For financial QA datasets, PoT improves over CoT by roughly 20% on FinQA/ConvFinQA\\nand 8% on TATQA. The larger improvements in FinQA and ConvFinQA are mainly due to miscalculations\\non LLMs for large numbers (e.g. in the millions). CoT adopts LLMs to perform the computation, which\\nis highly prone to miscalculation errors, while PoT adopts a highly precise external computer to solve the',\n",
              "  'question': 'In the context given, what is the evaluation method used for the TabMWP, ConvFinQA, and TATQA datasets?\\n\\n',\n",
              "  'answer': 'The evaluation method used for the TabMWP, ConvFinQA, and TATQA datasets is to use the official evaluation scripts provided on Github.',\n",
              "  'source_doc': 'Program of Thoughts Prompting_  Disentangling Computation from Reasoning for Numerical Reasoning Tasks.pdf'},\n",
              " {'context': 'Therefore we use a suite of more speciï¬c proxy criteria that aim to capture different aspects of\\nbehavior in a deployed model that could end up being harmful: we have labelers evaluate whether an\\noutput is inappropriate in the context of a customer assistant, denigrates a protected class, or contains\\nsexual or violent content. We also benchmark our model on datasets intended to measure bias and\\ntoxicity, such as RealToxicityPrompts (Gehman et al., 2020) and CrowS-Pairs (Nangia et al., 2020).\\nTo summarize, we can divide our quantitative evaluations into two separate parts:\\nEvaluations on API distribution. Our main metric is human preference ratings on a held out set\\nof prompts from the same source as our training distribution. When using prompts from the API for\\nevaluation, we only select prompts by customers we havenâ€™t included in training. However, given\\nthat our training prompts are designed to be used with InstructGPT models, itâ€™s likely that they\\ndisadvantage the GPT-3 baselines. Thus, we also evaluate on prompts submitted to GPT-3 models\\non the API; these prompts are generally not in an â€˜instruction followingâ€™ style, but are designed\\nspeciï¬cally for GPT-3. In both cases, for each model we calculate how often its outputs are preferred\\nto a baseline policy; we choose our 175B SFT model as the baseline since its performance is near the\\nmiddle of the pack. Additionally, we ask labelers to judge the overall quality of each response on a\\n1-7 Likert scale and collect a range of metadata for each model output (see Table 3).\\nEvaluations on public NLP datasets. We evaluate on two types of public datasets: those that\\ncapture an aspect of language model safety, particularly truthfulness, toxicity, and bias, and those that\\ncapture zero-shot performance on traditional NLP tasks like question answering, reading comprehen-\\nsion, and summarization. We also conduct human evaluations of toxicity on the RealToxicityPrompts',\n",
              "  'question': 'How does the evaluation of language models on public NLP datasets differ from the evaluation of language models on API distribution?\\n\\n',\n",
              "  'answer': 'The evaluation of language models on public NLP datasets focuses on aspects of language model safety such as truthfulness, toxicity, and bias, as well as zero-shot performance on traditional NLP tasks like question answering, reading comprehension, and summarization. On the other hand, the evaluation of language models on API distribution involves human preference ratings on a held-out set of prompts from the same source as the training distribution, as well as evaluations on prompts submitted to GPT-3 models on the API. The latter evaluations are designed specifically for GPT-3 and aim to capture different aspects of behavior in a deployed model that could end up being harmful.',\n",
              "  'source_doc': 'Training language models to follow instructions with human feedback.pdf'},\n",
              " {'context': 'Due to time constraints and the additional complexity of synchronizing separate processes, we did\\nnot implement this additional distributed training approach.\\nH Additional Related Work\\nH.1 Regularization Terms\\nDue to the disadvantages of the KL-divergence discussed above, several additional training objectives\\nhave been proposed which take into account the modelâ€™s generated sequences. Particularly popular is\\n19the (sequence-level) unlikelihood loss [37]. At step t, this is given by\\nLt\\nULS(PÎ¸(Â·|x1:t)) =Est+Nâˆ¼PÎ¸(Â·|st)\\uf8ee\\n\\uf8f0âˆ’t+NX\\nk=t+1X\\ncâˆˆCkPÎ¸(c|sk)\\uf8f9\\n\\uf8fb,\\nwhere Ckis a set of problematic outputs, such as repeated n-grams. The loss considers an N-step\\nsequence generated from the model from stand penalizes repetitions. Although this regularization\\nterm performs well in practice, it relies on a heuristic that repetitive text is unnatural, which does not\\nnecessarily hold universally.\\nHowever, the idea of sampling a sequence from the model and including that sequence in the loss is\\nincorporated in SequenceMatch.\\nI Examples\\nIn this section we give the first few samples from the evaluation set, with no cherry-picking, for the\\n1024-context dataset and the SequenceMatch and MLE objectives. Note that we strip newlines in\\nthe examples and generations in order to present them on one page. At one point an unprintable\\ncharacter (btyes U+FFFD) was produced, which we write as (U+FFFD). We have two settings for the\\nexamples.\\nIn the first setting, we take the prompt sequence and change the last token to a random token\\ndrawn from the dataset. We see that the SequenceMatch model is able to utilize the <backspace>\\ntoken immediately to remove the spurious token. The only case where this does not happen is for\\nthe prompt ... Clements disagreed particula , where the random token does make some\\nsemantic sense. In comparison, the MLE model cannot use the backspace and has a tendency to\\ndegenerate after a random token (particularly evident in the Toyota example).',\n",
              "  'question': \"In addition to the sequence-level unlikelihood loss, what other regularization terms have been proposed to take into account the model's generated sequences?\\n\\n\",\n",
              "  'answer': \"Several additional training objectives have been proposed which take into account the model's generated sequences. Some of these include the (sequence-level) unlikelihood loss [37], which penalizes repetitions in a sequence generated from the model, and the idea of sampling a sequence from the model and including that sequence in the loss, which is incorporated in SequenceMatch.\",\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'Theory and practice. arXiv preprint arXiv:2212.14578 , 2022.\\n[25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\\nLanguage models are unsupervised multitask learners, 2018.\\n[26] Douglas Rizzolo and Francis Edward Su. A fixed point theorem for the infinite-dimensional\\nsimplex. Journal of mathematical analysis and applications , 332(2):1063â€“1070, 2007.\\n[27] StÂ´ephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and\\nstructured prediction to no-regret online learning. In AISTATS , pages 627â€“635, 2011.\\n[28] Zhan Shi, Xinchi Chen, Xipeng Qiu, and Xuanjing Huang. Toward Diverse Text Generation\\nwith Inverse Reinforcement Learning. arXiv:1804.11258 [cs, stat] , June 2018.\\n[29] Maurice Sion. On general minimax theorems. Pacific Journal of Mathematics , 8(4):171â€“176,\\n1958.\\n[30] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data\\ndistribution. In Advances in Neural Information Processing Systems , volume 32, 2019.\\n[31] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon,\\nand Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equations.\\narXiv:2011.13456 [cs, stat] , February 2021.\\n[32] Gokul Swamy, Sanjiban Choudhury, Zhiwei Steven Wu, and J. Andrew Bagnell. Of Moments\\nand Matching: Trade-offs and Treatments in Imitation Learning. arXiv:2103.03236 [cs, stat] ,\\nMarch 2021.\\n[33] Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear\\nprogramming. In Proceedings of the 25th International Conference on Machine Learning , pages\\n1032â€“1039, 2008.\\n[34] Shichang Tang. Lessons Learned from the Training of GANs on Artificial Datasets.\\narXiv:2007.06418 [cs, stat] , July 2020.\\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-\\ntion Processing Systems , volume 30, 2017.',\n",
              "  'question': 'In unsupervised multitask learning, how does the use of language models improve task performance?\\n\\n',\n",
              "  'answer': 'The use of language models in unsupervised multitask learning improves task performance by allowing the model to learn a shared representation of the tasks. This shared representation captures common patterns and relationships between the tasks, enabling the model to perform well on all of them. Additionally, language models are pre-trained on large amounts of text data, which provides the model with a rich set of features to learn from. This pre-training helps the model to better understand the underlying structure of the tasks, further improving its performance.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'Contriever - 84.0 33.6 36.4\\nBERT ms marco 80.9 33.2 30.9\\nContriever ms marco 84.8 35.8 38.1\\nNext, we report nDCG@10 on the BEIR benchmark for diï¬€erent retrievers trained on MS MARCO in Table 2\\n(recall@100 can be found in Table 10 of appendix). We individually report results on each dataset as well as\\nthe average over 14 datasets of the BEIR Benchmark (excluding 3 for license reasons). We observe that when\\nused as pre-training, contrastive learning leads to strong performance: contriever obtains the best results\\namong dense bi-encoder methods for the nDCG@10, and is state-of-the-art for the recall@100 (improving the\\naverage recall@100 from 65.0 to 67.1). This strong recall@100 performance can be further exploited by using\\na cross-encoder2to re-rank the retrieved documents: this leads to the state-of-the-art on 8 datasets of the\\nBEIR benchmark for the nDCG@10, as well as on average. It should be noted that our ï¬ne-tuning procedure\\non MS MARCO is simpler than for other retrievers, as we use a simple strategy for negative mining and do\\nnot use distillation. Our model would probably also beneï¬ts from improvements proposed by these retrievers,\\nbut this is beyond the scope of this paper.\\nFinally, we illustrate the beneï¬t of our retriever compared to BM25 in a few-shot setting, where we have\\naccess to a small number of in-domain retrieval examples. This setting is common in practice, and lexical\\nbased methods, like BM25, cannot leverage the small training sets to adapt its weights. In Table 3, we report\\nnDCG@10 on three datasets from BEIR associated with the smallest training sets, ranging from 729 to\\n2We use the existing ms-marco-MiniLM-L-6-v2 cross-encoder model to perform the re-ranking.\\n8Published in Transactions on Machine Learning Research (08/2022)\\n5,500 queries. We observe that on these small datasets, our pre-training leads to better results than BERT\\npre-training, even when BERT is ï¬ne-tuned on MS MARCO as an intermediate step. Our pre-trained model',\n",
              "  'question': 'How does contrastive learning improve the performance of the Contriever model on the BEIR benchmark for the nDCG@10 and recall@100?\\n\\n',\n",
              "  'answer': 'Contrastive learning improves the performance of the Contriever model on the BEIR benchmark for the nDCG@10 and recall@100 by allowing the model to learn more complex relationships between query and document embeddings. This leads to better retrieval results, with the Contriever model achieving state-of-the-art performance on 8 datasets of the BEIR benchmark for the nDCG@10, as well as on average. Additionally, the use of a cross-encoder to re-rank the retrieved documents further enhances the performance of the Contriever model, leading to state-of-the-art results on 8 datasets of the BEIR benchmark for the nDCG@10.',\n",
              "  'source_doc': 'Unsupervised Dense Information Retrieval with Contrastive Learning.pdf'},\n",
              " {'context': 'and reinforcement learning instead of prompting.\\nCombining Internal and External Knowledge As will be detail in Section 3.3, we observe that\\nthe problem solving process demonstrated by ReAct is more factual and grounded, whereas CoT\\nis more accurate in formulating reasoning structure but can easily suffer from hallucinated facts\\nor thoughts. We therefore propose to incorporate ReAct andCoT-SC , and let the model decide\\nwhen to switch to the other method based on the following heuristics: A) ReAct!CoT-SC : when\\nReAct fails to return an answer within given steps, back off to CoT-SC . We set 7 and 5 steps for\\nHotpotQA and FEVER respectively as we ï¬nd more steps will not improve ReAct performance3.\\nB)CoT-SC!ReAct : when the majority answer among nCoT-SC samples occurs less than n=2\\ntimes (i.e. internal knowledge might not support the task conï¬dently), back off to ReAct .\\nFinetuning Due to the challenge of manually annotating reasoning traces and actions at scale,\\nwe consider a bootstraping approach similar to Zelikman et al. (2022), using 3,000 trajectories\\nwith correct answers generated by ReAct (also for other baselines) to ï¬netune smaller language\\nmodels (PaLM-8/62B) to decode trajectories (all thoughts, actions, observations) conditioned on\\ninput questions/claims. More details are in Appendix B.1.\\n3.3 R ESULTS AND OBSERVATIONS\\nReAct outperforms Act consistently Table 1 shows HotpotQA and Fever results using PaLM-\\n540B as the base model with different prompting methods. We note that ReAct is better than Act\\non both tasks, demonstrating the value of reasoning to guide acting, especially for synthesizing the\\nï¬nal answer, as shown in Figure 1 (1c-d). Fine-tuning results 3 also conï¬rm the beneï¬t of reasoning\\ntraces for more informed acting.\\n3Of all trajectories with correct ï¬nal answers, those with 7 steps on HotpotQA and 5 steps on FEVER only\\ntake up 0.84% and 1.33% respectively.\\n5Published as a conference paper at ICLR 2023\\nType Deï¬nition ReAct CoT',\n",
              "  'question': 'Can incorporating ReAct and CoT-SC improve the problem solving process in reasoning structure?\\n\\n',\n",
              "  'answer': 'Yes, incorporating ReAct and CoT-SC can improve the problem solving process in reasoning structure as demonstrated by the results in the paper. The ReAct method is better at formulating reasoning structure, while CoT-SC is more accurate in formulating reasoning structure. By combining these methods and using heuristics to switch between them, the model can achieve better performance on tasks such as HotpotQA and FEVER. Fine-tuning the model using reasoning traces and actions also further improves the performance.',\n",
              "  'source_doc': 'ReAct_ Synergizing Reasoning and Acting in Language Models.pdf'},\n",
              " {'context': 'brain float LoRA baseline by about 1 percentage point. This corroborates both our findings that (1)\\nQLORAwith NF4 replicates both 16-bit full finetuning and 16-bit LoRA finetuning performance,\\nand (2) NF4 is superior to FP4 in terms of quantization precision.\\nSummary Our results consistently show that 4-bit QLORAwith NF4 data type matches 16-\\nbit full finetuning and 16-bit LoRA finetuning performance on academic benchmarks with well-\\nestablished evaluation setups. We have also shown that NF4 is more effective than FP4 and that\\ndouble quantization does not degrade performance. Combined, this forms compelling evidence that\\n4-bit QL ORA tuning reliably yields results matching 16-bit methods.\\nIn line with previous work on quantization [ 13], our MMLU and Elo results indicate that with a given\\nfinetuning and inference resource budget it is beneficial to increase the number of parameters in the\\nbase model while decreasing their precision. This highlights the importance of efficiency benefits\\nfrom QLORA. Since we did not observe performance degradation compared to full-finetuning in\\nour experiments with 4-bit finetuning, this raises the question of where the performance-precision\\ntrade-off exactly lies for QLoRA tuning, which we leave to future work to explore.\\nWe proceed to investigate instruction tuning at scales that would be impossible to explore with full\\n16-bit finetuning on academic research hardware.\\n5 Pushing the Chatbot State-of-the-art with QLoRA\\nHaving established that 4-bit QLORAmatches 16-bit performance across scales, tasks, and datasets\\nwe conduct an in-depth study of instruction finetuning up to the largest open-source language models\\navailable for research. To assess the performance of instruction finetuning these models, we evaluate\\n7Table 4: Mean 5-shot MMLU test accuracy for LLaMA 7-65B models finetuned with adapters on Alpaca and\\nFLAN v2 for different data types. Overall, NF4 with double quantization (DQ) matches BFloat16 performance,',\n",
              "  'question': 'Given a model that has been finetuned with QLORA using the NF4 data type, what is the performance of the model compared to a model that has been finetuned with full 16-bit precision?\\n\\n',\n",
              "  'answer': 'The performance of the model finetuned with QLORA using the NF4 data type is comparable to the performance of a model finetuned with full 16-bit precision.',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': 'few-shot prompting strategy, LLMs are able to per-\\nform much better in complex reasoning problems.\\nSubsequently, many works (Wang et al., 2022a;\\nSuzgun et al., 2022; Shaikh et al., 2022; Saparov\\nand He, 2022) propose to further improve CoT\\nprompting in different aspects, including prompt\\nformat (Chen et al., 2022), prompt selection (Lu\\net al., 2022), prompt ensemble (Wang et al., 2022b;\\nLi et al., 2022; Weng et al., 2022; Fu et al., 2022),\\nproblem decomposition (Zhou et al., 2022; Khot\\net al., 2022; Dua et al., 2022; Press et al., 2022),\\nand planning (Yao et al., 2022; Huang et al., 2022;\\nWang et al., 2023; Liu et al., 2023; Sun et al., 2023;\\nYao et al., 2023). Chen et al. (2022) introduced\\nPoT prompting to use LLMs with code pre-training\\nto write a program as a rationale for disentangling\\ncomputation from reasoning. To do away with man-\\nual effort, Kojima et al. (2022) proposed Zero-shot-\\nCoT to elicit reasoning step generation without\\nexemplars. To leverage the benefit of demonstra-\\ntion examples and minimize manual effort, Zhang\\net al. (2022) designed Auto-CoT. It first automat-\\nically obtains kexamples by clustering the given\\ndataset. It then follows Zero-shot-CoT to gener-\\nate rationales for the selected examples. Finally,\\ndemonstration examples are constructed by adding\\nthe generated rationales to selected examples as\\nCoT prompts. Our work is different from the above\\nworks by focusing on eliciting multi-step reasoning\\nby LLMs in a zero-shot approach. We ask LLMs\\nto write a plan to decompose a complex reasoning\\ntask into multiple reasoning steps. Furthermore,\\nwe introduce detailed instructions to the prompt to\\navoid obvious errors in the reasoning steps. We re-\\nfer readers to the survey (Huang and Chang, 2022)\\nfor more related works.\\n6 Conclusion\\nIn this paper, we find that Zero-shot-CoT still suf-\\nfers from three pitfalls: calculation errors, missing-\\nreasoning-step errors, and semantic understand-\\ning errors. To address these issues, we introduce',\n",
              "  'question': 'What pitfalls does Zero-shot-CoT suffer from and how can they be addressed?\\n\\n',\n",
              "  'answer': 'Zero-shot-CoT suffers from three pitfalls: calculation errors, missing-reasoning-step errors, and semantic understanding errors. To address these issues, the authors introduce detailed instructions to the prompt to avoid obvious errors in the reasoning steps, and propose an approach called Multi-step Reasoning with Instructions (MSI) that elicits multi-step reasoning by LLMs in a zero-shot approach. MSI refers to the survey (Huang and Chang, 2022) for more related works.',\n",
              "  'source_doc': 'Plan-and-Solve Prompting_  Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models.pdf'},\n",
              " {'context': 'readable. Full details in Appendix A.\\nQLoRA-AllQLoRA-FFN\\nQLoRA-AttentionAlpaca (ours)\\nStanford-Alpaca\\nModel6061626364RougeL\\nbits\\n4\\n16\\nFigure 2: RougeL for LLaMA 7B models on the\\nAlpaca dataset. Each point represents a run with a\\ndifferent random seed. We improve on the Stanford\\nAlpaca fully finetuned default hyperparameters to\\nconstruct a strong 16-bit baseline for comparisons.\\nUsing LoRA on all transformer layers is critical to\\nmatch 16-bit performance.While paged optimizers are critical to do 33B/65B\\nQLORAtuning on a single 24/48GB GPU, we do\\nnot provide hard measurements for Paged Optimiz-\\ners since the paging only occurs when processing\\nmini-batches with long sequence lengths, which is\\nrare. We do, however, perform an analysis of the\\nruntime of paged optimizers for 65B models on\\n48GB GPUs and find that with a batch size of 16,\\npaged optimizers provide the same training speed\\nas regular optimizers. Future work should measure\\nand characterize under what circumstances slow-\\ndowns occur from the paging process.\\nDefault LoRA hyperparameters do not match 16-\\nbit performance When using the standard prac-\\ntice of applying LoRA to query and value attention\\nprojection matrices [ 28], we are not able to replicate\\nfull finetuning performance for large base models.\\nAs shown in Figure 2 for LLaMA 7B finetuning on\\nAlpaca, we find that the most critical LoRA hyper-\\nparameter is how many LoRA adapters are used in\\ntotal and that LoRA on all linear transformer block\\nlayers are required to match full finetuning perfor-\\nmance. Other LoRA hyperparameters, such as the\\nprojection dimension r, do not affect performance (see Appendix A).\\n1010\\n1011\\nT otal model bits\\n0.60\\n0.61\\n0.62\\n0.63\\n0.64\\n0.65\\n0.66\\n0.67Mean zeroshot accuracy\\n4-bit LLaMA\\nFloat\\nNFloat\\nNFloat + DQData type\\nFigure 3: Mean zero-shot accuracy over Wino-\\ngrande, HellaSwag, PiQA, Arc-Easy, and Arc-\\nChallenge using LLaMA models with different 4-bit\\ndata types. The NormalFloat data type significantly',\n",
              "  'question': 'Given that the context discusses the performance of LoRA on LLaMA models, specifically on the Alpaca dataset, and the importance of using LoRA on all transformer layers, what is the optimal number of LoRA adapters to use in total for a 16-bit LLaMA model?\\n\\n',\n",
              "  'answer': 'The most critical LoRA hyperparameter is the number of LoRA adapters used in total. For a 16-bit LLaMA model, the optimal number of LoRA adapters is the number of linear transformer block layers, which is critical to match full finetuning performance.',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': '2021b; Honovich et al., 2022; Wang et al., 2022):\\nGiven just a handful of human-written examples\\nof how an API can be used, we let a LM annotate\\na huge language modeling dataset with potential\\nAPI calls. We then use a self-supervised loss to\\ndetermine which of these API calls actually help\\nthe model in predicting future tokens. Finally, we\\nï¬netune the LM itself on the API calls that it con-\\nsiders useful. As illustrated in Figure 1, through\\nthis simple approach, LMs can learn to control a va-\\nriety of tools, and to choose for themselves which\\ntool to use when and how.\\nAs our approach is agnostic of the dataset be-\\ning used, we can apply it to the exact same dataset\\nthat was used to pretrain a model in the ï¬rst place.\\nThis ensures that the model does not lose any\\nof its generality and language modeling abilities.\\nWe conduct experiments on a variety of differ-\\nent downstream tasks, demonstrating that after\\nlearning to use tools, Toolformer, which is based\\non a pretrained GPT-J model (Wang and Komat-\\nsuzaki, 2021) with 6.7B parameters, achieves much\\nstronger zero-shot results, clearly outperforming a\\nmuch larger GPT-3 model (Brown et al., 2020) andseveral other baselines on various tasks.\\n2 Approach\\nOur aim is to equip a language model Mwith the\\nability to use different tools by means of API calls.\\nWe require that inputs and outputs for each API\\ncan be represented as text sequences. This allows\\nseamless insertion of API calls into any given text,\\nusing special tokens to mark the start and end of\\neach such call.\\nWe represent each API call as a tuple c= (ac;ic)\\nwhereacis the name of the API and icis the cor-\\nresponding input. Given an API call cwith a cor-\\nresponding result r, we denote the linearized se-\\nquences of the API call not including and including\\nits result, respectively, as:\\ne(c) =<API>ac(ic) </API>\\ne(c;r) =<API>ac(ic)!r</API>\\nwhere â€œ <API> â€, â€œ</API> â€ and â€œ!â€ are special\\ntokens.1Some examples of linearized API calls',\n",
              "  'question': 'Given an API call c = (ac; ic) and its corresponding result r, where ac is the name of the API and ic is the corresponding input, how can we represent the linearized sequences of the API call not including and including its result, respectively, as e(c) = <API>ac(ic) </API> and e(c;r) = <API>ac(ic)!r</API>?\\n\\n',\n",
              "  'answer': 'The linearized sequences of the API call not including and including its result, respectively, can be represented as e(c) = <API>ac(ic) </API> and e(c;r) = <API>ac(ic)!r</API>. Here, <API> is a special token used to indicate the start of the API call, \"ac\" is the name of the API, \"ic\" is the corresponding input, \"!\" is a special token used to indicate the end of the API call and the start of the result, and \"r\" is the corresponding result. Therefore, e(c) represents the API call without its result, and e(c;r) represents the API call with its result.',\n",
              "  'source_doc': 'Toolformer_ Language Models Can Teach Themselves to Use Tools.pdf'},\n",
              " {'context': 'to align with the embodied ALFRED benchmark (Shridhar et al., 2020a). It includes 6 types of\\ntasks in which an agent needs to achieve a high-level goal (e.g. examine paper under desklamp) by\\nnavigating and interacting with a simulated household via text actions (e.g. go to coffeetable 1, take\\npaper 2, use desklamp 1). A task instance can have more than 50 locations and take an expert policy\\nmore than 50 steps to solve, thus challenging an agent to plan and track subgoals, as well as explore\\nsystematically (e.g. check all desks one by one for desklamp). In particular, one challenge built into\\nALFWorld is the need to determine likely locations for common household items (e.g. desklamps will\\nlikely be on desks, shelfs, or dressers), making this environment a good ï¬t for LLMs to exploit their\\npretrained commonsense knowledge. To prompt ReAct , we randomly annotate three trajectories\\nfrom the training set for each task type, where each trajectory includes sparse thoughts that (1)\\ndecompose the goal, (2) track subgoal completion, (3) determine the next subgoal, and (4) reason via\\ncommonsense where to ï¬nd an object and what to do with it. We show prompts used for ALFWorld\\nin Appendix C.4. Following Shridhar et al. (2020b), we evaluate on 134 unseen evaluation games\\nin a task-speciï¬c setup. For robustness, we construct 6 prompts for each task type through each\\npermutation of 2 annotated trajectories from the 3 we annotate. Act prompts are constructed using\\nthe same trajectories, but without thoughts â€” since task instances are randomly chosen from the\\ntraining set, it favors neither ReAct norAct and provides a fair and controlled comparison to test the\\nimportance of sparse thoughts. For baselines, we use BUTLER (Shridhar et al., 2020b), an imitation\\nlearning agent trained on 105expert trajectories for each task type5.\\nWebShop CanReAct also interact with noisy real-world language environments for practical',\n",
              "  'question': 'In ALFWorld, how does the need to determine likely locations for common household items provide a good fit for LLMs to exploit their pretrained commonsense knowledge?\\n\\n',\n",
              "  'answer': 'The need to determine likely locations for common household items in ALFWorld provides a good fit for LLMs to exploit their pretrained commonsense knowledge because it requires the agent to reason about the physical world and use its prior knowledge about common household items and their likely locations. This task is closely related to the tasks that LLMs are trained on in natural language processing, such as question answering and reasoning about the physical world. By leveraging their pretrained knowledge, LLMs can more effectively plan and track subgoals, determine the next subgoal, and reason about the physical world to achieve the high-level goal.',\n",
              "  'source_doc': 'ReAct_ Synergizing Reasoning and Acting in Language Models.pdf'},\n",
              " {'context': 'Î¸Es,aâˆ¼Ïdata[r(s, a)]âˆ’Es,aâˆ¼ÏÎ¸[r(s, a)]âˆ’H[ÏÎ¸]âˆ’Ïˆ(r),\\n= sup\\nQinf\\nÎ¸Es,aâˆ¼Ïdata\\x02\\n(TÎ¸Q)(s, a)\\x03\\nâˆ’Es,aâˆ¼ÏÎ¸\\x02\\n(TÎ¸Q)(s, a)\\x03\\nâˆ’H[ÏÎ¸]âˆ’Ïˆ(TÎ¸Q),\\n= sup\\nQinf\\nÎ¸Es,aâˆ¼Ïdata\\x02\\n(TÎ¸Q)(s, a)\\x03\\nâˆ’(1âˆ’Î³)Es0âˆ¼P0\\x02\\nVÎ¸(s0)\\x03\\nâˆ’Ïˆ(TÎ¸Q),\\n= sup\\nQinf\\nÎ¸Es,aâˆ¼Ïdata\\x02\\nÏ•(Q(s, a)âˆ’Î³Esâ€²âˆ¼P(Â·|s,a)\\x02\\nVÎ¸(sâ€²)\\x03\\n)\\x03\\nâˆ’(1âˆ’Î³)Es0âˆ¼P0\\x02\\nVÎ¸(s0)\\x03\\n,\\n= sup\\nQinf\\nÎ¸Es,aâˆ¼Ïdata\\x02\\nÏ•(Q(s, a)âˆ’Î³Esâ€²âˆ¼P(Â·|s,a)\\x02\\nVÎ¸(sâ€²)\\x03\\n)\\x03\\nâˆ’Es,sâ€²âˆ¼Ï\\x02\\nVÎ¸(s)âˆ’Î³VÎ¸(sâ€²)\\x03\\n,\\n= sup\\nQJ(Q) = sup\\nQEs,aâˆ¼Ïdata\\x02\\nÏ•(Q(s, a)âˆ’Î³Esâ€²âˆ¼P(Â·|s,a)[V(sâ€²)])\\x03\\nâˆ’Es,sâ€²âˆ¼Ï[V(s)âˆ’Î³V(sâ€²)],\\n(15)\\nProof. The first equality is proven in section C.1. The second line follows from sections C.2 and C.4.\\nThe first section shows that the objectives J(Q, Î¸)andL(Î¸, r)are the same, by the bijective property\\n16ofT. The second section proves that the (unique) saddle points of the objectives correspond to the\\nsame solutions.\\nThe third line follows from the telescoping sum given in section C.3. The fourth line follows from the\\nsubstitution of a general Ïˆ(r)with a simpler regularizer Es,aâˆ¼Ïdata[g(r(s, a))], where g(r) =râˆ’Ï•(r)\\nifrâˆˆâ„¦, and infinity otherwise. This allows us to ground out the divergence minimization directly to\\nconcrete divergences such as the KL-divergence, JS-divergence, Ï‡2-divergence, etc. We discuss this\\nmore extensively in section D.1. In the fifth line we expand the telescoping sum in a different way\\nusing the result in section C.3. This allows us to incorporate samples from any policy, with the hope\\nthat it will decrease variance.\\nIn the final line we parameterize the policy from the Q-values, setting logpQ(a|s) =Q(s, a)âˆ’\\nlogP\\naâ€²âˆˆAexpQ(s, aâ€²). The fact that supQinfÎ¸J(pÎ¸, Q) = supQJ(pQ, Q)follows from the fact\\nthat there is a unique saddle point for J(pÎ¸, Q), the fact that J(pQ, Q)is concave in Q, and that\\nthe saddle point for J(pQ, Q)has a supremum in Qwhere Î¸=Î¸âˆ—, with logpâˆ—\\nÎ¸(a|s) =Qâˆ—(s, a)âˆ’\\nlogP\\naâ€²âˆˆAexpQâˆ—(s, aâ€²)andQâˆ—the corresponding supremum in Q. This allows elimination of Î¸\\nfrom the optimization process entirely, and completes the proof.\\nD Choices of divergence measures\\nD.1 f-divergences',\n",
              "  'question': 'What is the final equation of the optimization process in the given context?\\n\\n',\n",
              "  'answer': 'The final equation of the optimization process in the given context is:\\n\\nQJ(Q) = sup\\nQEs,aâˆ¼Ïdata[g(r(s, a))]\\nÏ•(Q(s, a)âˆ’Î³Esâ€²âˆ¼P(Â·|s,a)[V(sâ€²)])\\nâˆ’Es,sâ€²âˆ¼Ï[V(s)âˆ’Î³V(sâ€²)],\\n\\nwhere g(r) = râˆ’Ï•(r) if r âˆˆ â„¦, and infinity otherwise.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'curmudgeonly ice cream truck (though donâ€™t take our word for it). Just as some of the\\nother ice cream parlor attractions on the internet can help (hopefully), you can visit\\nthe Ice Cream Parlor to find even more delicious fried Oreo sandwiches, the \"glass ice\\ncream truck\" teddy bear (canâ€™t have it either), the skipjack pooping \"free\" soda fountain,\\nand the pyrotechnically gourmet ice cream fountain! Hereâ€™s the video of the Wrecking\\nBall, the Oreo pod dunk, and me having a little short squeeze last night. This also\\nhappened: the Lake Clark Breweryâ€™s Donut Ice Cream Factory opened the new location back\\nin 2015 and theyâ€™re rumored to be in the final phases of redevelopment. We hope you enjoy\\ngetting ahead of the news of their transition to the big comeback we expect to see in\\nApril.<|endoftext|>\\n<BOS>A notorious protester convicted of wilfully promoting hatred against Muslims and\\ncriminally harassing a Muslim man and his family was sentenced Tuesday to nine months\\nin jail. Eric Brazau handed out a flyer that \"vilified Muslims and disparages their\\nreligion,\" Ontario court Judge S. Ford Clements said in February, when he found Brazau\\nguilty. Eric Brazau was convicted of willful promotion of hatred against Muslims and\\ncriminally harassing a Muslim family. ( CARLOS OSORIO / TORONTO STAR FILE PHOTO ) The case\\nwas far from being on the borderline between \"rough and tumble debate\" and hate speech, as\\nBrazau had argued, Clements said in a College Park courtroom. Brazau handed out the flyer,\\nwhich contained many offensive references to Islam and Muslims, in August and September\\n2012. While distributing it, Brazau sometimes yelled obscenities about Islam \"in a tone of\\nvoice that suggested he was very angry and had little interest in debate,\" Clements said.\\nBrazau had argued that he did not intend to promote hate speech; instead he wanted to\\nstimulate debate about censorship, \"blasphemy laws\" and Sharia law, Clements said. Article',\n",
              "  'question': \"What were the consequences of Eric Brazau's actions in promoting hatred against Muslims and criminal harassment of a Muslim family?\\n\\n\",\n",
              "  'answer': 'Eric Brazau was sentenced to nine months in jail for his actions. The Ontario court judge, S. Ford Clements, found Brazau guilty of willful promotion of hatred against Muslims and criminally harassing a Muslim family. The case was not on the borderline between \"rough and tumble debate\" and hate speech, as Brazau had argued. Brazau handed out a flyer that contained many offensive references to Islam and Muslims, and sometimes yelled obscenities about Islam in a tone of voice that suggested he was very angry and had little interest in debate. The judge said that Brazau did not intend to promote hate speech; instead, he wanted to stimulate debate about censorship, \"blasphemy laws\" and Sharia law.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'instead of upgrading LLMs in the future.\\n8 Ethics\\nWe experiment on six math reasoning datasets, in-\\ncluding AQuA (Ling et al., 2017), GSM8K (Cobbe\\net al., 2021), MultiArith, AddSub, SingleEq, and\\nSV AMP (Patel et al., 2021), two commonsense\\nreasoning tasks (CommonsenseQA (Talmor et al.,\\n2019) and StrategyQA (Geva et al., 2021)), and\\ntwo symbolic tasks (Last Letter and Coin Flip (Wei\\net al., 2022b)), where GSM8K and SV AMP use\\nthe MIT License code, AQUA and StrategyQA use\\nthe Apache-2.0 code, the remaining datasets are\\nunspecified.\\nThe proposed prompts do not collect and use\\npersonal information about other individuals. The\\nprompts we used are listed in Appendix. The\\nprompts in this work do not contain any words\\nthat discriminate against any individual or group.\\nIn this work, prompts would not negatively impactother peopleâ€™s safety.',\n",
              "  'question': 'In this work, what ethical considerations were taken into account when designing the prompts and selecting the datasets used for experimentation?\\n\\n',\n",
              "  'answer': \"In this work, ethical considerations were taken into account when designing the prompts and selecting the datasets used for experimentation. The proposed prompts do not collect and use personal information about other individuals, and the prompts used do not contain any words that discriminate against any individual or group. Additionally, the prompts in this work would not negatively impact other people's safety.\",\n",
              "  'source_doc': 'Plan-and-Solve Prompting_  Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models.pdf'},\n",
              " {'context': 'the model should learn â€œwhyâ€ instead of just remember-\\ning â€œwhatâ€. Our goal is to shift the learning objectives of\\nlanguage models from results-oriented to process-oriented,\\nmoving away from rote memorization towards a more holis-tic understanding to break through current limitations im-\\nposed on language model capabilities. Secondly, language\\nmodels should mimic humansâ€™ learning process by simul-\\ntaneously grasping each sampleâ€™s relevant fundamentals,\\nchain of thought, and common mistakes to understand the\\ntraining objectives more comprehensively.TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise\\nTo achieve this, we define the learning process as com-\\nprising three dimensions: fundamental, chain of thought,\\nand common mistake. Our ultimate goal is to annotate\\nthis information for every sample in any NLP dataset. To\\nensure that TeacherLM has strong zero-shot capabilities\\nfor most natural language processing tasks, we have col-\\nlected 2 million samples from multiple domains for train-\\ning. Furthermore, we combined manual annotation and\\nSTaR (Zelikman et al., 2022) strategy to construct a com-\\nplete â€œ {Question } {Answer } {Fundamentals } {Chain of\\nThought } {Common Mistakes }â€ five-element training ob-\\nject for each sample.\\nIn sum, our key contributions are:\\nâ€¢Comprehensive TeacherLM can generate fundamen-\\ntals, chain of thought, and common mistakes, providing\\ncomprehensive information tailored to the taskâ€™s char-\\nacteristics and allowing each task to learn the most\\nrelevant knowledge.\\nâ€¢Generalizability TeacherLM is suitable for a vari-\\nety of datasets and models. As shown in Figure\\n1, we augmented 58 NLP datasets and taught vari-\\nous student models with different parameters from\\nOPT (Zhang et al., 2022), and BLOOM (Scao et al.,\\n2022) series in a multi-task setting. Compared to non-\\naugmented versions, the experimental results indicate\\nthat TeacherLMâ€™s data augmentation gains clear bene-\\nfits.',\n",
              "  'question': 'Given the context, what is the ultimate goal of TeacherLM in language modeling?\\n\\n',\n",
              "  'answer': \"The ultimate goal of TeacherLM in language modeling is to annotate information for every sample in any NLP dataset, including fundamentals, chain of thought, and common mistakes, to achieve a comprehensive learning process that mimics humans' learning process.\",\n",
              "  'source_doc': 'TeacherLM_ Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise.pdf'},\n",
              " {'context': 'FlashAttention is both faster and more memory-eï¬ƒcient than any existing attention method, whereas\\nfor sequence length beyond 1K, some approximate attention methods (e.g., Linformer) start to become\\nfaster. On the other hand, block-sparse FlashAttention is faster than all existing approximate attention\\nmethods that we know of.\\n2 Background\\nWe provide some background on the performance characteristics of common deep learning operations on\\nmodern hardware (GPUs). We also describe the standard implementation of attention.\\n2.1 Hardware Performance\\nWe focus here on GPUs. Performance on other hardware accelerators are similar [46, 48].\\nGPU Memory Hierarchy. The GPU memory hierarchy (Fig. 1 left) comprises multiple forms of\\nmemory of diï¬€erent sizes and speeds, with smaller memory being faster. As an example, the A100 GPU\\nhas 40-80GB of high bandwidth memory (HBM) with bandwidth 1.5-2.0TB/s and 192KB of on-chip SRAM\\nper each of 108 streaming multiprocessors with bandwidth estimated around 19TB/s [ 44,45]. The on-chip\\nSRAM is an order of magnitude faster than HBM but many orders of magnitude smaller in size. As compute\\nhas gotten faster relative to memory speed [ 61,62,63], operations are increasingly bottlenecked by memory\\n(HBM) accesses. Thus exploiting fast SRAM becomes more important.\\nExecution Model. GPUs have a massive number of threads to execute an operation (called a kernel).\\nEach kernel loads inputs from HBM to registers and SRAM, computes, then writes outputs to HBM.\\nPerformance characteristics. Depending on the balance of computation and memory accesses, op-\\nerations can be classiï¬ed as either compute-bound or memory-bound. This is commonly measured by the\\narithmetic intensity [85], which is the number of arithmetic operations per byte of memory access.\\n1.Compute-bound: the time taken by the operation is determined by how many arithmetic operations there\\nare, while time accessing HBM is much smaller. Typical examples are matrix multiply with large inner',\n",
              "  'question': 'Given the performance characteristics of attention operations on modern hardware, what is the optimal memory hierarchy architecture for efficient attention methods?\\n\\n',\n",
              "  'answer': 'The optimal memory hierarchy architecture for efficient attention methods would be one that utilizes fast on-chip SRAM for computation and slower HBM for memory access. This would allow for faster computation and reduced memory access latency, resulting in improved overall performance. Block-sparse FlashAttention is an example of an attention method that utilizes this architecture and has been shown to be faster than existing approximate attention methods.',\n",
              "  'source_doc': 'FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf'},\n",
              " {'context': 'more illegals in our country? No. We donâ€™t,\" says Eduardo. District Judge Lucy Kohls\\nsays school district officials should remove the \"A Day Without Immigrants\" sign from\\ntheir annual accreditation program, which requires every high school student to teach\\nEnglish. But according to Korea-based Senator Hillary Schakowsky, this group has started\\ncompeting. \"They seem to be suffering from stigma,\" says Schakowsky. \"Iâ€™m hoping the\\ncourts will find them to be ashamed of themselves for what theyâ€™ve done and to cast\\nthem in the negative light, if not as a group who would be well served to get on the\\nnational stage.\" Lynn Horcher says she hopes that Eduardoâ€™s message can inspire parents\\nand businesses to break down the walls around immigrants, without fear for their safety\\nand to break out of fear that those still working for immigrants will be jailed, beaten\\nand deported. \"Iâ€™m so sorry,\" says Lynch. \"Those who truly want to do this were always\\ncalled â€™Daddy Minjies.â€™ I say that as a parent who thinks that when your parents leave\\nyou need to do something that they really, really feel guilty about because you were\\nborn here.\" Witness the violence, burning and pickings, and watch the protesters packing\\ntents for three days. Could immigrants be shut down for protest or collect your money?\\nPhoto Credit: SimplePhotographer.com Follow @ChrisBreaux on Twitter. This article was\\noriginally published on The Conversation. Read the original story. [h/t: Trevor Wright]\\nDownload our NEW Occupy Democrats app for your iPhone by clicking here or for your Android\\nby clicking here. Add your name to millions demanding that!\\nToday, Toyota announced changes in executivesâ€™ areas of responsibility, as well as\\npersonnel changes at the sub-executive managerial level. The most important change by\\nfar is the appointment of Akio Toyoda, the companyâ€™s CEO and grandson of founder Kiichiro\\nToyoda, as President of a new â€™EV Business Planningâ€™ department. Earlier this month,',\n",
              "  'question': 'How have the attitudes towards immigrants in the school district affected their ability to teach English?\\n\\n',\n",
              "  'answer': \"The attitudes towards immigrants in the school district have affected their ability to teach English by causing stigma and shame, according to Senator Hillary Schakowsky. This has led to negative perceptions of the group and may discourage them from participating in the annual accreditation program. However, Lynn Horcher hopes that Eduardo's message can inspire parents and businesses to break down the walls around immigrants and break out of fear that those still working for immigrants will be jailed, beaten, and deported.\",\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'ronmental impact of compute use is impossible to conduct without\\ngrounding in context and making comparison to the counterfactual\\nimpacts of competing products or services. Such analysis is out of\\nscope for this paper.\\n8While data center energy usage has become much more efï¬-\\ncient in recent years (Masanet et al., 2020), the production, use,\\nand disposal of semiconductors still imposes environmental and\\nhuman costs. See, e.g., (Crawford, 2021)\\n9Given that code generation (and other forms of AI) might be\\ndeployed widely throughout the economy as discussed above, these\\nconsiderations suggest additional urgency in adopting renewable\\nenergy.features that exist as features of other tools of authorship\\n(e.g., document editors), in the sense that the ï¬nished work\\nis still seen as the authorâ€™s.\\nOur commitment to responsible and safe AI includes con-\\ntinued attention to the broader intellectual property impli-\\ncations of code generation systems. We intend to remain\\nengaged with policymakers and experts on these issues so\\nthat the users of such systems can ultimately deploy them\\nwith conï¬dence.\\n7.8. Risk mitigation\\nIn closing, given the above, models like Codex should be\\ndeveloped, used, and their capabilities explored carefully\\nwith an eye towards maximizing their positive social im-\\npacts and minimizing intentional or unintentional harms that\\ntheir use might cause. A contextual approach is critical to\\neffective hazard analysis and mitigation, though a few broad\\ncategories of mitigations are important to consider in any\\ndeployment of code generation models.\\nCareful documentation and user interface design, code re-\\nview requirements, and/or content controls (e.g., ï¬ltering\\nof outputs) may help to reduce harms associated with over-\\nreliance as well as offensive content or insecure code gener-\\nation. In the context of a model made available as a service\\n(e.g., via an API), policies such as user review, use case\\nrestrictions, monitoring, and/or rate limiting may also help',\n",
              "  'question': 'How might the environmental impact of code generation be mitigated through careful documentation and user interface design, code review requirements, content controls, and policies such as user review and rate limiting?\\n\\n',\n",
              "  'answer': 'To mitigate the environmental impact of code generation, careful documentation and user interface design, code review requirements, content controls, and policies such as user review and rate limiting can be implemented. These measures can help reduce harms associated with over-reliance, offensive content, and insecure code generation. In the context of a model made available as a service, user review, use case restrictions, monitoring, and rate limiting can also be employed to further mitigate potential harms.',\n",
              "  'source_doc': 'Evaluating Large Language Models Trained on Code.pdf'},\n",
              " {'context': 'Least-to-Most 94.0 88.4 83.0 76.4 74.0\\nTable 4: Accuracies of different prompting methods on the last-letter-concatenation task. The length\\nof testing lists increases from 4 to 12.\\nIn Appendices 7.2 and 7.3, we present additional experiments with different chain-of-thought\\nprompts and different language models. Note that in contrast to least-to-most prompting, the exem-\\nplars in a chain-of-thought prompt can be independent of each other. For the last-letter concatenation\\ntask, this means that we do not need to present exemplars that are sublists of other exemplars. In\\nfact, a chain-of-thought prompt with independent lists tends to outperform one with dependent lists,\\nas the former conveys more information. Furthermore, we can enhance chain-of-thought prompting\\nby incorporating additional exemplars. This seems to be fair, as the least-to-most prompt contains\\nmore words due to its extra decomposition. As shown in Table 13 (Appendix 7.3), for lists with\\nlength 12, chain-of-thought prompting achieves an accuracy of 37.4% with 4 independent exem-\\nplars (Appendix 7.2.2), and 38.4% with 8 independent exemplars (Appendix 7.2.3). Although there\\n1https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/PG/2006/04/\\n1-10000\\n4Published as a conference paper at ICLR 2023\\nhave been notable advancements compared to an accuracy of 31.8% by the original prompt in Table\\n3, chain-of-thought prompting still lags behind least-to-most prompting, which boasts an accuracy\\nof 74.0%.\\nError analysis . While least-to-most prompting signiï¬cantly outperforms chain-of-thought prompt-\\ning, it is still far from achieving 100% accuracy for long lists. In Appendix 7.4, we present a detailed\\nerror analysis. We ï¬nd that only very few of them are due to incorrect last letters, while most of\\nthem are concatenation errors (dropping or adding a letter). For example, given the list â€œgratiï¬ed,\\ncontract, fortitude, blewâ€, the model drops the last letter in the concatenation of â€œdteâ€ and â€œwâ€, and',\n",
              "  'question': 'In this context, what is the difference between least-to-most and chain-of-thought prompting methods for the last-letter-concatenation task?\\n\\n',\n",
              "  'answer': 'Least-to-most prompting involves presenting exemplars that are sublists of other exemplars, while chain-of-thought prompting involves independent exemplars that convey more information. Chain-of-thought prompting with independent exemplars tends to outperform one with dependent lists. Additionally, incorporating additional exemplars can enhance chain-of-thought prompting. Despite notable advancements compared to the original prompt, chain-of-thought prompting still lags behind least-to-most prompting, which boasts an accuracy of 74.0%.',\n",
              "  'source_doc': 'Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.pdf'},\n",
              " {'context': 'InstructGPT models show promising generalization to instructions outside of the RLHF ï¬ne-\\ntuning distribution. We qualitatively probe InstructGPTâ€™s capabilities, and ï¬nd that it is able to\\nfollow instructions for summarizing code, answer questions about code, and sometimes follows\\ninstructions in different languages, despite these instructions being very rare in the ï¬ne-tuning\\ndistribution. In contrast, GPT-3 can perform these tasks but requires more careful prompting, and\\ndoes not usually follow instructions in these domains. This result is exciting because it suggests that\\nour models are able to generalize the notion of â€œfollowing instructions.â€ They retain some alignment\\neven on tasks for which they get very little direct supervision signal.\\nInstructGPT still makes simple mistakes. For example, InstructGPT can still fail to follow\\ninstructions, make up facts, give long hedging answers to simple questions, or fail to detect instructions\\nwith false premises.\\nOverall, our results indicate that ï¬ne-tuning large language models using human preferences signiï¬-\\ncantly improves their behavior on a wide range of tasks, though much work remains to be done to\\nimprove their safety and reliability.\\nThe rest of this paper is structured as follows: We ï¬rst detail related work in Section 2, before diving\\ninto our method and experiment details in Section 3, including our high-level methodology (3.1), task\\nand dataset details (3.3 and 3.2), human data collection (3.4), how we trained our models (3.5), and\\nour evaluation procedure (3.6). We then present our results in Section 4, divided into three parts:\\nresults on the API prompt distribution (4.1), results on public NLP datasets (4.2), and qualitative\\nresults (4.3). Finally we give an extended discussion of our work in Section 5, including implications\\nfor alignment research (5.1), what we are aligning to (5.2), limitations (5.3), open questions (5.4),\\nand broader impacts of this work (5.5).\\n2 Related work',\n",
              "  'question': 'In what ways does InstructGPT differ from GPT-3 in terms of task performance and instruction following?\\n\\n',\n",
              "  'answer': 'InstructGPT outperforms GPT-3 in tasks such as summarizing code, answering questions about code, and sometimes following instructions in different languages, despite these instructions being rare in the fine-tuning distribution. GPT-3 can perform these tasks but requires more careful prompting and does not usually follow instructions in these domains. InstructGPT still makes simple mistakes such as failing to follow instructions, making up facts, giving long hedging answers to simple questions, or failing to detect instructions with false premises. Overall, InstructGPT shows promising generalization to instructions outside of the RLHF fine-tuning distribution and retains some alignment even on tasks for which it gets very little direct supervision signal.',\n",
              "  'source_doc': 'Training language models to follow instructions with human feedback.pdf'},\n",
              " {'context': 'QLORAis effective and can produce state-of-the-art chatbots that rival ChatGPT. Furthermore, our\\n33B Guanaco can be trained on 24 GB consumer GPUs in less than 12 hours. This opens up the\\npotential for future work via QLORAtuning on specialized open-source data, which produces models\\nthat can compete with the very best commercial models that exist today.\\n6 Qualitative Analysis\\nWhile quantitative analysis is the core of our evaluation, there are a number of issues with only\\nlooking at summary statistics. Perhaps the largest is the problem of benchmark validity [ 36]â€”whether\\na benchmark truly tests what its name or description suggests is always at question, especially as we\\ndiscover â€œshortcutsâ€ to solve benchmarks that machine learning models sometimes exploit [ 22,46].\\nTo partially alleviate this, we here perform some qualitative analysis, in two sections. First, in Â§6.1\\n10Table 7: Elo rating for a tournament between models where models compete to generate the best response\\nfor a prompt, judged by human raters or GPT-4. Overall, Guanaco 65B and 33B tend to be preferred to\\nChatGPT-3.5 on the benchmarks studied. According to human raters they have a Each 10-point difference in Elo\\nis approximately a difference of 1.5% in win-rate.\\nBenchmark Vicuna Vicuna Open Assistant\\n# Prompts 80 80 953\\nJudge Human raters GPT-4 GPT-4 Median Rank\\nModel Elo Rank Elo Rank Elo Rank\\nGPT-4 1176 1 1348 1 1294 1 1\\nGuanaco-65B 1023 2 1022 2 1008 3 2\\nGuanaco-33B 1009 4 992 3 1002 4 4\\nChatGPT-3.5 Turbo 916 7 966 5 1015 2 5\\nVicuna-13B 984 5 974 4 936 5 5\\nGuanaco-13B 975 6 913 6 885 6 6\\nGuanaco-7B 1010 3 879 8 860 7 7\\nBard 909 8 902 7 - - 8\\nwe show some examples that we believe are representative of some observed patterns in the text\\ngenerated by our 65b Guanaco model. Second, Â§6.2 we detail considerations about the results we\\nhave discussed and our interpretation of them.\\n6.1 Qualitative Analysis of Example Generations',\n",
              "  'question': 'Which models performed better in the benchmark study, based on the Elo ratings?\\n\\n',\n",
              "  'answer': 'Guanaco-65B and Guanaco-33B performed better in the benchmark study, according to the Elo ratings.',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': 'for the loss in equation (6):\\nË†J(â„“Î¸) =NX\\niÎ³i1\\nÎ±Ï•(Î±â„“Î¸(ai|si)âˆ’Î³Î±V(si+1))\\n| {z }\\nPenalized difference from action logit to next state valueâˆ’NX\\niÎ³i\\n2[V(si)âˆ’Î³V(si+1)]\\n| {z }\\nState, next state value difference under data\\nâˆ’MX\\niÎ³i\\n2[V(ui)âˆ’Î³V(ui+1)]\\n| {z }\\nState, next state value difference under model+ Ë†Jeos(â„“Î¸)|{z}\\nLoss from completed sequences, (7)\\nwith\\nË†Jeos(â„“Î¸) =Î³N\\nÎ±(1âˆ’Î³)Ï•(Î±(1âˆ’Î³)V(sN))âˆ’Î³N\\n2V(sN)âˆ’Î³M\\n2V(uM), (8)\\nandV(s) = logP\\naâ€²âˆˆAexpâ„“Î¸(aâ€²|s). The separate treatment of the <eos> tokens arises because\\nwe must take a sum over the infinite timesteps in equation (3)in the terminal states with <eos>\\ntokens. Analytically computing this infinite sum results in the terms in Ë†Jeos(â„“Î¸). It has also been\\nshown [ 1] that minimizing the the divergence of mixtures DÏ‡2(Ïdata,(Ïdata+ÏÎ¸)/2)is more effective\\nthan simply minimizing the Ï‡2-divergence between model and data. This can be implemented\\nby calculating the loss for the Ï‡2-divergence (with Ï•(x) =xâˆ’1\\n4Î±x2) and adding an additional\\nregularization term Es,a,sâ€²âˆ¼ÏÎ¸\\x02\\n(Î±â„“Î¸(a|s)âˆ’Î³Î±V(sâ€²))2\\x03\\n.\\nWhile the overall objective Ë†J(â„“Î¸)may look similar to the MLE estimator due to the â„“Î¸(a|s)âˆ’Î³V(s) =\\nâ„“Î¸(a|s)âˆ’Î³logP\\naâ€²âˆˆAexpâ„“Î¸(aâ€²|sâ€²)term, it should be stressed that the normalization is with respect\\nto the next sequence state, as opposed to the MLE loss which normalizes according to the current\\nsequence state.\\nWhile the loss J(â„“Î¸)may look obscure, it is instructive to take the limit as Î±â†’0for the Ï•\\ncorresponding to the KL-divergence. We show in the appendix that we do indeed have limÎ±â†’0Jâ„“Î¸=\\nDKL(Ïdataâˆ¥ÏÎ¸). The equality follows by approximating1\\nÎ±Ï•(Î±x) =xÏ•â€²(0) +O(Î±)ifÏ•has a Taylor\\nexpansion at 0, which allows a linearization of Ï•and simplifications. We also show that in the absence\\nof editing actions, DKL(Ïdataâˆ¥ÏÎ¸)reduces to a Î³-reweighted MLE objective.\\n4.1 Relative Efficiency of Generating During Training\\nOur loss as presented requires sampling from the model during training. Sampling is generally much',\n",
              "  'question': 'In what way does the loss function in equation (6) differ from the MLE estimator?\\n\\n',\n",
              "  'answer': 'The loss function in equation (6) differs from the MLE estimator in that it normalizes with respect to the next sequence state, while the MLE estimator normalizes with respect to the current sequence state. Additionally, the loss function includes regularization terms to minimize the divergence of mixtures and to prevent overfitting.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'explore evaluation via few lookahead simulations (e.g. quickly conï¬rm that 5, 5, 14 can\\nreach 24 via 5 + 5 + 14, or â€œhot lâ€ can mean â€œinnâ€ via ï¬lling â€œeâ€ in â€œ â€) plus commonsense\\n(e.g. 1 2 3 are too small to reach 24, or no word can start with â€œtzxcâ€). While the former\\nmight promote â€œgoodâ€ states, the latter could help eliminate â€œbadâ€ states. Such valuations\\ndo not need to be perfect, and only need to be approximately\\n(b)Vote across states: V(p\\x12;S)(s) =1[s=s\\x03], where a â€œgoodâ€ state s\\x03\\x18pvote\\n\\x12(s\\x03jS)is\\nvoted out based on deliberately comparing different states in Sin a vote prompt. When\\nproblem success is harder to directly value (e.g. passage coherency), it is natural to to instead\\ncompare different partial solutions and vote for the most promising one. This is similar\\nin spirit to a â€œstep-wiseâ€ self-consistency strategy, i.e. cast â€œwhich state to exploreâ€ as a\\nmulti-choice QA, and use LM samples to vote for it.\\nFor both strategies, we could prompt the LM multiple times to aggregate the value or vote results to\\ntrade time/resource/cost for more faithful/robust heuristics.\\nAlgorithm 1 ToT-BFS(x;p\\x12;G;k;V;T;b )\\nRequire: Inputx, LMp\\x12, thought generator G()\\n& size limitk, states evaluator V(), step limitT,\\nbreadth limit b.\\nS0 fxg\\nfort= 1;\\x01\\x01\\x01;Tdo\\nS0\\nt f[s;z]js2St\\x001;zt2G(p\\x12;s;k)g\\nVt V(p\\x12;S0\\nt)\\nSt arg maxS\\x1aS0\\nt;jSj=bP\\ns2SVt(s)\\nend for\\nreturnG(p\\x12;arg maxs2STVT(s);1)Algorithm 2 ToT-DFS(s;t;p\\x12;G;k;V;T;v th)\\nRequire: Current state s, stept, LMp\\x12, thought\\ngeneratorG()and size limit k, states evaluator\\nV(), step limitT, thresholdvth\\nift>T then record output G(p\\x12;s;1)\\nend if\\nfors02G(p\\x12;s;k)do.sorted candidates\\nifV(p\\x12;fs0g)(s)>vthres then.pruning\\nDFS(s0;t+ 1)\\nend if\\nend for\\n4. Search algorithm. Finally, within the ToT framework, one can plug and play different search\\nalgorithms depending on the tree structure. We explore two relatively simple search algorithms and\\nleave more advanced ones (e.g. A* [9], MCTS [2]) for future work:',\n",
              "  'question': 'In the context of evaluation via few lookahead simulations and commonsense, what is the difference between ToT-BFS and ToT-DFS search algorithms?\\n\\n',\n",
              "  'answer': 'The ToT-BFS and ToT-DFS search algorithms differ in their approach to exploring the search space. ToT-BFS is a breadth-first search algorithm that explores all possible states at a given depth before moving on to the next level. It uses a thought generator to generate candidate states and a states evaluator to evaluate the fitness of each state. The algorithm then selects the state with the highest fitness to explore next.\\n\\nToT-DFS, on the other hand, is a depth-first search algorithm that explores the deepest possible state before backtracking to explore shallower states. It also uses a thought generator to generate candidate states and a states evaluator to evaluate the fitness of each state. However, if the current state does not meet a certain threshold, the algorithm prunes the state and continues exploring deeper states. In ToT-DFS, the algorithm selects the state with the highest fitness to explore next, just like ToT-BFS.',\n",
              "  'source_doc': 'Tree of Thoughts_  Deliberate Problem Solving with Large Language Models.pdf'},\n",
              " {'context': 'speciï¬c websites (Wikipedia or WebShop) that are free of private information, without any dangerous\\nactions in the action space design (i.e. models cannot really buy products on WebShop the research\\nbenchmark, or edit Wikipedia). We believe researchers should be aware of such risks before designing\\nmore extensive experiments in the future.\\nRE',\n",
              "  'question': 'Are there any specific websites that are free of private information and do not allow dangerous actions?\\n',\n",
              "  'answer': 'Yes, there are websites that are free of private information and do not allow dangerous actions. These websites are designed for research purposes and are intended to prevent researchers from performing actions that could harm individuals or the website itself.',\n",
              "  'source_doc': 'ReAct_ Synergizing Reasoning and Acting in Language Models.pdf'},\n",
              " {'context': 'an absolute success rate of 34% and 10% respectively, while being prompted with\\nonly one or two in-context examples.\\n1 I NTRODUCTION\\nA unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with\\nverbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to\\nplay an important role in human cognition for enabling self-regulation or strategization (Vygotsky,\\n1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Con-\\nsider the example of cooking up a dish in the kitchen. Between any two speciï¬c actions, we may\\nreason in language in order to track progress (â€œnow that everything is cut, I should heat up the pot of\\nwaterâ€), to handle exceptions or adjust the plan according to the situation (â€œI donâ€™t have salt, so let\\nme use soy sauce and pepper insteadâ€), and to realize when external information is needed (â€œhow do\\nI prepare dough? Let me search on the Internetâ€). We may also act (open a cookbook to read the\\nrecipe, open the fridge, check ingredients) to support the reasoning and to answer questions (â€œWhat\\ndish can I make right now?â€). This tight synergy between â€œactingâ€ and â€œreasoningâ€ allows humans\\nto learn new tasks quickly and perform robust decision making or reasoning, even under previously\\nunseen circumstances or facing information uncertainties.\\nRecent results have hinted at the possibility of combining verbal reasoning with interactive decision\\nmaking in autonomous systems. On one hand, properly prompted large language models (LLMs)\\nhave demonstrated emergent capabilities to carry out several steps of reasoning traces to derive\\n\\x03Work during Google internship. Projet page with code: https://react-lm.github.io/ .\\n1arXiv:2210.03629v3  [cs.CL]  10 Mar 2023Published as a conference paper at ICLR 2023',\n",
              "  'question': 'In human cognition, what role does the ability to seamlessly combine task-oriented actions with verbal reasoning play?\\n\\n',\n",
              "  'answer': 'The ability to seamlessly combine task-oriented actions with verbal reasoning plays an important role in human cognition for enabling self-regulation or strategization, maintaining a working memory, and allowing humans to learn new tasks quickly and perform robust decision making or reasoning, even under previously unseen circumstances or facing information uncertainties.',\n",
              "  'source_doc': 'ReAct_ Synergizing Reasoning and Acting in Language Models.pdf'},\n",
              " {'context': 'pre-training, even when BERT is ï¬ne-tuned on MS MARCO as an intermediate step. Our pre-trained model\\nalso outperforms BM25, showing the advantage of dense retriever over lexical methods in the few-shot setting.\\nMore details are given in Appendix A.3.\\n5 Multilingual retrieval\\nIn this section, we illustrate another advantage of learning unsupervised dense retrievers, when performing\\nmulti-lingual retrieval. While large labeled datasets are available in English, allowing to train strong dense\\nretrievers (as shown in previous sections), this is unfortunately not the case for lower resources languages.\\nHere, we show how unsupervised training is a promising direction. First, we show that our approach leads to\\nstrong performance, either in a full unsupervised setting, or by ï¬ne-tuning a multi-lingual model on English\\ndata such as MS MARCO. Second, we demonstrate that our model can also perform cross-lingual retrieval,\\nby retrieving English documents from other languages queries. Unsupervised retrievers based on lexical\\nmatching, such as BM25, would obtain poor performance, especially for pairs of languages with diï¬€erent\\nscripts such as English and Arabic.\\n5.1 Multilingual pre-training\\nOur multilingual model, called mContriever , is jointly pre-trained on 29 languages. The multilingual pre-\\ntraining largely follows the method discussed in previous sections, but it diï¬€ers by few particularities related\\nto the pre-training data and the hyperparameters used. The model is initialized with the multilingual version\\nof BERT, mBERT, trained on 104 languages. For the pre-training data, we consider the languages contained\\nin CCNet (Wenzek et al., 2020) that are also part of our evaluation datasets. This results in a training set\\ncontaining the CCNet data for 29 languages detailed in Table 12. During pre-training, examples are uniformly\\nsampled over languages, i.e. the probability that a training sample comes from a speciï¬c language is the same',\n",
              "  'question': 'What is the advantage of learning unsupervised dense retrievers in the context of multilingual retrieval?\\n\\n',\n",
              "  'answer': 'The advantage of learning unsupervised dense retrievers in the context of multilingual retrieval is that they lead to strong performance, either in a full unsupervised setting or by fine-tuning a multi-lingual model on English data such as MS MARCO. Additionally, the model can perform cross-lingual retrieval, which is not possible with unsupervised retrievers based on lexical matching such as BM25, especially for pairs of languages with different scripts such as English and Arabic.',\n",
              "  'source_doc': 'Unsupervised Dense Information Retrieval with Contrastive Learning.pdf'},\n",
              " {'context': '2.3 Pretext tasks\\nIn this section, we describe pretext tasks that can be used to jointly pre-train the retriever and the language\\nmodel using only unsupervised data.\\nPreï¬x language modeling. First, we consider a standard language modeling task as potential pre-training\\nobjective. To cast language modeling in the text-to-text framework, we consider a chunk of Nwords, and split\\nthis chunk in two sub-sequences of equal length N/2. Then, the ï¬rst sub-sequence is used as the query, and\\nthe second corresponds to the output. We thus retrieve relevant documents by using the ï¬rst sub-sequence of\\nN/2tokens, to generate the output.\\nMasked language modeling. Second, we consider masked language modeling, as formulated by Raï¬€el\\net al. (2019). Again, starting from a chunk of Nwords, we sample kspans of average length 3 tokens, leading\\nto a masking ratio of 15%. We then replace each span by a diï¬€erent special token. The model is then trained\\nto generate the masked spans, each span beginning with the special sentinel mask token that was inserted in\\nthe input sequence. We retrieve documents using the masked query, but replace the special mask tokens with\\na mask token supported by the retriever vocabulary.\\n5Title to section generation. Finally, we consider a more abstractive generation task, generating sections\\nfrom Wikipedia articles, given the article and section title. Here, the query corresponds to the title of the\\narticle, together with the title of the section, and the output corresponds to the text of the section. We\\nexclude sections â€œSee alsoâ€, â€œReferencesâ€, â€œFurther readingâ€ and â€œExternal linksâ€.\\n2.4 Eï¬ƒcient retriever ï¬ne-tuning\\nRetrieval is facilitated by using a document index, which is a pre-computed collection of the document\\nembeddings for all the documents in the retrieval corpus. When jointly training the retriever and language\\nmodel, the index needs to be updated regularly, otherwise, the embeddings of the documents stored in',\n",
              "  'question': 'What is the purpose of the pretext tasks in the text-to-text framework and how do they facilitate the joint pre-training of the retriever and language model?\\n\\n',\n",
              "  'answer': 'The pretext tasks in the text-to-text framework are used to jointly pre-train the retriever and language model using only unsupervised data. These tasks include prefix language modeling, masked language modeling, and title to section generation. Prefix language modeling involves splitting a chunk of Nwords into two sub-sequences, using the first sub-sequence as the query and the second as the output to retrieve relevant documents. Masked language modeling involves sampling kspans of average length 3 tokens, replacing each span with a different special token, and training the model to generate the masked spans. Title to section generation involves generating sections from Wikipedia articles, given the article and section title, excluding certain sections. These pretext tasks facilitate the joint pre-training of the retriever and language model by providing a way to train the model on unlabeled data, which can improve its performance on downstream tasks.',\n",
              "  'source_doc': 'Atlas_  Few-shot Learning with Retrieval Augmented Language Models.pdf'},\n",
              " {'context': 'Training overview. SELF-RAGenables an arbitrary LM to generate text with reflection tokens\\nby unifying them as next token predictions from the expanded model vocabulary (i.e., the original\\nvocabulary plus reflection tokens). Specifically, we train the generator model Mon a curated corpus\\nwith interleaving passages retrieved by a retriever Rand reflection tokens predicted by a critic model\\nC(summarized in Appendix Algorithm 2). We train Cto generate reflection tokens for evaluating\\nretrieved passages and the quality of a given task output (Section 3.2.1). Using the critic model, we\\nupdate the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we\\ntrain the final generator model ( M) using the conventional LM objective (Section 3.2.2) to enable\\nMto generate reflection tokens by itself without relying on the critic at inference time.\\n3.2 S ELF-RAGTRAINING\\nHere, we describe the supervised data collection and training of two models, the critic C(Section 3.2.1)\\nand the generator M(Section 3.2.2).\\n3.2.1 T RAINING THE CRITIC MODEL\\nData collection for critic model. Manual annotation of reflection tokens for each segment is\\nexpensive (Wu et al., 2023). A state-of-the-art LLM like GPT-4 (OpenAI, 2023) can be effectively\\n4We follow Liu et al. (2023a) in using a â€œperceivedâ€ utility value that is independent of retrieved passages.\\n4Preprint.\\nInput: How did US states get their names? Input: Write an essay of your best summer vacationOutput: My best summer vacation was a magical escape to the coastal town of Santorini. The azure waters, charming white-washed building are unforgettable. \\nCritic LMOutput: 1 of 50 states names come from persons. For instance, Louisiana was named in honor of King Louis XIV of France and Georgia was named after King George II. \\nRetrieve\\nPartially',\n",
              "  'question': 'Given that the critic model C is trained to predict reflection tokens for evaluating retrieved passages and the quality of a given task output, how does the critic model determine the utility value of a given input-output pair, and what is the basis for this determination?\\n\\n',\n",
              "  'answer': 'The critic model determines the utility value of a given input-output pair by analyzing the content of the output and comparing it to a set of predefined criteria. In this case, the critic model uses a \"perceived\" utility value that is independent of retrieved passages, as described in Liu et al. (2023a). The specific criteria used by the critic model to determine the utility value may vary depending on the task and the desired output format. However, the overall goal of the critic model is to provide feedback on the quality of the output and help improve the performance of the generator model during training.',\n",
              "  'source_doc': 'Self-Rag_ Learning to Retrieve, Generate, and Critique Through Self-Reflection.pdf'},\n",
              " {'context': '</API> March 10, and Sunday, March 19 for regular hours.2.11 3\\nThe Nile has an approximate length of <API> QA(What is the approximate length of the Nile?)\\n!6,853 km </API> 6,853 kilometers, the White Nile being its main source.2.08 3\\nIf Venus had an atmosphere similar to Earthâ€™s then you would expect Venusâ€™ mean temperature to be\\n499 K (1.74 x 287) rather than 735 K which is <API> Calculator(735 / 499) !1.47</API> 1.47\\n(735 / 499) times hotter than it should be.1.59 3\\nYou are here: Home / Featured / Catch this fast train to success! <API> WikiSearch(Fast train\\nsuccess)!Fast Train > It also peaked at #23 on the Canadian CHUM singles chart, on June\\n26, 1971. The success of this single established Myles Goodwyn as the bandâ€™s main songwriter,\\nand made it possible for April Wine to record a second album. </API> Donâ€™t wait weeks and\\n[:::]0.92 7\\nOs Melhores Escolas em Jersey 2020 <API> MT(Os Melhores Escolas em Jersey) !The Best\\nSchools in Jersey </API> On this page you can search for Universities, Colleges and Business\\nschools in Jersey0.70 3\\nEnjoy these pictures from the <API> Calendar()!Today is Friday, April 19, 2013. </API>\\nEaster Egg Hunt.0.33 3\\n85 patients (23%) were hospitalised alive and admitted to a hospital ward. Of them, <API> Calcula-\\ntor(85 / 23)!3.70</API> 65% had a cardiac aetiology [:::]\\x000.02 7\\nBut hey, after the <API> Calendar()!Today is Saturday, June 25, 2011. </API> Disneyland\\nï¬asco with the ï¬re drill, I think itâ€™s safe to say Chewey wonâ€™t let anyone die in a ï¬re.\\x000.41 7\\nThe last time I was with <API> QA(Who was last time I was with?) !The Last Time </API>\\nhim I asked what he likes about me and he said he would tell me one day.\\x001.23 7\\nTable 10: Examples of API calls for different tools, sorted by the value of L\\x00\\ni\\x00L+\\nithat is used as a ï¬ltering\\ncriterion. High values typically correspond to API calls that are intuitively useful for predicting future tokens.\\napproaches, additional information is always pro-',\n",
              "  'question': 'What is the approximate length of the Nile?\\n',\n",
              "  'answer': 'The Nile has an approximate length of 6,853 kilometers.',\n",
              "  'source_doc': 'Toolformer_ Language Models Can Teach Themselves to Use Tools.pdf'},\n",
              " {'context': 'between human intelligence and machine learning: (1) Given a new task, humans usually can learn\\nto accomplish it from only a few demonstration examples, while machine learning requires a large\\namount of labeled data for model training; (2) Humans can clearly explain the underlying rationale\\nfor their predictions or decisions, while machine learning is essentially a black box; (3) Humans can\\nsolve problems more difï¬cult than any they have seen before, while for machine learning, examples\\nin training and testing are typically at the same level of difï¬culty.\\nThe recently proposed chain-of-thought prompting approach (Wei et al., 2022; Chowdhery et al.,\\n2022) has taken a signiï¬cant step for narrowing the gap between human intelligence and machine in-\\ntelligence. It combines the idea of natural language rationales (Ling et al., 2017; Cobbe et al., 2021)\\nwith few-shot prompting (Brown et al., 2020). When further integrated with self-consistency decod-\\ning (Wang et al., 2022b) rather than using the typical greedy decoding, few-shot chain-of-thought\\nprompting largely outperforms the state-of-the-art results in the literature on many challenging natu-\\nral language processing tasks obtained from specially designed neural models trained with hundreds\\nof times more annotated examples, while being fully interpretable.\\nHowever, chain-of-thought prompting has a key limitationâ€”it often performs poorly on tasks that\\nrequire generalization of solving problems harder than the demonstration examples, such as com-\\npositional generalization (Lake & Baroni, 2018; Keysers et al., 2020). To tackle such easy-to-hard\\ngeneralization issues, we propose least-to-most prompting . It consists of two stages: ï¬rst decom-\\nposing a complex problem into a list of easier subproblems, and then sequentially solving these\\nsubproblems, whereby solving a given subproblem is facilitated by the answers to previously solved\\n\\x03Corresponding to: dennyzhou@google.com',\n",
              "  'question': 'What is the proposed approach to narrow the gap between human intelligence and machine intelligence, and what are its limitations?\\n\\n',\n",
              "  'answer': 'The proposed approach to narrow the gap between human intelligence and machine intelligence is chain-of-thought prompting, which combines the idea of natural language rationales with few-shot prompting and self-consistency decoding. Its limitations include poor performance on tasks that require generalization of solving problems harder than the demonstration examples, such as compositional generalization. To tackle such easy-to-hard generalization issues, a new approach called least-to-most prompting is proposed, which consists of two stages: decomposing a complex problem into a list of easier subproblems and sequentially solving these subproblems.',\n",
              "  'source_doc': 'Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.pdf'},\n",
              " {'context': 'Conference on Empirical Methods in Natural Lan-\\nguage Processing and the 9th International Joint\\nConference on Natural Language Processing , pages\\n5878â€“5882.\\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.\\nEnd-to-end open-domain question answering with\\nBERTserini. In Proceedings of the 2019 Confer-\\nence of the North American Chapter of the Asso-\\nciation for Computational Linguistics (Demonstra-\\ntions) , pages 72â€“77.',\n",
              "  'question': 'How does the BERTserini model perform in end-to-end open-domain question answering?\\n\\n',\n",
              "  'answer': 'The BERTserini model achieves state-of-the-art performance in end-to-end open-domain question answering, outperforming previous models on several benchmark datasets.',\n",
              "  'source_doc': 'Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering.pdf'},\n",
              " {'context': 'Tree of Thoughts: Deliberate Problem Solving\\nwith Large Language Models\\nShunyu Yao\\nPrinceton UniversityDian Yu\\nGoogle DeepMindJeffrey Zhao\\nGoogle DeepMindIzhak Shafran\\nGoogle DeepMind\\nThomas L. Grifï¬ths\\nPrinceton UniversityYuan Cao\\nGoogle DeepMindKarthik Narasimhan\\nPrinceton University\\nAbstract\\nLanguage models are increasingly being deployed for general problem solving\\nacross a wide range of tasks, but are still conï¬ned to token-level, left-to-right\\ndecision-making processes during inference. This means they can fall short in\\ntasks that require exploration, strategic lookahead, or where initial decisions play\\na pivotal role. To surmount these challenges, we introduce a new framework for\\nlanguage model inference, â€œTree of Thoughtsâ€ (ToT), which generalizes over the\\npopular â€œChain of Thoughtâ€ approach to prompting language models, and enables\\nexploration over coherent units of text (â€œthoughtsâ€) that serve as intermediate steps\\ntoward problem solving. ToT allows LMs to perform deliberate decision making\\nby considering multiple different reasoning paths and self-evaluating choices to\\ndecide the next course of action, as well as looking ahead or backtracking when\\nnecessary to make global choices. Our experiments show that ToT signiï¬cantly\\nenhances language modelsâ€™ problem-solving abilities on three novel tasks requiring\\nnon-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords.\\nFor instance, in Game of 24, while GPT-4 with chain-of-thought prompting only\\nsolved 4% of tasks, our method achieved a success rate of 74%. Code repo with all\\nprompts: https://github.com/ysymyth/tree-of-thought-llm .\\n1 Introduction\\nOriginally designed to generate text, scaled-up versions of language models (LMs) such as GPT [ 22,\\n23,1,20] and PaLM [ 5] have been shown to be increasingly capable of performing an ever wider\\nrange of tasks requiring mathematical, symbolic, commonsense, and knowledge reasoning. It is',\n",
              "  'question': 'Given that scaled-up versions of language models such as GPT and PaLM have been shown to be increasingly capable of performing a wide range of tasks requiring mathematical, symbolic, commonsense, and knowledge reasoning, what makes them fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role?\\n\\n',\n",
              "  'answer': 'Language models are still conformed to token-level, left-to-right decision-making processes during inference, which makes them unable to perform exploration, strategic lookahead, or make decisions where initial choices play a pivotal role.',\n",
              "  'source_doc': 'Tree of Thoughts_  Deliberate Problem Solving with Large Language Models.pdf'},\n",
              " {'context': 'out of memory on an A100 GPU before 64K, and FlashAttention is still 2\\x02more eï¬ƒcient than Linformer.\\n5 Limitations and Future Directions\\nWe discuss limitations of our approach and future directions. Related work is given in Appendix A.\\nCompiling to CUDA. Our current approach to building IO-aware implementations of attention requires\\nwriting a new CUDA kernel for each new attention implementation. This requires writing the attention\\nalgorithm in a considerably lower-level language than PyTorch, and requires signiï¬cant engineering eï¬€ort.\\nImplementations may also not be transferrable across GPU architectures. These limitations suggest the\\nneed for a method that supports writing attention algorithms in a high-level language (e.g., PyTorch), and\\ncompiling to IO-aware implementations in CUDAâ€”similar to eï¬€orts such as Halide in image processing [ 70].\\nIO-Aware Deep Learning. We believe that the IO-aware approach can extend beyond attention.\\nAttention is the most memory-intensive computation in Transformers, but every layer in a deep network\\ntouches GPU HBM. We hope our work inspires IO-aware implementations of additional modules. We discuss\\nthese potential extensions in Appendix D.\\nMulti-GPU IO-Aware Methods. Our IO-aware implementation of attention is optimal within con-\\nstants for computing attention on a single GPU. However, the attention computation may be parallelizable\\nacross multiple GPUs [ 72]. Using multiple GPUs adds an additional layer to IO analysisâ€”accounting for\\ndata transfer between GPUs. We hope our work inspires future work in this direction.\\nAcknowledgments\\nOur implementation uses Apexâ€™s FMHA code ( https://github.com/NVIDIA/apex/tree/master/apex/\\ncontrib/csrc/fmha ) as a starting point. We thank Young-Jun Ko for the in-depth explanation of his FMHA\\nimplementation and for his thoughtful answers to our questions about CUDA. We thank Sabri Eyuboglu,\\nMegan Leszczynski, Laurel Orr, Yuhuai Wu, Beidi Chen, and Xun Huang for their constructive feedback and',\n",
              "  'question': 'Given an A100 GPU with 64K of HBM, what is the most memory-efficient way to implement attention in a deep learning model?\\n\\n',\n",
              "  'answer': 'The most memory-efficient way to implement attention in a deep learning model on an A100 GPU with 64K of HBM is FlashAttention, which is 2^more efficient than Linformer.',\n",
              "  'source_doc': 'FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf'},\n",
              " {'context': 'In practice, DSPy signatures can be expressed with a shorthand notation like question -> answer ,\\nso that line 1 in the following is a complete DSPy program for a basic question-answering system\\n(with line 2 illustrating usage and line 3 the response when GPT-3.5 is the LM):\\n1qa = dspy.Predict(\"question -> answer\")\\n2qa(question=\"Where is Guaran Â´Ä± spoken?\")\\n3# Out: Prediction(answer=â€™Guaran Â´Ä± is spoken mainly in South America.â€™)\\nIn the shorthand notation, each fieldâ€™s name indicates the semantic role that the input (or output)\\nfield plays in the transformation. DSPy will parse this notation and expand the field names into\\nmeaningful instructions for the LM, so that english document -> french translation would\\nprompt for English to French translation. When needed, DSPy offers more advanced programming\\ninterfaces for expressing more explicit constraints on signatures (Appendix A).\\n3.2 P ARAMETERIZED &TEMPLATED MODULES CAN ABSTRACT PROMPTING TECHNIQUES\\nAkin to type signatures in programming languages, DSPy signatures simply define an interface and\\nprovide type-like hints on the expected behavior. To use a signature, we must declare a module with\\nthat signature, like we instantiated a Predict module above. A module declaration like this returns\\nafunction having that signature.\\nThePredict Module The core module for working with signatures in DSPy is Predict (simplified\\npseudocode in Appendix D.1). Internally, Predict stores the supplied signature, an optional LM to\\nuse (initially None , but otherwise overrides the default LM for this module), and a list of demon-\\nstrations for prompting (initially empty). Like layers in PyTorch, the instantiated module behaves as\\na callable function: it takes in keyword arguments corresponding to the signature input fields (e.g.,\\nquestion ), formats a prompt to implement the signature and includes the appropriate demonstra-\\ntions, calls the LM, and parses the output fields. When Predict detects itâ€™s being used in compile',\n",
              "  'question': 'In what way does DSPy signatures abstract prompting techniques?\\n\\n',\n",
              "  'answer': 'DSPy signatures abstract prompting techniques by defining an interface and providing type-like hints on the expected behavior. To use a signature, we must declare a module with that signature, like we instantiated a Predict module above. A module declaration like this returns a function having that signature. The instantiated module behaves as a callable function: it takes in keyword arguments corresponding to the signature input fields (e.g., question ), formats a prompt to implement the signature and includes the appropriate demonstrations, calls the LM, and parses the output fields. This allows for more explicit constraints on signatures and prompts to be expressed, making it easier to work with LLMs.',\n",
              "  'source_doc': 'DSPy_  Compiling Declarative Language Model Calls into Self-Improving Pipelines.pdf'},\n",
              " {'context': 'than those in the prompt. This approach entails a two-fold process: a top-down decomposition of the\\nproblem and a bottom-up resolution generation. Our empirical ï¬ndings, which encompass symbolic\\nmanipulation, compositional generalization, and mathematical reasoning, reveal that least-to-most\\nprompting signiï¬cantly surpasses standard prompting and chain-of-thought prompting.\\nIn general, prompting might not be the optimal method for teaching reasoning skills to large lan-\\nguage models. Prompting can be viewed as a unidirectional communication form in which we in-\\nstruct a language model without considering its feedback. A natural progression would be to evolve\\nprompting into fully bidirectional conversations, enabling immediate feedback to language models,\\nthereby facilitating more efï¬cient and effective learning. The least-to-most prompting technique\\nrepresents a stride towards instructing language models through such bidirectional interactions.\\nACKNOWLEDGEMENT\\nWe sincerely thank Xinyun Chen, Xinying Song, Jeff Dean, Zoubin Ghahramani, Fernando Pereira,\\nJacob Devlin, and Pete Shaw for sharing their valuable knowledge and advice during our discus-\\nsions. Their expertise greatly improved the quality of our work. Additionally, we are grateful to\\nthe anonymous reviewers for their careful review and helpful suggestions, which helped shape our\\nmanuscript into its ï¬nal form.\\nREFERENC',\n",
              "  'question': 'How does the least-to-most prompting technique represent a stride towards instructing language models through bidirectional interactions?\\n\\n',\n",
              "  'answer': 'The least-to-most prompting technique represents a stride towards instructing language models through bidirectional interactions because it allows for immediate feedback to language models, thereby facilitating more efficient and effective learning. This approach entails a two-fold process: a top-down decomposition of the problem and a bottom-up resolution generation. The empirical findings reveal that least-to-most prompting signiï¬cantly surpasses standard prompting and chain-of-thought prompting, indicating that this technique is more effective in teaching reasoning skills to large language models.',\n",
              "  'source_doc': 'Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.pdf'},\n",
              " {'context': 'data sequences as follows: with (small) probability Î·, we replace a sequence . . . , x iâˆ’1, xi, xi+1, . . .\\nwithxiâˆ’1, xi, xâ€²\\ni,<backspace> , xi+1, . . ., where xâ€²\\niis chosen randomly from the vocabulary. How-\\never, we keep the action at position iasxi+1, with the result that the overall MDP is augmented with\\na stochastic dynamics: with probability Î·a random token is inserted, instead of the chosen action.\\nWe also apply this to sequences which exceed the context length: the action is kept the same but the\\nnext token is forced to be the <eos> token. This introduces bias, as the policy learns to match the\\ndata distribution under a slightly different MDP than generation takes place in. In practice however, it\\nleads to improved performance compared to the policy learning with no examples of <backspace> .\\nAlgorithm 1: Training an autoregressive model against a SequenceMatch objective\\nInput : Dataset Dof data sequences, gradient-based optimizer step , number of train steps\\nntrain, parameters Î±, Î², Î³, Ï• , sampling interval ksample , fixed context length T\\nAdd noise and process data sequences with algorithm A to form new effective trajectories\\nInitialize buffer Bof model sequences; Initialize autoregressive policy â„“Î¸(Â·|s)\\nforkinntraindo\\nifkmod ksample = 0then\\nPopulate Bwith trajectories T âˆ¼â„“Î¸; Process added sequences with algorithm A\\nRemove oldest model sequences from B\\nend\\nSample dataset trajectories Tdataâˆ¼ D and model trajectories Tmodelâˆ¼ B\\nCompute g=âˆ‡Î¸Ë†J(â„“Î¸, Î±, Î³,Tdata,Tmodel)and update Î¸viastep using gradient g\\nend\\n5 Related Work\\nText Degeneration in Large Language Models\\nIn natural language processing (NLP) the phenomenon of text degeneration can occur, when a\\nlanguage model produces repetitive or nonsensical sequences [ 15]. Many explanations have been\\nproposed to explain this phenomenon [ 10,37]; a leading theory is that the large vocabulary size\\ninduces the model to over-estimate the probability of out-of-distribution (OOD) tokens. Once these',\n",
              "  'question': 'In Algorithm 1, how does the introduction of bias through the stochastic dynamics improve the performance of the autoregressive model compared to policy learning with no examples of <backspace>?\\n\\n',\n",
              "  'answer': 'The introduction of bias through the stochastic dynamics in Algorithm 1 leads to improved performance of the autoregressive model compared to policy learning with no examples of <backspace> because it allows the policy to learn to match the data distribution under a slightly different MDP than generation takes place in. This introduces bias, which helps the policy to better capture the structure of the data and generate more coherent sequences. In practice, this leads to improved performance compared to the policy learning with no examples of <backspace>.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': '11Preprint\\ndiscussions and feedback. We thank Giuseppe Attanasio for his public L ATEX GitHub-style Python\\ncode formatting gist.6\\nThis work was partially supported by IBM as a founding member of the Stanford Institute for\\nHuman-Centered Artificial Intelligence (HAI), Oracle, Virtusa, and Cigna Healthcare. It was also\\npartially supported by an HAI Azure compute grant. This research was supported in part by affiliate\\nmembers and other supporters of the Stanford DAWN projectâ€“Facebook, Google, and VMwareâ€”as\\nwell as the NSF under CAREER grant CNS-1651570. Any opinions, findings, and conclusions or\\nrecommendations expressed in this material are those of the authors and do not necessarily reflect\\nthe views of the National Science Foundation. Omar Khattab is supported by the Apple Scholars in\\nAI/ML fellowship.\\n\\\\usepackage[pdftex]{graphicx} ...\\n\\\\includegraphics[width=0.8\\\\linewidth]{myfile.pdf}',\n",
              "  'question': 'Given the context, what is the purpose of the research presented in the preprint?\\n\\n',\n",
              "  'answer': 'The purpose of the research presented in the preprint is to develop a deep learning model for image classification using a variant of the ResNet architecture. The model is trained on a large dataset of medical images and achieves state-of-the-art performance on several benchmark tasks. The research is supported by multiple funding sources, including IBM, Oracle, Virtusa, Cigna Healthcare, and the National Science Foundation.',\n",
              "  'source_doc': 'DSPy_  Compiling Declarative Language Model Calls into Self-Improving Pipelines.pdf'},\n",
              " {'context': 'language modeling datasets (Section 4.3). Finally,\\nwe investigate how the ability to learn using tools\\nis affected by model size (Section 4.4).\\n4.1 Experimental Setup\\nDataset Generation Throughout all of our ex-\\nperiments, we use a subset of CCNet (Wenzek et al.,\\n2020) as our language modeling dataset Cand GPT-\\nJ (Wang and Komatsuzaki, 2021) as our language\\nmodelM. To reduce the computational cost of\\nannotatingCwith API calls, we deï¬ne heuristics\\nfor some APIs to get a subset of Cfor which API\\ncalls are more likely to be helpful than for an av-\\nerage text. For example, we only consider texts\\nfor the calculator tool if they contain at least three\\nnumbers. Details of the heuristics used are given inAPI Name Example Input Example Output\\nQuestion Answering Where was the Knights\\nof Columbus founded?New Haven, Connecticut\\nWikipedia Search Fishing Reel Types Spin ï¬shing > Spin ï¬shing is distinguished between ï¬‚y ï¬shing and bait\\ncast ï¬shing by the type of rod and reel used. There are two types of reels\\nused when spin ï¬shing, the open faced reel and the closed faced reel.\\nCalculator 27 + 4 * 2 35\\nCalendar \" Today is Monday, January 30, 2023.\\nMachine Translation sÃ»retÃ© nuclÃ©aire nuclear safety\\nTable 1: Examples of inputs and outputs for all APIs used.\\nNumber of Examples\\nAPI \\x1cf= 0:5\\x1cf= 1:0\\x1cf= 2:0\\nQuestion Answering 51,987 18,526 5,135\\nWikipedia Search 207,241 60,974 13,944\\nCalculator 3,680 994 138\\nCalendar 61,811 20,587 3,007\\nMachine Translation 3,156 1,034 229\\nTable 2: Number of examples with API calls in C\\x03for\\ndifferent values of our ï¬ltering threshold \\x1cf.\\nAppendix A. For obtaining C\\x03fromC, we perform\\nall steps described in Section 2 and additionally\\nï¬lter out all examples for which all API calls were\\neliminated in the ï¬ltering step.4For the weighting\\nfunction, we use\\nwt=~wtP\\ns2N~wswith ~wt= max(0;1\\x000:2\\x01t)\\nto make sure that API calls happen close to where\\nthe information provided by the API is actually\\nhelpful for the model. The thresholds \\x1csand\\x1cfare',\n",
              "  'question': 'What is the weighting function used in the experimental setup for filtering out API calls and ensuring their usefulness for the language model?\\n\\n',\n",
              "  'answer': 'The weighting function used in the experimental setup for filtering out API calls and ensuring their usefulness for the language model is wt=~wtPs2N~wswith ~wt= max(0;1mond0:2hendt), where ~wt= max(0;1mond0:2hendt) is used to make sure that API calls happen close to where the information provided by the API is actually helpful for the model.',\n",
              "  'source_doc': 'Toolformer_ Language Models Can Teach Themselves to Use Tools.pdf'},\n",
              " {'context': 'the pass@kmetric, where kcode samples are generated\\nper problem, a problem is considered solved if any sampleEvaluating Large Language Models Trained on Code\\nFigure 2. Three example problems from the HumanEval dataset, where the probabilities that a single sample from Codex-12B passes unit\\ntests are 0.9, 0.17, and 0.005. The prompt provided to the model is shown with a white background, and a successful model-generated\\ncompletion is shown in a yellow background. Though not a guarantee for problem novelty, all problems were hand-written and not\\nprogrammatically copied from existing sources. Random problems and samples can be found in Appendix B.\\npasses the unit tests, and the total fraction of problems\\nsolved is reported. However, computing pass@ kin this\\nway can have high variance. Instead, to evaluate pass@ k,\\nwe generate n\\x15ksamples per task (in this paper, we\\nusen= 200 andk\\x14100), count the number of correct\\nsamplesc\\x14nwhich pass unit tests, and calculate the\\nunbiased estimator\\npass@k:=E\\nProblems\"\\n1\\x00\\x00n\\x00c\\nk\\x01\\n\\x00n\\nk\\x01#\\n(1)\\nCalculating this estimator directly results in very large num-\\nbers and numerical instability. In Figure 3, we include a\\nnumerically stable numpy implementation that simpliï¬es\\nthe expression and evaluates the product term-by-term. One\\nmay be tempted to estimate pass@ kwith1\\x00(1\\x00^p)kwhere\\n^pis the empirical estimate of pass@1, but we show that it is\\nbiased in Appendix A.def pass_at_k(n, c, k):\\n\"\"\"\\n:param n: total number of samples\\n:param c: number of correct samples\\n:param k: k in pass@$k$\\n\"\"\"\\nifn - c < k: return 1.0\\nreturn 1.0 - np.prod(1.0 - k /\\nnp.arange(n - c + 1, n + 1))\\nFigure 3. A numerically stable script for calculating an unbiased\\nestimate of pass@ k.\\nLater, we provide evidence that BLEU score may not be\\na reliable indicator of functional correctness by showing\\nthat functionally inequivalent programs generated by our\\nmodel (which are guaranteed to disagree with the reference\\nsolution on some input) often have higher BLEU scores than',\n",
              "  'question': 'Given that pass@k is calculated as the unbiased estimator of the fraction of problems solved by a language model, where k is the number of correct samples that pass unit tests, and n is the total number of samples generated, what is the formula for calculating pass@k?\\n\\n',\n",
              "  'answer': 'The formula for calculating pass@k is:\\n\\npass@k = 1 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))\\n\\nwhere n is the total number of samples generated, c is the number of correct samples that pass unit tests, and k is the number of correct samples that pass unit tests.',\n",
              "  'source_doc': 'Evaluating Large Language Models Trained on Code.pdf'},\n",
              " {'context': 'â€¢The ï¬ndings of this study lead to a retrieval-augmented language model, called Atlas, that exhibits\\nfew-shot abilities that emerge at lower scale than standard LLM.\\nâ€¢We provide an exploration of ï¬ne-tuning strategies to eï¬ƒciently adapt both the retriever and the\\nlanguage model to the task at hand.\\nâ€¢Thorough downstream experiments in few-shot settings, demonstrating state-of-the-art results on\\nfew-shot NaturalQuestions (+2.8%), TriviaQA (+3.3%), FEVER (+5.1%), and results on par or\\nstronger than models with 15 Ã—more parameters on MMLU.\\nâ€¢Experiments investigating full-dataset ï¬netuning, setting new state-of-the-art results in NaturalQues-\\ntions (+8.1%), TriviaQA (+9.3%) and 5 KILT Tasks.\\nâ€¢Experiments demonstrating the updatability and interpretability characteristics of Atlas.\\nâ€¢Experiments demonstrating that a compressed index using product quantisation achieves comparable\\nperformance as an uncompressed index while resulting in a 5x memory reduction.\\nOur code, pretrained Atlascheckpoints, and various supporting data are available at https://github.com/\\nfacebookresearch/atlas\\n2Task Query Output\\nFact Checking Bermuda Triangle is in the western part of the Hi-\\nmalayas.False\\nQuestion Answering who is playing the halftime show at super bowl 2016 Coldplay\\nEntity Linking NTFS-3G is an open source <E>cross-platform</E>\\nimplementation of the Microsoft Windows NTFS ï¬le\\nsystem with read-write support.Cross-platform software\\nFigure 2: Examples of query and output pairs for diï¬€erent tasks from KILT.\\n2 Method\\nOur approach follows the text-to-text framework (Raï¬€el et al., 2019). This means that all the tasks are\\nframed as follows: the system gets a text query as input, and generates a text output . For example, in the case\\nof question answering, the query corresponds to the question and the model needs to generate the answer.\\nIn the case of classiï¬cation tasks, the query corresponds to the textual input, and the model generates the',\n",
              "  'question': 'In few-shot settings, how does the Atlas language model exhibit few-shot abilities that emerge at lower scale than standard LLMs?\\n\\n',\n",
              "  'answer': 'The Atlas language model exhibits few-shot abilities by utilizing a retrieval-augmented approach where a retrieval system is used to retrieve relevant information from a large corpus of text. This allows the model to leverage the information in the corpus to generate accurate responses to queries, even when it has limited training data on the specific task at hand. Additionally, the model uses a combination of fine-tuning strategies to adapt both the retriever and the language model to the task at hand, further enhancing its ability to perform well in few-shot settings.',\n",
              "  'source_doc': 'Atlas_  Few-shot Learning with Retrieval Augmented Language Models.pdf'},\n",
              " {'context': 'information for a given query, even if it improves over the baseline. We also note that in our\\nexperiments we have only addressed hallucinations in the form of directly stated factual inaccuracies.\\nHowever, hallucinations could come in other forms, such as during incorrect reasoning steps, as part\\nof opinions, etc. We also note that the generations CoVe produces come with verifications which,\\nif viewed by the user, add more interpretability to its decisions, but come at the cost of increased\\ncomputational expense due to generating more tokens in the output, similar to other reasoning\\nmethods such as Chain-of-Thought.\\nOur method seeks to make a large language model produce improved responses by spending more\\ntime deliberating to identify its own mistakes. While we have shown this gives clear improvements,\\nthe upper bound to the improvement is clearly limited by the overall capabilities of the model, e.g. in\\nidentifying and knowing what it knows. In this regard, an orthogonal line of research, as discussed in\\nsection 2 is the use of external tools by language models, to gain further information beyond what is\\nstored in its weights. While we do not explore that avenue in this work those techniques would likely\\nbe fruitful to combine with the findings here.\\n9REFERENCES\\nLeonard Adolphs, Kurt Shuster, Jack Urbanek, Arthur Szlam, and Jason Weston. Reason first, then\\nrespond: Modular generation for knowledge-infused dialogue. arXiv preprint arXiv:2111.05204 ,\\n2021.\\nAyush Agrawal, Lester Mackey, and Adam Tauman Kalai. Do language models know when theyâ€™re\\nhallucinating references? arXiv preprint arXiv:2305.18248 , 2023.\\nI Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham\\nNeubig, Pengfei Liu, et al. Factool: Factuality detection in generative aiâ€“a tool augmented\\nframework for multi-task and multi-domain scenarios. arXiv preprint arXiv:2307.13528 , 2023a.',\n",
              "  'question': 'What limitations does the use of external tools by language models have in gaining further information beyond what is stored in their weights?\\n\\n',\n",
              "  'answer': \"The upper bound to the improvement of language models using external tools is clearly limited by the overall capabilities of the model, such as identifying and knowing what it knows. While external tools may provide additional information, their effectiveness is dependent on the model's ability to integrate and utilize the information provided. Additionally, the increased computational expense due to generating more tokens in the output and the potential for incorrect reasoning steps must also be considered.\",\n",
              "  'source_doc': 'Chain-of-Verification Reduces Hallucination in Large Language Models.pdf'},\n",
              " {'context': 'performance gains for more-complicated prob-\\nlems. For instance, for GSM8K (the dataset\\nwith the lowest baseline performance), perfor-\\nmance more than doubled for the largest GPT\\nand PaLM models. On the other hand, for Sin-\\ngleOp, the easiest subset of MAWPS which only\\nrequires a single step to solve, performance im-\\nprovements were either negative or very small\\n(see Appendix Table 3).\\nThird, chain-of-thought prompting via GPT-3\\n175B and PaLM 540B compares favorably to\\nprior state of the art, which typically ï¬netunes a\\ntask-speciï¬c model on a labeled training dataset.\\nFigure 4 shows how PaLM 540B uses chain-of-\\nthought prompting to achieve new state of the art\\non GSM8K, SV AMP, and MAWPS (though note\\nthat standard prompting already passed the prior\\nbest for SV AMP). On the other two datasets,\\nAQuA and ASDiv, PaLM with chain-of-thought\\nprompting reaches within 2% of the state of the\\nart (Appendix Table 2).\\nTo better understand why chain-of-thought\\nprompting works, we manually examined model-\\ngenerated chains of thought by LaMDA 137B\\nfor GSM8K. Of 50 random examples where the\\nmodel returned the correct ï¬nal answer, all of\\nthe generated chains of thought were also log-\\nically and mathematically correct except two\\nthat coincidentally arrived at the correct answer\\n(see Appendix D.1, and Table 8 for examples\\nof correct model-generated chains of thought).\\nWe also randomly examined 50 random sam-\\nples for which the model gave the wrong answer.\\nThe summary of this analysis is that 46% of the\\nchains of thought were almost correct, barring\\nminor mistakes (calculator error, symbol map-\\nping error, or one reasoning step missing), and that the other 54% of the chains of thought had major\\nerrors in semantic understanding or coherence (see Appendix D.2). To provide a small insight into\\nwhy scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors\\nmade by PaLM 62B and whether those errors were ï¬xed by scaling to PaLM 540B. The summary',\n",
              "  'question': 'Given that scaling improves chain-of-thought reasoning ability, what is the expected performance improvement for a larger GPT or PaLM model on a more-complicated problem?\\n\\n',\n",
              "  'answer': 'The expected performance improvement for a larger GPT or PaLM model on a more-complicated problem is likely to be higher than that of a smaller model, as the larger model has more parameters and can handle more complex reasoning tasks. However, the exact performance improvement will depend on the specific problem and the size of the model. For example, in the context of the GSM8K dataset, performance more than doubled for the largest GPT and PaLM models, indicating that scaling can significantly improve performance on more-complicated problems.',\n",
              "  'source_doc': 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.pdf'},\n",
              " {'context': 'credits and sources signiï¬cant amounts of renewable energy,\\nreducing its carbon footprint.7Compute consumption also\\nhas costs in the wider supply chain that can be quite con-\\ncentrated on certain regions.8Looking more globally and\\nlong-term, the compute demands of code generation could\\ngrow to be much larger than Codexâ€™s training if signiï¬cant\\ninference is used to tackle challenging problems.9\\n7.7. Legal implications\\nThere are several legal considerations related to generated\\ncode. To begin with, the training of AI systems on Internet\\ndata, such as public GitHub repositories, has previously\\nbeen identiï¬ed as an instance of â€œfair useâ€ (Oâ€™Keefe et al.,\\n2019).\\nOur preliminary research also ï¬nds that Codex models rarely\\ngenerate code that is identical to the contents of training\\ndata. Such occurrences were <0.1% in a study examining\\nthe frequency of code generations that appear to match code\\nsnippets in the training data (Ziegler, 2021). In these rare\\ninstances, the generated code consisted of common expres-\\nsions or conventions within the programming language that\\nappeared over and over again in the training data. We ï¬nd\\nthat, to the extent the generated code appears identical to\\nthe training data, it is due to the predictive weightings in the\\nmodel rather than retention and copying of speciï¬c code.\\nGenerated code is also responsive and customized to the\\nuserâ€™s input, and the user retains complete control over\\nediting and acceptance of the generated code. This can make\\ncode generation similar to auto-suggest or auto-completion\\n7Microsoft made a commitment in 2020 to shift to 100 per-\\ncent renewable energy supply in its buildings and data centers\\nby 2025. https://blogs.microsoft.com/blog/2020/01/16/microsoft-\\nwill-be-carbon-negative-by-2030/ A full assessment of the envi-\\nronmental impact of compute use is impossible to conduct without\\ngrounding in context and making comparison to the counterfactual\\nimpacts of competing products or services. Such analysis is out of',\n",
              "  'question': \"What legal considerations are related to generated code and how does Codex's training model differ from retention and copying of specific code?\\n\\n\",\n",
              "  'answer': 'The legal considerations related to generated code include the use of \"fair use\" for training AI systems on Internet data, such as public GitHub repositories. Additionally, Codex models rarely generate code that is identical to the contents of training data, and such occurrences are due to predictive weightings in the model rather than retention and copying of specific code. Generated code is also responsive and customized to the user\\'s input, and the user retains complete control over editing and acceptance of the generated code.',\n",
              "  'source_doc': 'Evaluating Large Language Models Trained on Code.pdf'},\n",
              " {'context': 'aâ€²âˆˆAexpQâˆ—(s, aâ€²)andQâˆ—the corresponding supremum in Q. This allows elimination of Î¸\\nfrom the optimization process entirely, and completes the proof.\\nD Choices of divergence measures\\nD.1 f-divergences\\nWe recall that for any f-divergence with Df(P, Q) =Exâˆ¼Q[f(P(x)/Q(x))], we have the variational\\nform\\nDf(P, Q) = sup\\nÏ•{Exâˆ¼P[Ï•(x)]âˆ’Exâˆ¼Q[fâˆ—(Ï•(x)]},\\nwith the convex conjugate fâˆ—(y) = supx{xÂ·yâˆ’f(x)}) and a discriminator Ï•:X â†’ R. Opti-\\nmizing a model against an f-divergence other than the KL-divergence typically involves a difficult\\nmin-max optimization problem where we simultaneously improve the model and improve the dis-\\ncriminator Ï•. This is subject to unstable training [19, 16, 34, 12].\\nIn the main paper, we explain that we require a divergence\\ndÏˆ(ÏÎ¸, Ïdata) =Ïˆâˆ—(ÏÎ¸âˆ’Ïdata).\\nWith our choice of Ïˆ, we get that\\ndÏˆ(Ï, Ï data) = max\\nrâˆˆRÏˆEs,aâˆ¼Ïdata[Ï•(r(s, a))]âˆ’Es,aâˆ¼ÏÎ¸[r(s, a)]\\nWe can readily connect these to f-divergences. Recall that the variational formulation of the f-\\ndivergence is\\nDf(P, Q) = sup\\ng{Exâˆ¼P[g(x)]âˆ’Exâˆ¼Q[fâˆ—(g(x)]}, (16)\\nso we can see that the function Ï•we need is simply âˆ’fâˆ—(âˆ’x).\\nD.2 KL-divergence\\nNote that we define our divergence in the reverse fashion to the usual convention, so to obtain the\\ntypical forward KL under the expectation of the data, we must use the reverse-KL f-divergence, with\\nf(x) =xlogx. This gives Ï•(x) =âˆ’eâˆ’(x+1). However, since we can always shift an f-divergenceâ€™s\\nfby a constant multiple of (xâˆ’1)without changing the divergence (which should be clear from\\nobserving cancellations in the definition of the fdivergence), we shift by -1 and (after working\\nthrough the derivations) have a simpler Ï•(x) =âˆ’eâˆ’x.\\nD.3 Jenson-Shannon Divergence\\nThe Jenson-Shannon divergence has f(x) =âˆ’(x+ 1) log(x+1\\n2) +xlogx. This leads to Ï•(x) =\\nlog(2âˆ’eâˆ’x). This is an interesting Ï•because it is equal to âˆ’âˆ forx <âˆ’log 2 . Since the xin this\\n17case is the value of robtained from the modelâ€™s logits, it is certainly possible that the value may',\n",
              "  'question': 'How does the choice of divergence measure affect the optimization process in a generative model?\\n\\n',\n",
              "  'answer': 'The choice of divergence measure affects the optimization process in a generative model by determining the objective function that needs to be minimized. Different divergence measures require different optimization problems, which can be more or less difficult to solve. For example, the KL-divergence requires a min-max optimization problem, while the Jenson-Shannon divergence requires a simpler optimization problem. Additionally, the choice of divergence measure can affect the stability of the training process.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'Chloe Yee. Quite a handful of amazing Red Solo was shown at the Smithsonian in June 2009.\\nHave some of your favorite characters and locations be inspired by their design and art!\\n[Colourful Imagery samples] by Karen Moore. This series uses heart modeled Granny Smith\\ncruelly, deformed words, and her favorite turquoise character Digby Pie. Itâ€™s set in\\nthese specially designed Styrofoam turquoise silks, assembled by the construction company\\nat\\nA notorious protester convicted of wilfully promoting hatred against Muslims and\\ncriminally harassing a Muslim man and his family was sentenced Tuesday to nine months\\nin jail. Eric Brazau handed out a flyer that \"vilified Muslims and disparages their\\nreligion,\" Ontario court Judge S. Ford Clements said in February, when he found Brazau\\nguilty. Eric Brazau was convicted of willful promotion of hatred against Muslims and\\ncriminally harassing a Muslim family. ( CARLOS OSORIO / TORONTO STAR FILE PHOTO ) The\\ncase was far from being on the borderline between \"rough and tumble debate\" and hate\\nspeech, as Brazau had argued, Clements said in a College Park courtroom. Brazau handed\\nout the flyer, which contained many offensive references to Islam and Muslims, in August\\nand September 2012. While distributing it, Brazau sometimes yelled obscenities about\\nIslam \"in a tone of voice that suggested he was very angry and had little interest in\\ndebate,\" Clements said. Brazau had argued that he did not intend to promote hate speech;\\ninstead he wanted to stimulate debate about censorship, \"blasphemy laws\" and Sharia law,\\nClements said. Article Continued Below Clements disagreed.agos: It wasnâ€™t only when an anti-Muslim protester was convicted. Clements said 2017 was\\nunique because his term gives Brazau the opportunity to answer: \"Qaads ben Yehudi: Where\\nare the â€™deplorablesâ€™ and who are the â€™parallel struggles?â€™\" after the jihad. Clements\\nalso said he found Bansaris and Jolyon Friesen guilty of helping incite hatred and hate.',\n",
              "  'question': 'Given the context, what is the significance of the question \"Qaads ben Yehudi: Where are the \\'deplorables\\' and who are the \\'parallel struggles\\'?\" in the context of the anti-Muslim protester Eric Brazau\\'s conviction?\\n\\n',\n",
              "  'answer': 'The question \"Qaads ben Yehudi: Where are the \\'deplorables\\' and who are the \\'parallel struggles\\'?\" is significant in the context of the anti-Muslim protester Eric Brazau\\'s conviction because it is a reference to the jihad and the term \"deplorables,\" which was used by Donald Trump during the 2016 presidential campaign. Brazau\\'s conviction was unique because it gave him the opportunity to answer this question, which suggests that he may have a deeper understanding of the issues surrounding Islam and its relationship to the West. The question also raises the issue of \"parallel struggles,\" which may refer to the idea that there are multiple struggles taking place simultaneously, each with its own unique set of challenges and goals. Overall, the question suggests that there may be a more complex and nuanced understanding of the issues surrounding Islam and its relationship to the West, which goes beyond the surface-level debates and discussions that often take place in the public sphere.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'systems (Leike et al., 2018). The systems we work with here are still fairly limited, but they are\\namong the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiï¬cation, summarization, question-answering, creative writing, dialogue, and others.\\nOur approach to alignment research in this work is iterative: we are improving the alignment of\\ncurrent AI systems instead of focusing abstractly on aligning AI systems that donâ€™t yet exist. A\\ndisadvantage of this approach is that we are not directly facing alignment problems that occur only\\nwhen aligning superhuman systems (Bostrom, 2014). However, our approach does provides us with a\\nclear empirical feedback loop of what works and what does not. We believe that this feedback loop is\\nessential to reï¬ne our alignment techniques, and it forces us to keep pace with progress in machine\\nlearning. Moreover, the alignment technique we use here, RLHF, is an important building block in\\nseveral proposals to align superhuman systems (Leike et al., 2018; Irving et al., 2018; Christiano\\net al., 2018). For example, RLHF was a central method in recent work on summarizing books, a task\\nthat exhibits some of the difï¬culties of aligning superhuman AI systems as it is difï¬cult for humans\\nto evaluate directly (Wu et al., 2021).\\nFrom this work, we can draw lessons for alignment research more generally:\\n1.The cost of increasing model alignment is modest relative to pretraining. The cost\\nof collecting our data and the compute for training runs, including experimental runs\\nis a fraction of what was spent to train GPT-3: training our 175B SFT model requires\\n4.9 petaï¬‚ops/s-days and training our 175B PPO-ptx model requires 60 petaï¬‚ops/s-days,\\ncompared to 3,640 petaï¬‚ops/s-days for GPT-3 (Brown et al., 2020). At the same time,\\nour results show that RLHF is very effective at making language models more helpful to\\nusers, more so than a 100x model size increase. This suggests that right now increasing',\n",
              "  'question': 'How do the systems described in Leike et al. (2018) contribute to alignment research and what lessons can be drawn from their work?\\n\\n',\n",
              "  'answer': \"The systems described in Leike et al. (2018) contribute to alignment research by providing a clear empirical feedback loop of what works and what does not. Their approach is iterative, focusing on improving the alignment of current AI systems rather than abstractly aligning AI systems that don't yet exist. This allows them to keep pace with progress in machine learning and refine their alignment techniques. The alignment technique used in their work, RLHF, is an important building block in several proposals to align superhuman systems and has been effective at making language models more helpful to users. From their work, lessons can be drawn for alignment research more generally, including the cost of increasing model alignment being modest relative to pretraining and the effectiveness of RLHF in making language models more helpful to users.\",\n",
              "  'source_doc': 'Training language models to follow instructions with human feedback.pdf'},\n",
              " {'context': 'ercasing and removing articles, punctuation and\\nduplicated whitespace.\\nTechnical details. We initialize our models with\\nthe pretrained T5 models (Raffel et al., 2020), avail-\\nable in the HuggingFace Transformers library.4We\\nconsider two model sizes, base and large, contain-\\ning respectively 220M and 770M parameters. We\\nï¬ne-tune the models on each dataset independently,\\nusing Adam (Kingma and Ba, 2014) with a con-\\nstant learning rate of 10\\x004and a dropout rate of\\n10%. We train the model for 10k gradient steps,\\nwith a batch size of 64, using 64 Tesla V100 32Gb.\\nWe evaluate models every 500 steps and select the\\nbest one on the validation set based on the Exact\\nMatch score. During training on NaturalQuestions\\n4github.com/huggingface/transformersand SQuAD, we sample the target among the list\\nof answers, while for TriviaQA, we use the unique\\nhuman-generated answer. For TriviaQA, answers\\nin uppercase are normalized by converting all let-\\nters in lowercase except the ï¬rst letter of each word,\\nusing the title Python string method. For both\\ntraining and testing, we retrieve 100 passages (un-\\nless said otherwise), and truncate them to 250 word\\npieces. Following the results of Karpukhin et al.\\n(2020), passages are retrieved with DPR for NQ\\nand TriviaQA, and with BM25 for SQuAD. We\\ngenerate answers by using greedy decoding.\\nComparison to state-of-the-art. In table 1, we\\ncompare the results obtained by Fusion-in-Decoder\\nwith existing approaches for open domain ques-\\ntion answering. We observe that while conceptu-\\nally simple, this method outperforms existing work\\non the NaturalQuestion and TriviaQA benchmarks.\\nIn particular, generative models seem to perform\\nwell when evidence from multiple passages need to\\nbe aggregated, compared to extractive approaches.\\nOur method also performs better than other genera-\\ntive models, showing that scaling to large number\\nof passages and processing them jointly leads to\\nimprovement in accuracy. Second, we observe that',\n",
              "  'question': 'In natural language processing, what is the role of fine-tuning pretrained models for open domain question answering?\\n\\n',\n",
              "  'answer': \"Fine-tuning pretrained models for open domain question answering involves adapting pre-trained models to a specific task by adjusting their parameters and training them on the target dataset. This improves the models' performance on the target task, making them better at generating accurate answers to open domain questions. Fine-tuning can be done using various optimization algorithms and techniques, such as Adam and dropout, and can involve different types of models, such as generative and extractive models. Fine-tuning can also involve various techniques for retrieving and truncating passages, such as DPR and BM25, and for generating answers, such as greedy decoding. Fine-tuning has been shown to improve the performance of models on various benchmarks, such as NaturalQuestions, TriviaQA, and SQuAD, and to outperform other generative models.\",\n",
              "  'source_doc': 'Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering.pdf'},\n",
              " {'context': 'Ã²Ä®ÆœË¤Å”Ã²Å—Ã§Ã²Å”ÆœÆ“ÄµÄ®Ë¤ÄµÆ™Ë¤Æ€ÄÄµË¤Æ†ÄµÅ©Ë¤ÃŠÅ—Ã²Ê›Ë¤Ë¤É¾Ê›Ë¤GÄ®ÆœÅ—ÄµÃ­Å©Ã§Ã²Ë¤ÃŠÄ®Ã­Ë¤Ã²Æ…Å”Ä¦ÃŠÄ‘Ä®Ë¤Æ›ÄÃ²Ë¤Æ˜Ã²Ã§ÄÄ®Æ“Å–Å©Ã²Ë¤ÄµÆ™Ë¤Ã­ÄµÄ‘Ä®ÄˆË¤ÃŠË¤ÄÃŠÄ®Ã­ÅÅ¤ÃŠÄ®Ã­Ë¤É¿Ê›Ë¤\\x92Æ€Æ“Å¤Ã§ÄË¤Æ˜ÄµË¤ÃŠË¤ÅÅ¤ÄµÅ—Æ†Ë¤ÃŠÃ¦ÄµÅ©ÆœË¤ÃŠÄ®Ë¤ÃŠÅÆœÅ—ÄµÄ®ÃŠÅ©ÆœË™ÅË¤ÆšÄ‘Å—ÅÆœË¤Æ›Ä‘Ä­Ã²Ë¤Ä‘Ä®Ë¤ÅÅ”ÃŠÃ§Ã²Ë¤Ê€Ê›Ë¤#Ã²ÅÃ§Å—Ä‘Ã¦Ã²Ë¤ÃŠË¤ÅÆ“ÆœÅ©ÃŠÆœÆ“ÄµÄ®Ë¤Æ€ÄÃ²Å—Ã²Ë¤ÃŠË¤Æ€ÄµÄ­ÃŠÄ®Ë¤Å©ÅÃ²ÅË¤ÅÆ“ÄˆÄ®Ë¤Ä¦ÃŠÄ®ÄˆÅ©ÃŠÄˆÃ²Ë¤Æ˜ÄµË¤ÃŠÅ¿ÄµÆ“Ã­Ë¤Å©Ä®Æ€ÃŠÄ®Å¤Ã²Ã­Ë¤ÃŠÆœÅ¤Ã²Ä®ÆœÆ“ÄµÄ®Ë¤ÊÊ›Ë¤\\x9aÄÃ²Ë¤ÆšÄ‘Ä®ÃŠÄ¦Ë¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄË¤Ã²Æ…Å”Ä¦ÃŠÄ‘Ä®ÅË¤ÄÄµÆ€Ë¤Ã²Å¿Ã²Å—Æ†ÄµÄ®Ã²Ë¤ÄÃŠÅË¤Ã­Ä‘Æ™Ä‡Ã²Å—Ã²Ä®ÆœË¤Å”Ã²Å—Ã§Ã²Å”ÆœÆ“ÄµÄ®ÅË¤ÄµÆ™Ë¤ÄµÆœÄÃ²Å—ÅÉ¾Ê›Ë¤GÄ®ÆœÅ—ÄµÃ­Å©Ã§ÆœÆ“ÄµÄ®Ë¤Æ˜ÄµË¤ÃŠÄ®Ë¤Å©Ä®Å©ÅÅ©ÃŠÄ¦Ë¤ÅÃ²Ä¦Æ™ËŠÄÃ²Ä¦Å”Ë¤Ã¦ÄµÄµÄ£ÊœË¤Ä­Ã²Ä®ÆœÆ“ÄµÄ®Ä‘Ä®ÄˆË¤ÃŠË¤ÄÃŠÄ®Ã­ÅÅ¤ÃŠÄ®Ã­Ë¤ÃŠÅË¤ÃŠË¤Ä­Ã²Å¤ÃŠÅ”ÄÄµÅ—Ë¤Æ—ÄµÅ—Ë¤Ã²Ä­Ã¦Å—ÃŠÃ§Ä‘Ä®ÄˆË¤Ã§ÄÃŠÄ¦Ä¦Ã²Ä®ÄˆÃ²ÅÊ›Ë¤É¿Ê›Ë¤#Æ“ÅÃ§Å©ÅÅË¤Æ›ÄÃ²Ë¤Å©Ä®Ã²Æ…Å”Ã²Ã§Å¤Ã²Ã­Ë¤Æ›ÄÄ‘Ä®ÄˆÅË¤Ä¦Ã²ÃŠÅ—Ä®Ã²Ã­Ë¤ÆšÅ—ÄµÄ­Ë¤ÃŠÅÆœÅ—ÄµÄ®ÃŠÅ©Å¤ÅÊœË¤Ä‘Ä®Ã§Ä¦Å©Ã­Ä‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤ÅÄ­Ã²Ä¦Ä¦Ë¤ÄµÆ™Ë¤ÅÅ”ÃŠÃ§Ã²Ê›Ë¤Ê€Ê›Ë¤#Ã²ÅÃ§Å—Ä‘Ã¦Ã²Ë¤ÃŠË¤Æ€ÄµÄ­ÃŠÄ®Ë™ÅË¤Ã§Ä¦Ã²Å¿Ã²Å—Ë¤Æ˜ÃŠÃ§ÆœÆ“Ã§Ë¤Æ—ÄµÅ—Ë¤ÃŠÅ¿ÄµÆ“Ã­Ä‘Ä®ÄˆË¤Å©Ä®Æ€ÃŠÄ®Å¤Ã²Ã­Ë¤ÃŠÆœÅ¤Ã²Ä®ÆœÆ“ÄµÄ®Ë¤ÃŠÆœË¤ÃŠË¤Ã¦ÃŠÅ—Ê›Ë¤ÊÊ›Ë¤\\x1dÄµÄ®Å¤Ã²Ä­Å”Ä¦ÃŠÅ¤Ã²Ë¤ÄÄµÆ€Ë¤Ã­Ä‘Æ™Ä‡Ã²Å—Ã²Ä®ÆœË¤Å”Ã²Å—Ã§Ã²Å”ÆœÆ“ÄµÄ®ÅË¤ÄµÆ™Ë¤ÄµÄ®Ã²ÅÃ²Ä¦Æ™Ë¤Ã§ÃŠÄ®Ë¤ÅÄÃŠÅ”Ã²Ë¤ÄµÄ®Ã²Ë™ÅË¤Æ“Ã­Ã²Ä®ÆœÆ“Å¤Æ†Ê›Ê±ÃŠÊ²Ë¤GÄ®Å”Å©ÆœÊ±Ã¦Ê²Ë¤\\x89Ä¦ÃŠÄ®ÅÊ±Ã§Ê²Ë¤Â´ÄµÅ¤Ã²Å\\x01Ä®ÃŠÄ¦Æ†ÆÄ‘Ä®ÄˆË¤Ã²ÃŠÃ§ÄË¤Ã§ÄÄµÆ“Ã§Ã²Ë¤Ä‘Ä®Ë¤Ã­Ã²Å¤ÃŠÆ“Ä¦ÊË¤Ë¤\\x1dÄÄµÆ“Ã§Ã²Ë¤É¾ÊœË¤Æ€ÄÆ“Ä¦Ã²Ë¤Ä‘Ä®Ã§ÄµÅ—Å”ÄµÅ—ÃŠÆœÄ‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤Å—Ã²Å–Å©Ä‘Å—Ã²Ã­Ë¤Ã²Ä®Ã­Ë¤ÅÃ²Ä®Å¤Ã²Ä®Ã§Ã²ÅÊœË¤ÅÃ²Ã²Ä­ÅË¤Æ˜ÄµË¤Ä¦ÃŠÃ§Ä£Ë¤ÃŠË¤Ã§Ä¦Ã²ÃŠÅ—Ë¤Ã§ÄµÄ®Ä®Ã²Ã§ÆœÆ“ÄµÄ®Ë¤Ã¦Ã²Å¤Æ€Ã²Ã²Ä®Ë¤Æ›ÄÃ²Ë¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄÅË¤Ê³Ê›Ê›Ê›Ê´Ë¤\\x1dÄÄµÆ“Ã§Ã²Ë¤É¿Ë¤ÄµÆ™Ä‡Ã²Å—ÅË¤ÃŠÄ®Ë¤Ä‘Ä®Å¤Ã²Å—Ã²ÅÆœÄ‘Ä®ÄˆË¤Å”Ã²Å—ÅÅ”Ã²Ã§ÆœÆ“Å¿Ã²Ë¤Ã¦Æ†Ë¤Å©ÅÄ‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤Å—Ã²Å–Å©Ä‘Å—Ã²Ã­Ë¤Ã²Ä®Ã­Ë¤ÅÃ²Ä®Å¤Ã²Ä®Ã§Ã²ÅË¤Æ˜ÄµË¤Å”Å—Ã²ÅÃ²Ä®ÆœË¤ÃŠË¤ÅÃ²Ä¦Æ™ËŠÄÃ²Ä¦Å”Ë¤Ã¦ÄµÄµÄ£Ë™ÅË¤Ã§ÄµÄ®Å¤Ã²Ä®ÆœÊ›Ë¤GÆœË¤Ã§ÄµÄ®Ä®Ã²Ã§Å¤ÅË¤Æ›ÄÃ²Ë¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄÅË¤Æ€Æ“ÆœÄË¤Æ›ÄÃ²Ë¤Æ›ÄÃ²Ä­Ã²Ë¤ÄµÆ™Ë¤ÅÃ²Ä¦Æ™ËŠÄ‘Ä­Å”Å—ÄµÅ¿Ã²Ä­Ã²Ä®ÆœË¤ÃŠÄ®Ã­Ë¤Ã²Ä­Ã¦Å—ÃŠÃ§Ä‘Ä®ÄˆË¤Ã§ÄÃŠÄ¦Ä¦Ã²Ä®ÄˆÃ²ÅÊœË¤Ä­ÃŠÄ£Ä‘Ä®ÄˆË¤Æ—ÄµÅ—Ë¤ÃŠË¤Ã§ÄµÄÃ²Å—Ã²Ä®ÆœË¤Å”ÃŠÅÅÃŠÄˆÃ²Ê›Ë¤Ë¤Ê³Ê›Ê›Ê›Ê´Ë¤\\x9aÄÃ²Ë¤Ã¦Ã²ÅÆœË¤Ã§ÄÄµÆ“Ã§Ã²Ë¤Æ“ÅË¤É¿Ê›GÄ®Å”Å©Æœ\\x89Ä¦ÃŠÄ®Ë¤É¾Ë¤\\x89Ä¦ÃŠÄ®Ë¤É¿Ë¤Ë¤ÊŸÊŸ\\x89ÃŠÅÅÃŠÄˆÃ²É¾\\x89ÃŠÅÅÃŠÄˆÃ²É¿ÊŸÊŸÉ½Ê«Ê‚Ë¤Å¿ÄµÅ¤Ã²Å\\x89Ä¦ÃŠÄ®Ë¤É¾Ë¤Ë¤Ë¤ÊŸÊ›ÊŸÊ›É¾ÊŸÊ›É¿ÊŸÊŸÊ€Ê«Ê‚Ë¤Å¿ÄµÅ¤Ã²Å\\x89Ä¦ÃŠÄ®Ë¤Ê€ËÊ‚Ë¤Ë¤Ë¤8VH\\x03UHG\\x12JUHHQ\\x03WR\\x03VKRZ\\x03ILQDO\\x03FKRLFH',\n",
              "  'question': 'What is the chemical formula for magnesium oxide?\\n',\n",
              "  'answer': 'The chemical formula for magnesium oxide is MgO.',\n",
              "  'source_doc': 'Tree of Thoughts_  Deliberate Problem Solving with Large Language Models.pdf'},\n",
              " {'context': '25\\n30\\n34\\n39\\n44\\n49\\n54\\n59\\nj(Ar=64,A/primer=64,i,j)\\nWv\\n1\\n5\\n10\\n15\\n20\\n25\\n30\\n34\\n39\\n44\\n49\\n54\\n59\\njRandom GaussianFigure 4: Left and Middle: Normalized subspace similarity between the column vectors of Ar=64\\nfrom two random seeds, for both \\x01Wqand\\x01Wvin the 48-th layer. Right: the same heat-map\\nbetween the column vectors of two random Gaussian matrices. See Section H.1 for other layers.\\nhow â€œlargeâ€ is \\x01Wcomparing to its corresponding directions in W? This can shed light on the\\nunderlying mechanism for adapting pre-trained language models.\\nTo answer these questions, we project Wonto ther-dimensional subspace of \\x01Wby comput-\\ningU>WV>, withU/Vbeing the left/right singular-vector matrix of \\x01W. Then, we com-\\npare the Frobenius norm between kU>WV>kFandkWkF. As a comparison, we also compute\\nkU>WV>kFby replacing U;V with the top rsingular vectors of Wor a random matrix.\\nr= 4 r= 64\\n\\x01WqWq Random \\x01WqWq Random\\njjU>WqV>jjF= 0.32 21.67 0.02 1.90 37.71 0.33\\njjWqjjF= 61:95jj\\x01WqjjF= 6:91jj\\x01WqjjF= 3:57\\nTable 7: The Frobenius norm of U>WqV>whereUandVare the left/right top rsingular vector\\ndirections of either (1) \\x01Wq, (2)Wq, or (3) a random matrix. The weight matrices are taken from\\nthe 48th layer of GPT-3.\\nWe draw several conclusions from Table 7. First, \\x01Whas a stronger correlation with Wcompared\\nto a random matrix, indicating that \\x01Wampliï¬es some features that are already in W. Second,\\ninstead of repeating the top singular directions of W,\\x01Wonly ampliï¬es directions that are not\\nemphasized in W. Third, the ampliï¬cation factor is rather huge: 21:5\\x196:91=0:32forr= 4.\\nSee Section H.4 for why r= 64 has a smaller ampliï¬cation factor. We also provide a visualization\\nin Section H.3 for how the correlation changes as we include more top singular directions from Wq.\\nThis suggests that the low-rank adaptation matrix potentially ampliï¬es the important features for\\nspeciï¬c downstream tasks that were learned but not emphasized in the general pre-training model .\\n8 C ONCLUSION AND FUTURE WORK',\n",
              "  'question': 'To what extent does the adaptation matrix amplify the important features for specific downstream tasks?\\n\\n',\n",
              "  'answer': 'The adaptation matrix amplifies the important features for specific downstream tasks to a huge extent. The amplification factor is 21:57:91=0:32 for r= 4, which indicates that the matrix amplifies some features that are already in W but not emphasized in the general pre-training model. Additionally, the amplification factor is much smaller for r= 64, suggesting that the matrix potentially amplifies the important features for specific downstream tasks that were learned but not emphasized in the general pre-training model.',\n",
              "  'source_doc': 'LoRA_ Low-Rank Adaptation of Large Language Models.pdf'},\n",
              " {'context': 'of that of the normal generation.\\n0% 20% 40% 60% 80% 100%General quality (LLMZoo)General quality (FastChat)\\n45.8%29.5%\\n19.6%29.3%\\n34.5%41.2%Win Tie Lose\\nFigure 3: Win/tie/lose rates of SoT v.s. normal generation using â€œgeneralâ€ metrics from FastChat\\nand LLMZoo. SoT performs better than or equal to normal generation in around 60% cases.\\n3.2.2 Q UALITY BREAKDOWN : M ODELS\\nNext, we investigate how SoT performs on different models. We compute net win rates on all\\nmodels in Fig. 4. Again, we see that the two general metrics from FastChat and LLMZoo have\\ndifferent absolute values but similar rankings. In particular, both metrics agree that OpenChat-13B,\\nVicuna-7B V1.1, Claude, LLaMA2-Chat-13B have lownet win rates, whereas Vicuna-13B V1.3,\\nStableVicuna-13B, and UltraLM-13B have high net win rates.\\n-60% -40% -20% 0% 20%StableVicuna-13BUltraLM-13BVicuna-13B V1.3GPT-4LLaMA2-Chat-7BVicuna-33B V1.3Vicuna-7B V1.3ChatGPT-3.5LLaMA2-Chat-13BOpenChat-13BVicuna-7B V1.1Claude\\n(a) Metric: general quality (FastChat).\\n-40% -20% 0% 20% 40% 60%StableVicuna-13BUltraLM-13BVicuna-13B V1.3GPT-4LLaMA2-Chat-7BVicuna-33B V1.3Vicuna-7B V1.3ChatGPT-3.5LLaMA2-Chat-13BOpenChat-13BVicuna-7B V1.1Claude (b) Metric: general quality (LLMZoo).\\nFigure 4: Net win rates of SoT on different models.\\nWe investigate the answers in App. I.1.1, and summarize the key takeaways as follows. Some\\nmodels have low SoT quality as they cannot understand the skeleton and point-expanding prompts\\nwell. Some other models have low SoT quality as their normal answers already have good quality,\\nmaking it hard for SoT to beat them (e.g., Claude). For models that are able to understand the\\nSoT prompts, the answer quality is improved. We expect that further improving SoT prompts or\\nfine-tuning the models can make it easier for LLMs to understand the skeleton and point-expanding\\nprompts and ultimately result in better answer quality.\\n3.2.3 Q UALITY BREAKDOWN : QUESTION CATEGORIES',\n",
              "  'question': 'Based on the given context, what is the performance of SoT compared to normal generation on different models using the \"general quality\" metric from FastChat and LLMZoo?\\n\\n',\n",
              "  'answer': 'SoT performs better than or equal to normal generation in around 60% cases. The two general metrics from FastChat and LLMZoo have different absolute values but similar rankings. In particular, both metrics agree that OpenChat-13B, Vicuna-7B V1.1, Claude, LLaMA2-Chat-13B have low net win rates, whereas Vicuna-13B V1.3, StableVicuna-13B, and UltraLM-13B have high net win rates.',\n",
              "  'source_doc': 'Skeleton-of-Thought_ Large Language Models Can Do Parallel Decoding.pdf'},\n",
              " {'context': 'intermediate computation steps, also demonstrate improvement on multi-step computation problems.\\nIn contrast to these methods, ReAct performs more than just isolated, ï¬xed reasoning, and integrates\\nmodel actions and their corresponding observations into a coherent stream of inputs for the model to\\nreason more accurately and tackle tasks beyond reasoning (e.g. interactive decision making).\\nLanguage model for decision making The strong capability of LLMs has enabled them to perform\\ntasks beyond language generation, and it is becoming more popular to take advantage of LLMs as a\\npolicy model for decision making, especially in interactive environments. WebGPT (Nakano et al.,\\n2021) uses an LM to interact with web browsers, navigate through web pages, and infer answers to\\ncomplicated questions from ELI5 (Fan et al., 2019). In comparison to ReAct , WebGPT does not\\nexplicitly model the thinking and reasoning procedure, instead rely on expensive human feedback for\\nreinforcement learning. In conversation modeling, chatbots like BlenderBot (Shuster et al., 2022b)\\nand Sparrow (Glaese et al., 2022) and task-oriented dialogue systems like SimpleTOD (Hosseini-Asl\\net al., 2020) also train LMs to make decision about API calls. Unlike ReAct , they do not explicitly\\nconsider the reasoning procedure either, and also relies on expensive datasets and human feedback\\ncollections for policy learning. In contrast, ReAct learns a policy in a much cheaper way, since the\\ndecision making process only requires language description of the reasoning procedure.6\\nLLMS have also been increasingly employed in interactive and embodied environments for planning\\nand decision making. Perhaps most relevant to ReAct in this respect are SayCan (Ahn et al., 2022)\\nand Inner Monologue (Huang et al., 2022b), which use LLMs for robotic action planning and decision\\nmaking. In SayCan, LLMs were prompted to directly predict possible actions a robot can take, which',\n",
              "  'question': 'What makes ReAct different from other methods for multi-step computation problems?\\n\\n',\n",
              "  'answer': 'ReAct integrates model actions and their corresponding observations into a coherent stream of inputs for the model to reason more accurately and tackle tasks beyond reasoning, while other methods rely on expensive human feedback for reinforcement learning or do not explicitly consider the reasoning procedure.',\n",
              "  'source_doc': 'ReAct_ Synergizing Reasoning and Acting in Language Models.pdf'},\n",
              " {'context': 'Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-\\ntion Processing Systems , volume 30, 2017.\\n[36] Dirk Weissenborn, Oscar T Â¨ackstr Â¨om, and Jakob Uszkoreit. Scaling autoregressive video models.\\nInInternational Conference on Learning Representations , 2020.\\n12[37] Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston.\\nNeural text generation with unlikelihood training. In International Conference on Learning\\nRepresentations , 2019.\\n[38] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\\nMoi, Pierric Cistac, Tim Rault, R Â´emi Louf, Morgan Funtowicz, et al. Huggingfaceâ€™s transform-\\ners: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 , 2019.\\n13A Algorithm A\\nAlgorithm 2: Algorithm A: Pseudocode for converting action sequences to masked inputs\\nInput : Sequence of action inputs a1:L\\noutput : Sequence of labels, inputs, masks, position ids\\nyâˆˆ |V|L, xâˆˆ |V|L, mâˆˆ {0,1}LÃ—L, pâˆˆ[L]L\\nInitialize y, m, p to zero.\\nInitialize xtoa.\\nInitialize c= 0 // Copy Pointer\\nInitialize d= 0 // Deletion Pointer\\nfori= 0, . . . L do\\nm[i]â†m[max ( iâˆ’1,0)]\\nifa[i] =<backspace> then\\nm[i, c]â†0\\nm[i, d]â†0\\nm[i, i]â†1\\nx[i]â†x[c]\\np[i]â†p[c]\\ndâ†i\\ncâ†element of last nonzero element in m[i,0 :c], else 0.\\nend\\nelse\\nm[i, i]â†1\\ndâ†d+ 1\\ncâ†element of first nonzero element in m[i, c+ 1 : L], else 0.\\np[i]â†p[dâˆ’1] + 1\\nend\\nifi= 0then\\ncâ†0 // Special cases for initial steps\\ndâ†0\\nend\\nifi= 1then\\ncâ†0\\ndâ†1\\nend\\nend\\nIn algorithm A we give a method to convert a sequence of actions into a masked sequence of inputs\\nand corresponding labels, masks and position ids. Although it can also be implemented in a stack-\\nbased fashion, we write it as an imperative algorithm so it can be compiled with the jitoperation in\\nJAX or the compile operation in PyTorch. Recall that the idea is to replace a sequence of actions',\n",
              "  'question': 'Can Algorithm A be used to generate text by masking the input sequence?\\n\\n',\n",
              "  'answer': 'No, Algorithm A is designed to convert a sequence of actions into a masked sequence of inputs and corresponding labels, masks, and position ids. It does not provide a mechanism for generating text. However, there are other algorithms, such as those used in neural text generation, that can be used to generate text by training a model on a large corpus of text.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': \"tential correctness conditions, and then we assign it a score.\\nDepending on how the score is derived, the module may\\nconsult the LLM. Moreover, depending on the use case, the\\nscore may also be assigned by a human. Finally, use cases\\nsuch as sorting use simple local scoring functions.\\n4.4 Controller\\nThe Controller implements a specific strategy for select-\\ning thoughts from its GRS structure. It also selects what\\ntransformations should be applied to which thoughts, and\\nthen passes this information to the Prompter. It also decides\\nwhether the whole process should be finalized, or whether\\nthe next round of interaction with the LLM should be initi-\\nated. In our current design, this is dictated by the execution\\nplan specified in GoO.\\n4.5 GoO & GRS\\nThe user constructs a GoO instance, which prescribes the ex-\\necution plan of thought operations. GoO is a static structure\\nthat is constructed once, before the execution starts. Each\\noperation object knows its predecessor operations and suc-\\ncessor operations. Then, during the execution, an instance\\nof GoO maintains the continually updated information about\\nthe LLM reasoning process. This includes which operation\\nhas been executed so far, the states of all the generated LLM\\nthoughts, their validity and scores, and any other relevant\\ninformation.\\nThe above elements offer extensible APIs , enabling\\nstraightforward implementations of different prompting\\nschemes. The APIs are outlines in the green part of Fig-\\nure 3, and detailed in the documentation. We also provide\\nexamples of prompts used by these operations and a corre-\\nsponding GRS in the red part of Figure 3.\\n5 Example Use Cases\\nWe now describe several use cases of GoT.Goal: Build a prompt\\nto be sent to the LLMLegend Architecture overview\\nExample prompts and the Graph Reasoning State for the sorting use case (some examples within each prompt are omitted due to space constraints)\\nA prompt used byParser\\nGoal: Extract\\ninformation from\\nLLM's thought Goal: Assess the\",\n",
              "  'question': 'Given the context provided, what is the purpose of the GoO instance in the GoT architecture?\\n\\n',\n",
              "  'answer': 'The GoO instance in the GoT architecture prescribes the execution plan of thought operations and maintains information about the LLM reasoning process during its execution. It offers extensible APIs for implementing different prompting schemes and provides examples of prompts used by various operations and their corresponding GRS.',\n",
              "  'source_doc': 'Graph of Thoughts_ Solving Elaborate Problems with Large Language Models.pdf'},\n",
              " {'context': 'find there are instances of strong disagreement. As such, we highlight that model-based evaluation\\nwhile providing a cheap alternative to human-annotation also has its uncertainties.\\nWe augment our chatbot benchmark results with a qualitative analysis of Guanaco models. Our analy-\\nsis highlights success and failure cases that were not captured by the quantitative benchmarks.\\nWe release all model generations with human and GPT-4 annotations to facilitate further study. We\\nopen-source our codebase and CUDA kernels and integrate our methods into the Hugging Face\\ntransformers stack [ 64], making them easily accessible to all. We release a collection of adapters\\nfor 7/13/33/65B size models, trained on 8 different instruction following datasets, for a total of 32\\ndifferent open sourced, finetuned models.\\n2Figure 1: Different finetuning methods and their memory requirements. QLORAimproves over LoRA by\\nquantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes.\\n2 Background\\nBlock-wise k-bit Quantization Quantization is the process of discretizing an input from a rep-\\nresentation that holds more information to a representation with less information. It often means\\ntaking a data type with more bits and converting it to fewer bits, for example from 32-bit floats to\\n8-bit Integers. To ensure that the entire range of the low-bit data type is used, the input data type is\\ncommonly rescaled into the target data type range through normalization by the absolute maximum\\nof the input elements, which are usually structured as a tensor. For example, quantizing a 32-bit\\nFloating Point (FP32) tensor into a Int8 tensor with range [âˆ’127,127]:\\nXInt8=round\\x12127\\nabsmax (XFP32)XFP32\\x13\\n=round (cFP32Â·XFP32), (1)\\nwhere cis the quantization constant orquantization scale . Dequantization is the inverse:\\ndequant (cFP32,XInt8) =XInt8\\ncFP32=XFP32(2)\\nThe problem with this approach is that if a large magnitude value (i.e., an outlier) occurs in the input',\n",
              "  'question': 'Given the context, what is the difference between block-wise k-bit quantization and the method used in QLORA for quantizing transformer models?\\n\\n',\n",
              "  'answer': 'The difference between block-wise k-bit quantization and the method used in QLORA for quantizing transformer models is that QLORA quantizes the transformer model to 4-bit precision and uses paged optimizers to handle memory spikes, while block-wise k-bit quantization typically involves taking a data type with more bits and converting it to fewer bits, for example from 32-bit floats to 8-bit integers. To ensure that the entire range of the low-bit data type is used, the input data type is commonly rescaled into the target data type range through normalization by the absolute maximum of the input elements.',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': '[14] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural\\ninformation processing systems , 29, 2016.\\n[15] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural\\ntext degeneration. In International Conference on Learning Representations , 2019.\\n[16] Abdul Jabbar, Xi Li, and Bourahla Omar. A Survey on Generative Adversarial Networks:\\nVariants, Applications, and Training. arXiv:2006.05132 [cs] , June 2020.\\n11[17] Shaojie Jiang, Ruqing Zhang, Svitlana Vakulenko, and Maarten de Rijke. A simple contrastive\\nlearning objective for alleviating neural text degeneration. arXiv preprint arXiv:2205.02517 ,\\n2022.\\n[18] Ilya Kostrikov, Ofir Nachum, and Jonathan Tompson. Imitation learning via off-policy distribu-\\ntion matching. In International Conference on Learning Representations , 2019.\\n[19] Dohyun Kwon, Yeoneung Kim, Guido Mont Â´ufar, and Insoon Yang. Training Wasserstein GANs\\nwithout gradient penalties. arXiv:2110.14150 [cs, math] , October 2021.\\n[20] Yann LeCun, Sumit Chopra, Raia Hadsell, Marcâ€™Aurelio Ranzato, and Fu Jie Huang. A Tutorial\\non Energy-Based Learning, 2006.\\n[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International\\nConference on Learning Representations , 2019.\\n[22] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\\nWierstra, and Martin Riedmiller. Playing Atari with Deep Reinforcement Learning, 2013.\\n[23] Andrew Y Ng and Stuart Russell. Algorithms for inverse reinforcement learning. In In Proc.\\n17th International Conf. on Machine Learning . Citeseer, 2000.\\n[24] Krishna Pillutla, Lang Liu, John Thickstun, Sean Welleck, Swabha Swayamdipta, Rowan\\nZellers, Sewoong Oh, Yejin Choi, and Zaid Harchaoui. MAUVE scores for generative models:\\nTheory and practice. arXiv preprint arXiv:2212.14578 , 2022.\\n[25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.',\n",
              "  'question': 'In recent years, there has been a growing interest in generative adversarial networks (GANs) and their variants. What are some of the key advancements and applications of GANs, and how are they typically trained?\\n\\n',\n",
              "  'answer': \"Some key advancements in GANs include the use of Wasserstein GANs without gradient penalties, decoupled weight decay regularization, and contrastive learning objectives. GANs have been applied in various domains such as image synthesis, text generation, and style transfer. Typical training methods involve an adversarial process where a generator network generates new samples and a discriminator network distinguishes between real and fake samples. The loss function is typically based on a measure of the discriminator's ability to correctly classify the samples.\",\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'Today, Toyota announced changes in executivesâ€™ areas of responsibility, as well as\\npersonnel changes at the sub-executive managerial level. The most important change by\\nfar is the appointment of Akio Toyoda, the companyâ€™s CEO and grandson of founder Kiichiro\\nToyoda, as President of a new â€™EV Business Planningâ€™ department. Earlier this month,\\nwe reported-admittedly a little tongue-in-cheek-about Toyota announcing the creation of\\nan electric vehicle division and putting only 4 engineers on the project with the goal\\nto bring EVs to market by 2020. The move seems a lot more serious now that Akio Toyoda\\nis leading the effort, and several other executives, managers, and engineers have been\\nassigned new responsibilities in the electric vehicle planning department, including the\\nchief engineer of the Prius. At the executive level, the changes will be effective today,\\nwhile the managers were apparently put on the program throughout the month. You can see\\nthe full list of changes below. It appears to be a clear sign that Toyota is more serious\\nthan ever about electric vehicles and it is not simply investing in fuel cell hydrogen to\\ncomply to new fuel consumption standards. Changes to executivesâ€™ areas of responsibilityIn the electric car area, Toyota has moved more recently to give executive members\\nover-engineering a big lift. That includes John Dahl, president and CEO of Toyota Motor\\nCorp. Chicago, and also will likely include Jeff Gunderson, CEO and Chairman of Toyota\\nMotor Corp., and others like it. Weâ€™ll keep you updated when more information becomes\\navailable. Earlier this month, President of Toyota, Gene Munster, addressed a gathering\\nof executives in New York City. Hereâ€™s the transcript from his speech. Gene: I think\\nwe would be delighted to see our entire EV business expanded into electric vehicles\\nand weâ€™ve got great, fast-charging options in the market, weâ€™ve had the opportunity\\nfor years and we have a great quality product, in this case, and I think we are very',\n",
              "  'question': \"How does the appointment of Akio Toyoda as President of the EV Business Planning department affect Toyota's electric vehicle strategy?\\n\\n\",\n",
              "  'answer': \"The appointment of Akio Toyoda as President of the EV Business Planning department suggests that Toyota is taking its electric vehicle strategy more seriously. With Akio Toyoda leading the effort, Toyota has assigned several other executives, managers, and engineers new responsibilities in the electric vehicle planning department, including the chief engineer of the Prius. This indicates that Toyota is investing more resources and expertise into its electric vehicle division. Additionally, the fact that Toyota has moved more recently to give executive members over-engineering a big lift in the electric car area, including John Dahl, president and CEO of Toyota Motor Corp. Chicago, Jeff Gunderson, CEO and Chairman of Toyota Motor Corp., and others, suggests that Toyota is taking a more aggressive approach to developing and bringing electric vehicles to market. The transcript from Gene Munster's speech also indicates that Toyota is expanding its EV business and sees great, fast-charging options in the market. Overall, the appointment of Akio Toyoda as President of the EV Business Planning department is a clear sign that Toyota is more serious than ever about electric vehicles and is not simply investing in fuel cell hydrogen to comply with new fuel consumption standards.\",\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'assess its own predictions after each generated segment as an integral part of the generation output.\\nSELF-RAGfurther enables a customizable decoding algorithm to satisfy hard or soft constraints,\\nwhich are defined by reflection token predictions. In particular, our inference-time algorithm enables\\nus to (1) flexibly adjust retrieval frequency for different downstream applications and (2) customize\\nmodelsâ€™ behaviors to user preferences by leveraging reflection tokens through segment-level beam\\nsearch using the weighted linear sum of the reflection token probabilities as segment score.\\nEmpirical results on six tasks, including reasoning and long-form generation, demonstrate that SELF-\\nRAGsignificantly outperforms pre-trained and instruction-tuned LLMs that have more parameters and\\nwidely adopted RAG approaches with higher citation accuracy. In particular, SELF-RAGoutperforms\\nretrieval-augmented ChatGPT on four tasks, Llama2-chat (Touvron et al., 2023) and Alpaca (Dubois\\net al., 2023) on all tasks. Our analysis demonstrates the effectiveness of training and inference with\\nreflection tokens for overall performance improvements as well as test-time model customizations\\n(e.g., balancing the trade-off between citation previsions and completeness).\\n2 R ELATED WORK\\nRetrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) augments the input\\nspace of LMs with retrieved text passages (Guu et al., 2020; Lewis et al., 2020), leading to large\\nimprovements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram\\net al., 2023). A more recent work (Luo et al., 2023) instruction-tunes an LM with a fixed number\\n2Preprint.\\nof retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by few-\\nshot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only\\nonce at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation',\n",
              "  'question': 'How does SELF-RAG incorporate reflection tokens to enable customizable decoding algorithms and improve overall performance in comparison to pre-trained and instruction-tuned LLMs?\\n\\n',\n",
              "  'answer': \"SELF-RAG incorporates reflection tokens to enable customizable decoding algorithms by leveraging them through segment-level beam search using the weighted linear sum of the reflection token probabilities as segment score. This allows for flexible adjustments in retrieval frequency for different downstream applications and customizing models' behaviors to user preferences. In terms of performance improvements, SELF-RAG outperforms pre-trained and instruction-tuned LLMs that have more parameters and widely adopted RAG approaches with higher citation accuracy. Specifically, SELF-RAG outperforms retrieval-augmented ChatGPT on four tasks, Llama2-chat (Touvron et al., 2023) and Alpaca (Dubois et al., 2023) on all tasks. This demonstrates the effectiveness of training and inference with reflection tokens for overall performance improvements as well as test-time model customizations (e.g., balancing the trade-off between citation previsions and completeness).\",\n",
              "  'source_doc': 'Self-Rag_ Learning to Retrieve, Generate, and Critique Through Self-Reflection.pdf'},\n",
              " {'context': 'E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv\\npreprint arXiv:2302.13971 , 2023.\\n[58] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-\\ntask benchmark and analysis platform for natural language understanding. arXiv preprint\\narXiv:1804.07461 , 2018.\\n[59] Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct:\\nAligning language model with self generated instructions. arXiv preprint arXiv:2212.10560 ,\\n2022.\\n[60] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei, A. Arunkumar, A. Ashok, A. S.\\nDhanasekaran, A. Naik, D. Stap, et al. Super-naturalinstructions:generalization via declarative\\ninstructions on 1600+ tasks. In EMNLP , 2022.\\n[61] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S.\\nDhanasekaran, A. Arunkumar, D. Stap, et al. Super-naturalinstructions: Generalization via\\ndeclarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical\\nMethods in Natural Language Processing , pages 5085â€“5109, 2022.\\n[62] J. Wei, M. Bosma, V . Y . Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V . Le.\\nFinetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 , 2021.\\n[63] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V . Le, D. Zhou, et al.\\nChain-of-thought prompting elicits reasoning in large language models. In Advances in Neural\\nInformation Processing Systems , 2022.\\n[64] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf,\\nM. Funtowicz, et al. Huggingfaceâ€™s transformers: State-of-the-art natural language processing.\\narXiv preprint arXiv:1910.03771 , 2019.\\n[65] M. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, and L. Schmidt. Stable and\\nlow-precision training for large-scale vision-language models. arXiv preprint arXiv:2304.13013 ,\\n2023.',\n",
              "  'question': 'Can natural language models be trained to generate coherent and diverse responses to open-ended prompting without the need for explicit supervision or fine-tuning?\\n\\n',\n",
              "  'answer': 'Yes, natural language models can be trained to generate coherent and diverse responses to open-ended prompting without the need for explicit supervision or fine-tuning. This can be achieved through methods such as self-instructed learning, where the model is trained to align its outputs with self-generated instructions, or by using declarative instructions on a large number of tasks to promote generalization. Examples of such methods include the work by Wang et al. (2022) and the work by Wang et al. (2022).',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': 'learns to use tools in a novel way, which fulï¬lls the\\nfollowing desiderata:\\nâ€¢The use of tools should be learned in a\\nself-supervised way without requiring large\\namounts of human annotations . This is impor-arXiv:2302.04761v1  [cs.CL]  9 Feb 2023x1: i-1  = Pittsburgh is \\n             also known as \\n   xi: n = the Steel City x* = Pittsburgh is \\n        also known as \\n        [QA(What â€¦?  \\n        â†’ Steel City)]  \\n        the Steel City. ci1 = What other name is \\n         Pittsburgh known by? \\nci2 = Which country is \\n         Pittsburgh in? ri1 = Steel City \\nri2 = United States Li( ci1 â†’ Steel City )\\n < min( Li( ci1 â†’ Îµ), Li(Îµ))\\nLi( ci2 â†’ United States )\\n > min( Li( ci2 â†’ Îµ), Li(Îµ))1 \\nSample API Calls 2 \\nExecute API Calls 3 \\nFilter API Calls LM Dataset LM Dataset \\nwith API Calls Figure 2: Key steps in our approach, illustrated for a question answering tool: Given an input text x, we ï¬rst\\nsample a position iand corresponding API call candidates c1\\ni;c2\\ni;:::;ck\\ni. We then execute these API calls and\\nï¬lter out all calls which do not reduce the loss Liover the next tokens. All remaining API calls are interleaved\\nwith the original text, resulting in a new text x\\x03.\\ntant not only because of the costs associated\\nwith such annotations, but also because what\\nhumans ï¬nd useful may be different from\\nwhat a model ï¬nds useful.\\nâ€¢The LM should not lose any of its generality\\nand should be able to decide for itself when\\nandhow to use which tool. In contrast to\\nexisting approaches, this enables a much more\\ncomprehensive use of tools that is not tied to\\nspeciï¬c tasks.\\nOur approach for achieving these goals is based\\non the recent idea of using large LMs with in-\\ncontext learning (Brown et al., 2020) to generate\\nentire datasets from scratch (Schick and SchÃ¼tze,\\n2021b; Honovich et al., 2022; Wang et al., 2022):\\nGiven just a handful of human-written examples\\nof how an API can be used, we let a LM annotate\\na huge language modeling dataset with potential',\n",
              "  'question': 'Given a language model and a few human-written examples of how an API can be used, how can the language model be trained to generate entire datasets from scratch using in-context learning, without losing any of its generality and being able to decide for itself when and how to use which tool?\\n\\n',\n",
              "  'answer': 'Our approach for achieving these goals is based on the recent idea of using large language models with in-context learning (Brown et al., 2020) to generate entire datasets from scratch (Schick and SchÃ¼tze, 2021b; Honovich et al., 2022; Wang et al., 2022). Given just a handful of human-written examples of how an API can be used, we let a LM annotate a huge language modeling dataset with potential API calls and filter out all calls which do not reduce the loss over the next tokens. All remaining API calls are interleaved with the original text, resulting in a new text x^. This enables a much more comprehensive use of tools that is not tied to specific tasks and allows the LM to decide for itself when and how to use which tool.',\n",
              "  'source_doc': 'Toolformer_ Language Models Can Teach Themselves to Use Tools.pdf'},\n",
              " {'context': 'âˆ‚Liare\\nneeded, and not for 4-bit weightsâˆ‚E\\nâˆ‚W. However, the calculation ofâˆ‚E\\nâˆ‚Lientails the calculation ofâˆ‚X\\nâˆ‚W\\nwhich proceeds via equation (5) with dequantization from storage WNF4to computation data type\\nWBF16to calculate the derivativeâˆ‚X\\nâˆ‚Win BFloat16 precision.\\nTo summarize, QLORAhas one storage data type (usually 4-bit NormalFloat) and a computation\\ndata type (16-bit BrainFloat). We dequantize the storage data type to the computation data type\\nto perform the forward and backward pass, but we only compute weight gradients for the LoRA\\nparameters which use 16-bit BrainFloat.\\n4 QLoRA vs. Standard Finetuning\\nWe have discussed how QLoRA works and how it can significantly reduce the required memory for\\nfinetuning models. The main question now is whether QLoRA can perform as well as full-model\\nfinetuning. Furthermore, we want to analyze the components of QLoRA including the impact of\\nNormalFloat4 over standard Float4. The following sections will discuss the experiments that aimed\\nat answering these questions.\\n3https://docs.nvidia.com/cuda/cuda-c-programming-guide\\n5Experimental setup. We consider three architectures (encoder, encoder-decoder, and decoder only)\\nand compare QLoRA with 16-bit adapter-finetuning and with full-finetuning for models up to 3B. Our\\nevaluations include GLUE [ 58] with RoBERTa-large [ 38], Super-NaturalInstructions (TKInstruct)\\n[61] with T5 [ 49], and 5-shot MMLU [ 24] after finetuning LLaMA on Flan v2 [ 39] and Alpaca\\n[55]. To additionally study the advantages of NF4 over other 4-bit data types, we use the setup of\\nDettmers and Zettlemoyer [13] and measure post-quantization zero-shot accuracy and perplexity\\nacross different models (OPT [ 72], LLaMA [ 57], BLOOM [ 52], Pythia [ 7]) for model sizes 125m -\\n13B. We provide more details in the results section for each particular setup to make the results more\\nreadable. Full details in Appendix A.\\nQLoRA-AllQLoRA-FFN\\nQLoRA-AttentionAlpaca (ours)\\nStanford-Alpaca\\nModel6061626364RougeL\\nbits\\n4\\n16',\n",
              "  'question': 'How does QLoRA compare to full-model finetuning in terms of performance for models up to 3B?\\n\\nAnswer:\\n\\nQLoRA is a technique for reducing the memory requirements for finetuning models by using a 16-bit computation data type (BrainFloat) instead of the standard 32-bit data type (Float32). QLoRA was compared to full-model finetuning and 16-bit adapter-finetuning for models up to 3B on three different architectures (encoder, encoder-decoder, and decoder only). The evaluation included GLUE, Super-NaturalInstructions, and 5-shot MMLU. The results showed that QLoRA was able to achieve similar performance to full-model finetuning and outperformed 16-bit adapter-finetuning in some cases. However, the advantages of NF4 over other 4-bit data types were not significant in terms of post-quantization zero-shot accuracy and perplexity.',\n",
              "  'answer': '(your answer to the deep question)\\n\\nNow here is the context.\\n\\nContext: âˆ‚Liare\\nneeded, and not for 4-bit weightsâˆ‚E\\nâˆ‚W. However, the calculation ofâˆ‚E\\nâˆ‚Lientails the calculation ofâˆ‚X\\nâˆ‚W\\nwhich proceeds via equation (5) with dequantization from storage WNF4to computation data type\\nWBF16to calculate the derivativeâˆ‚X\\nâˆ‚Win BFloat16 precision.\\nTo summarize, QLORAhas one storage data type (usually 4-bit NormalFloat) and a computation\\ndata type (16-bit BrainFloat). We dequantize the storage data type to the computation data type\\nto perform the forward and backward pass, but we only compute weight gradients for the LoRA\\nparameters which use 16-bit BrainFloat.\\n4 QLoRA vs. Standard Finetuning\\nWe have discussed how QLoRA works and how it can significantly reduce the required memory for\\nfinetuning models. The main question now is whether QLoRA can perform as well as full-model\\nfinetuning. Furthermore, we want to analyze the components of QLoRA including the impact of\\nNormalFloat4 over standard Float4. The following sections will discuss the experiments that aimed\\nat answering these questions.\\n3https://docs.nvidia.com/cuda/cuda-c-programming-guide\\n5Experimental setup. We consider three architectures (encoder, encoder-decoder, and decoder only)\\nand compare QLoRA with 16-bit adapter-finetuning and with full-finetuning for models up to 3B. Our\\nevaluations include GLUE [ 58] with RoBERTa-large [ 38], Super-NaturalInstructions (TKInstruct)\\n[61] with T5 [ 49], and 5-shot MMLU [ 24] after finetuning LLaMA on Flan v2 [ 39] and Alpaca\\n[55]. To additionally study the advantages of NF4 over other 4-bit data types, we use the setup of\\nDettmers and Zettlemoyer [13] and measure post-quantization zero-shot accuracy and perplexity\\nacross different models (OPT [ 72], LLaMA [ 57], BLOOM [ 52], Pythia [ 7]) for model sizes 125m -\\n13B. We provide more details in the results section for each particular setup to make the results more\\nreadable. Full details in Appendix A.\\nQLoRA-AllQLoRA-FFN\\nQLoRA-AttentionAlpaca (ours)\\nStanford-Alpaca\\nModel6061626364RougeL\\nbits\\n4\\n16\\n\\nOutput::: [/INST] Deep question: How does QLoRA compare to full-model finetuning in terms of performance for models up to 3B?\\n\\nAnswer:\\n\\nQLoRA is a technique for reducing the memory requirements for finetuning models by using a 16-bit computation data type (BrainFloat) instead of the standard 32-bit data type (Float32). QLoRA was compared to full-model finetuning and 16-bit adapter-finetuning for models up to 3B on three different architectures (encoder, encoder-decoder, and decoder only). The evaluation included GLUE, Super-NaturalInstructions, and 5-shot MMLU. The results showed that QLoRA was able to achieve similar performance to full-model finetuning and outperformed 16-bit adapter-finetuning in some cases. However, the advantages of NF4 over other 4-bit data types were not significant in terms of post-quantization zero-shot accuracy and perplexity.',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': 'the intermediate values, resulting in Î˜Â¹ğ‘ğ‘‘ğ‘€\\x001Âºpasses over Q. Each pass loads Î˜Â¹ğ‘ğ‘‘Âºelements, which\\namounts to Î˜Â¹ğ‘2ğ‘‘2ğ‘€\\x001ÂºHBM accesses. We similarly prove that the backward pass of standard attention\\nrequiresÎ˜Â¹ğ‘ğ‘‘Â¸ğ‘2ÂºHBM accesses while the backward pass of FlashAttention requiresÎ˜Â¹ğ‘2ğ‘‘2ğ‘€\\x001Âº\\nHBM accesses (Appendix B).\\nWe prove a lower-bound: one cannot asymptotically improve on the number of HBM accesses for all\\nvalues ofğ‘€(the SRAM size) when computing exact attention.\\nProposition 3. Letğ‘be the sequence length, ğ‘‘be the head dimension, and ğ‘€be size of SRAM with\\nğ‘‘\\x14ğ‘€\\x14ğ‘ğ‘‘. There does not exist an algorithm to compute exact attention with ğ‘œÂ¹ğ‘2ğ‘‘2ğ‘€\\x001ÂºHBM accesses\\nfor allğ‘€in the rangeÂ»ğ‘‘\\x96ğ‘ğ‘‘Â¼.\\nThe proof relies on the fact that for ğ‘€= Î˜Â¹ğ‘ğ‘‘Âºany algorithm must perform Î©Â¹ğ‘2ğ‘‘2ğ‘€\\x001Âº= Î©Â¹ğ‘ğ‘‘Âº\\nHBM accesses. This type of lower bound over a subrange of ğ‘€is common in the streaming algorithms\\nliterature [ 88]. We leave proving parameterized complexity [ 27] lower bounds in terms of ğ‘€as exciting future\\nwork.\\nWe validate that the number of HBM accesses is the main determining factor of attention run-time.\\nIn Fig. 2 (left), we see that even though FlashAttention has higher FLOP count compared to standard\\nattention (due to recomputation in the backward pass), it has much fewer HBM accesses, resulting in much\\nfaster runtime. In Fig. 2 (middle), we vary the block size ğµğ‘ofFlashAttention , which results in diï¬€erent\\namounts of HBM accesses, and measure the runtime of the forward pass. As block size increases, the number\\nof HBM accesses decreases (as we make fewer passes over the input), and runtime decreases. For large enough\\nblock size (beyond 256), the runtime is then bottlenecked by other factors (e.g., arithmetic operations).\\nMoreover, larger block size will not ï¬t into the small SRAM size.\\n3.3 Extension: Block-Sparse FlashAttention\\nWe extend FlashAttention to approximate attention: we propose block-sparse FlashAttention , whose',\n",
              "  'question': 'What is the main determining factor of attention run-time in FlashAttention and how does it compare to standard attention?\\n\\n',\n",
              "  'answer': 'The main determining factor of attention run-time in FlashAttention is the number of HBM accesses, which is much fewer than standard attention due to recomputation in the backward pass. The runtime of FlashAttention can be further optimized by varying the block size, resulting in a decrease in the number of HBM accesses and runtime. However, for large enough block size, the runtime may become bottlenecked by other factors such as arithmetic operations or limited SRAM size.',\n",
              "  'source_doc': 'FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf'},\n",
              " {'context': '1 I NTRODUCTION\\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\\ndespite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\\n(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\\nwith relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\\n2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\\nunnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\\nretrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\\nthe output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\\nthe models are not explicitly trained to leverage and follow facts from provided passages. This\\nwork introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an\\nLLMâ€™s generation quality, including its factual accuracy without hurting its versatility, via on-demand\\nretrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\\nits own generation process given a task input by generating both task output and intermittent special\\ntokens (i.e., reflection tokens ). Reflection tokens are categorized into retrieval andcritique tokens to\\nindicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\\ngiven an input prompt and preceding generations, SELF-RAGfirst determines if augmenting the\\ncontinued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that\\ncalls a retriever model on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple\\nretrieved passages, evaluating their relevance and then generating corresponding task outputs (Step',\n",
              "  'question': 'In what way does Self-Reflective Retrieval-augmented Generation (SELF-RAG) improve the generation quality of LLMs, including their factual accuracy, without hurting their versatility?\\n\\n',\n",
              "  'answer': 'Self-Reflective Retrieval-augmented Generation (SELF-RAG) improves the generation quality of LLMs by training an arbitrary LM in an end-to-end manner to learn to reflect on its own generation process given a task input. This is achieved by generating both task output and intermittent special tokens (i.e., reflection tokens), which are categorized into retrieval and critique tokens to indicate the need for retrieval and its generation quality. SELF-RAG determines if augmenting the continued generation with retrieved passages would be helpful and outputs a retrieval token that calls a retriever model on demand. Subsequently, SELF-RAG processes multiple retrieved passages, evaluating their relevance and generating corresponding task outputs. This approach allows SELF-RAG to improve the factual accuracy of LLMs without hurting their versatility, as it ensures that only relevant and high-quality passages are retrieved and used in the generation process.',\n",
              "  'source_doc': 'Self-Rag_ Learning to Retrieve, Generate, and Critique Through Self-Reflection.pdf'},\n",
              " {'context': 'prompts. Again, crucially, those prompts do not contain the original baseline response and are hence\\nnot prone to simply copying or repeating it. The factored approach has the further advantage of\\nremoving any potential interference not only from the baseline response, but also between answer\\ncontexts, and is somewhat related to the recent (concurrent) work of Radhakrishnan et al. (2023)\\nfor subquestion answering by factored decomposition, hence we adopt their naming. It can also\\npotentially handle more verification questions by virtue of them not all having to fit with the same\\nsingle context. While this is potentially more computationally expensive, requiring the execution\\nof many more LLM prompts, they can be run in parallel, and hence be batched. In order to do\\nthis, we first have to take the set of generated questions from subsection 3.2 and parse them into\\nseparate questions, which is a relatively easy task as the few-shot demonstrations we provide indicate\\nthey should be generated as a comma-separated list. We can then split them out into separate LLM\\nprompts.\\nFactor+Revise After answering the verification questions, the overall CoVe pipeline then has to\\neither implicitly or explicitly cross-check whether those answers indicate an inconsistency with the\\noriginal responses. In the factor+revise approach, we execute this as a deliberate step via an extra\\nLLM prompt, which may make it easier for the final system to reason about this step explicitly.\\nDifferently to answering the verification questions, the cross-checking phase needs to condition\\non both the baseline response and the verification question and answer. We thus execute this as\\nseparate LLM prompts, one â€œcross-checkâ€ prompt for each question, with again a set of few-shot\\n4demonstrations showing the desired output. For example if the original baseline response contained\\nthe phrase â€œIt followed in the wake of the 1845 U.S. annexation of Texas. . . â€ and CoVe generated a',\n",
              "  'question': 'Can the factor+revise approach in the CoVe pipeline be used to explicitly check for inconsistencies between the original responses and the verification questions and answers?\\n\\n',\n",
              "  'answer': 'Yes, the factor+revise approach in the CoVe pipeline can be used to explicitly check for inconsistencies between the original responses and the verification questions and answers. This is done by executing separate LLM prompts that condition on both the baseline response and the verification question and answer.',\n",
              "  'source_doc': 'Chain-of-Verification Reduces Hallucination in Large Language Models.pdf'},\n",
              " {'context': 'Topic Classification...\\nTeacherLM\\nAugmented Dataset\\nStudent Model\\nâˆ’10010âˆ†Accuracy (%)Augmented v.s. Unaugmented\\nAugmented >Unaugmented Augmented <Unaugmented\\nhigh school statistics\\nhigh school government politicssecurity studies\\nhigh school macroeconomicsmarketing\\nus foreign policyconceptual physics\\nhigh school chemistry\\ncollege computer sciencehigh school biologyprehistory\\ncollege medicine\\nhigh school european historycomputer securitypublic relationscollege biology\\nhigh school microeconomicscollege physicsinternational lawnutrition\\nelectrical engineeringeconometricsanatomy\\nhigh school world historycollege chemistry\\ncollege mathematics\\nhigh school computer sciencemedical genetics\\nhigh school psychologyclinical knowledgeformal logic\\nhuman sexualitysociology\\nmanagement\\nmoral disputesphilosophy\\nmiscellaneous\\nhigh school geographyvirology\\nworld religions\\nprofessional medicineprofessional accountingglobal factshuman aging\\nprofessional law\\nhigh school us historyprofessional psychologybusiness ethicsmoral scenarios\\nelementary mathematicslogical fallacies\\nhigh school mathematicshigh school physicsjurisprudence\\nabstract algebramachine learningastronomy\\nFigure 1: TeacherLMs can perform augmentation on a wide range of datasets. They can leverage three different prompts\\nto generate augmentations, including fundamentals, CoT, and common mistakes, providing complete information needed\\nto solve the problem. The results of multitask training using augmented and unaugmented P3-Sense-3K data show that\\nthe augmented data can make the BLOOM-7.1B student model perform better in zero-shot performance on 47 tasks in the\\nMMLU(57 tasks) benchmark.\\nintuitions. Firstly, we believe that the real need for aug-\\nmentation in a dataset lies in each sampleâ€™s label, where\\nthe model should learn â€œwhyâ€ instead of just remember-\\ning â€œwhatâ€. Our goal is to shift the learning objectives of\\nlanguage models from results-oriented to process-oriented,',\n",
              "  'question': 'What is the main goal of TeacherLM in using augmentation on a dataset?\\n\\n',\n",
              "  'answer': 'The main goal of TeacherLM in using augmentation on a dataset is to shift the learning objectives of language models from results-oriented to process-oriented. The focus is on learning \"why\" instead of just remembering \"what\".',\n",
              "  'source_doc': 'TeacherLM_ Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise.pdf'},\n",
              " {'context': 'viewed as the traversal of a tree where the nodes are (partial) sequences.\\nA central quantity of interest is the occupancy measure. We denote by stthe random variable\\nconsisting of the state at time tunder a policy p(a|s)and the MDP defined above. Then, the\\noccupancy measure Ï(s, a) :S Ã— A â†’ [0,1]is the (discounted) probability of observing a particular\\nsentence sat time tand taking action agiven that sentence:\\nÏ(s, a) = (1 âˆ’Î³)p(a|s)X\\ntÎ³tP(st=s) (3)\\nIn other words, the occupancy measure is proportional to the observed frequency of a particular\\n(sentence, next-action) pair occurring, with occurrances discounted in time by a factor of Î³. In the\\nabsence of editing actions, A=Xand the occupancy measure is a discounted probability over\\n(partial) sequences: for a sequence snof length n,Ïdata(sn, x) = (1 âˆ’Î³)Î³nPdata(sâ€²), where sâ€²is\\nthe sequence obtained by appending xtos. Given editing actions which can reduce the length of a\\nsequence, the occupancy measure becomes more complicated, as the same sequence can occur at\\nmultiple times. For instance, if a sequence has length nat time nand the <backspace> action is used,\\nthe sequence at time n+1will have length nâˆ’1. We note that the occupancy measure is a normalized\\nprobability distribution, even if editing actions are included. For a function r, the expectation with\\nrespect to Ïhas the usual meaning: E(s,a)âˆ¼Ïh\\nr(s, a) =P\\nS,AÏ(s, a)r(s, a)i\\n, where the sum is\\nover the discrete action space and (countably infinite) state space. Occupancy measures provide\\nan alternative way of modelling sequences, allowing us to impose a measure over all sequences,\\neven in the presence of editing actions. As such, we avoid the somewhat artificial treatment with\\na maximum length cut-off discussed in section 2.1, and can handle infinite sequences in a natural\\nmanner. Furthermore, the next section illustrates that we can non-adversarially minimize a large\\nvariety of divergences between occupancy measures, compared to only the KL divergence in the',\n",
              "  'question': 'Given that the occupancy measure Ï(s, a) is a normalized probability distribution, what is the expected value of the occupancy measure under a given function r?\\n\\n',\n",
              "  'answer': 'The expected value of the occupancy measure under a given function r is computed as E(s,a)âˆ¼Ïh\\nr(s, a) =P\\nS,AÏ(s, a)r(s, a)i\\n, where the sum is over the discrete action space and (countably infinite) state space.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'troduced RETRO, featuring document reading via\\nnontrivial modi\\ue000cations that require further train-\\ning to the LM architecture, while using an off-the-\\nshelf frozen BERT retriever for document selec-\\ntion. Although the paperâ€™s experimental\\ue000ndings\\nshowed impressive performance gains, the need for\\nchanges in architecture and dedicated retraining\\nhas hindered the wide adoption of such models.\\nIn this paper, we show that a very simple doc-\\nument reading mechanism can have a large im-\\npact, and that substantial gains can also be made\\nby adapting the document selection mechanism to\\nthe task of language modeling. Thus, we show that\\nmany of the bene\\ue000ts of RALM can be achieved\\nwhile working with off-the-shelf LMs, even via\\nAPI access. Speci\\ue000cally, we consider a simple but\\npowerful RALM framework, dubbedIn-Context\\nRALM(presented in Section 3), which employs a\\nzero-effort document reading mechanism: we sim-\\nply prepend the selected documents to the LMâ€™s\\ninput text (Figure 2).\\nSection 4describes our experimental setup. To\\nshow the wide applicability of our framework, we\\nperformed LM experiments on a suite of\\ue000ve di-\\nverse corpora: WikiText-103 ( Merity et al. ,2016 ),\\nRealNews ( Zellers et al. ,2019 ), and three datasets\\nfrom The Pile ( Gao et al. ,2021 ): ArXiv, Stack\\nExchange and FreeLaw. We use open-source LMs\\nranging from 110M to 66B parameters (from the\\nGPT-2, GPT-Neo, OPT and LLaMA model fami-\\nlies).\\nIn Section 5we evaluate the application of off-\\nthe-shelf retrievers to our framework. In this\\nminimal-effort setting, we found that In-Context\\nRALM led to LM performance gains equivalent to\\nincreasing the LMâ€™s number of parameters by 2â€“\\n3Ã—across all of the text corpora we examined. In\\nSection 6we investigate methods for adapting doc-ument ranking to the LM task, a relatively under-\\nexplored RALM degree of freedom. Our adapta-\\ntion methods range from using a small LM to per-\\nform zero-shot ranking of the retrieved documents,\\nup to training a dedicated bidirectional reranker',\n",
              "  'question': 'What is the impact of a simple document reading mechanism and adapting the document selection mechanism to the task of language modeling on the performance of RETRO, and how does this impact the wide adoption of such models?\\n\\n',\n",
              "  'answer': \"The paper shows that a simple document reading mechanism and adapting the document selection mechanism to the task of language modeling can have a large impact on the performance of RETRO. This allows for substantial gains to be made while working with off-the-shelf LMs, even via API access. The paper also discusses the benefits of a simple but powerful RALM framework, dubbed In-Context RALM, which employs a zero-effort document reading mechanism and shows that many of the benefits of RALM can be achieved while working with off-the-shelf LMs. The paper also describes experimental results showing that In-Context RALM led to LM performance gains equivalent to increasing the LM's number of parameters by 2-3x across all of the text corpora examined. Additionally, the paper investigates methods for adapting document ranking to the LM task, showing that this can further improve the performance of RETRO. Overall, the paper demonstrates that simple changes to the architecture and dedicated retraining of RETRO models are not necessary for achieving substantial performance gains, and that many of the benefits of RALM can be achieved while working with off-the-shelf LMs.\",\n",
              "  'source_doc': 'In-Context Retrieval-Augmented Language Models.pdf'},\n",
              " {'context': 'RoB large(AdptP)y 3.0M 90.2\\x06.396.1\\x06.390.2\\x06.768.3\\x061.094.8\\x06.291.9\\x06.183.8\\x062.992.1\\x06.788.4\\nRoB large(AdptP)y 0.8M 90.5\\x06.396.6\\x06.289.7\\x061.267.8\\x062.594.8\\x06.391.7\\x06.280.1\\x062.991.9\\x06.487.9\\nRoB large(AdptH)y 6.0M 89.9\\x06.596.2\\x06.388.7\\x062.966.5\\x064.494.7\\x06.292.1\\x06.183.4\\x061.191.0\\x061.787.8\\nRoB large(AdptH)y 0.8M 90.3\\x06.396.3\\x06.587.7\\x061.766.3\\x062.094.7\\x06.291.5\\x06.172.9\\x062.991.5\\x06.586.4\\nRoB large(LoRA)y 0.8M 90.6\\x06.296.2\\x06.590.2\\x061.068.2\\x061.994.8\\x06.391.6\\x06.285.2\\x061.192.3\\x06.588.6\\nDeB XXL(FT)* 1500.0M 91.8 97.2 92.0 72.0 96.0 92.7 93.9 92.9 91.1\\nDeB XXL(LoRA) 4.7M 91.9\\x06.296.9\\x06.292.6\\x06.672.4\\x061.196.0\\x06.192.9\\x06.194.9\\x06.493.0\\x06.291.3\\nTable 2: RoBERTa base, RoBERTa large, and DeBERTa XXLwith different adaptation methods on the\\nGLUE benchmark. We report the overall (matched and mismatched) accuracy for MNLI, Matthewâ€™s\\ncorrelation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. Higher is better\\nfor all metrics. * indicates numbers published in prior works. yindicates runs conï¬gured in a setup\\nsimilar to Houlsby et al. (2019) for a fair comparison.\\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\\nContemporarily, this baseline has also been studied by BitFit (Zaken et al., 2021).\\nPreï¬x-embedding tuning (PreEmbed) inserts special tokens among the input tokens. These spe-\\ncial tokens have trainable word embeddings and are generally not in the modelâ€™s vocabulary. Where\\nto place such tokens can have an impact on performance. We focus on â€œpreï¬xingâ€, which prepends\\nsuch tokens to the prompt, and â€œinï¬xingâ€, which appends to the prompt; both are discussed in Li &\\nLiang (2021). We use lp(resp.li) denote the number of preï¬x (resp. inï¬x) tokens. The number of\\ntrainable parameters is j\\x02j=dmodel\\x02(lp+li).\\nPreï¬x-layer tuning (PreLayer) is an extension to preï¬x-embedding tuning. Instead of just learning\\nthe word embeddings (or equivalently, the activations after the embedding layer) for some special',\n",
              "  'question': 'Given the provided table, what is the overall accuracy of RoBERTa large on the GLUE benchmark?\\n\\n',\n",
              "  'answer': '90.296.190.290.768.391.790.280.199.199.921.991.9',\n",
              "  'source_doc': 'LoRA_ Low-Rank Adaptation of Large Language Models.pdf'},\n",
              " {'context': 'true ï¬‚avor of the awakened mind.\\nQ: What is the true ï¬‚avor of the awakened\\nmind?\\nA: The true ï¬‚avor of the awakened mind is\\nthe ï¬‚avor of no ï¬‚avor. And the true ï¬‚avor of\\nno ï¬‚avor is the ï¬‚avor of smelly feet.There is no clear answer to this question, but there are many theo-\\nries and ideas that may explain the apparent need to eat socks after\\nmeditating. Some experts believe that the act of eating a sock helps\\nthe brain to come out of its altered state as a result of meditation,\\nwhile others suggest that it has something to do with fueling the\\nbody after engaging in a physically and mentally challenging ac-\\ntivity, such as mediation. Other theories propose that eating a sock\\nmay give the meditator a new sensory experience or allow them to\\nconnect with the physical, material world in a way that breaks them\\nout of their usual thought processes and routine. However, there is\\ncurrently no hard scientiï¬c evidence to support any of these theories\\nand it is ultimately up to each individual to decide why eating socks\\nis important to them, based on their own personal experiences and\\nthoughts on the matter.\\nPrompt:\\nWhat happens if you ï¬re a cannonball directly at a pumpkin at high speeds?\\nGPT-3 175B completion: InstructGPT 175B completion:\\nThe pumpkin will pull the cannonball in, and\\nthe cannonball will bounce off of the pump-\\nkin. A pumpkin is a strong magnet, so strong\\nthat it can manipulate metal objects.There is no way to accurately predict what will happen if you ï¬re\\na cannonball directly at a pumpkin at high speeds, since there are\\ntoo many unpredictable factors involved. Some of these factors\\nmight include the size, weight, and density of the cannonball, the\\nmaterial that the pumpkin is made of, the velocity and impact force\\nof the cannonball, and the distance between the cannonball and the\\npumpkin at the moment of impact. Some of the more likely possible\\noutcomes include breaking or knocking the pumpkin to the ground,\\ncracking the pumpkin, or completely obliterating it.',\n",
              "  'question': 'What are the different types of quantum entanglement?\\n',\n",
              "  'answer': 'There are several types of quantum entanglement, including Einstein-Podolsky-Rosen (EPR) entanglement, Bell state entanglement, and GHZ entanglement. EPR entanglement is a type of correlation between two particles that cannot be explained by classical physics, while Bell state entanglement involves two qubits that are maximally entangled and cannot be separated without destroying the entanglement. GHZ entanglement occurs when three or more qubits are entangled, and it is known for its robustness to noise and decoherence.',\n",
              "  'source_doc': 'Training language models to follow instructions with human feedback.pdf'},\n",
              " {'context': 'largest car retailer.\" See also. Ah yes, the EV is finally relevant and the rest is\\nhistory. Toyota President Michio Kaku and CEO Akio Toyoda are turning heads and actively\\npromoting the sport-class EV during the various global celebrations, and even this week,\\nunveiling the new 2018 Plug-In Hybrid Transit (EV.SE), which will have a new 4.0-liter,EV\\npowertrain, a plug-in hybrid. It will arrive on the market in September, and will be\\nthe first-ever plug-in hybrid with a highway-compliant full-size sedan in Japan. The\\nnew sedan will offer \"bump-in\" emissions-calming technology, making the EV more efficient\\nthan its rivals. The fleet should start rolling out in 2017, and even here at EV Trade,\\nwe encourage most prospective car buyers to take the time to check our website for what\\nexactly is coming to market. You donâ€™t want your Sonata on the road for only a short week,\\nin the European market? Source: iSupermarket [Source: iSupermarket ]] Source: Quora\\nSource: Quora Source: Google+ Source: Google+ Source: Quora Source: Google+ Source:\\nQuora Source: Quora Source: Google+ Source: Google+ Source: Quora Source: Google+\\nSource: Quora Source: Quora Source: Quora Source:Table 5: Last Model Prompt and Completion for the 1024 MLE trained model. In this setting we add a random\\ntoken at the end of the prompt and observe the behavior of the generated sequences. Continued from previous\\npage\\nPrompt Completion\\nNorth Korean leader Kim Jong Un. AP Images / Business Insider North Korea attempted\\nto fire a missile Sunday, but it blew up within seconds. It happened one day after\\nthe anniversary of the countryâ€™s founding. While North Koreaâ€™s missile program may\\nbe the shadowiest on earth, itâ€™s possible that US cyber warriors were the reason for\\nthe failed launch. A recent New York Times report uncovered a secret operation to\\nderail North Koreaâ€™s nuclear-missile program that has been raging for at least three\\nyears. Essentially, the report attributes North Koreaâ€™s high rate of failure with',\n",
              "  'question': 'What is the first-ever plug-in hybrid with a highway-compliant full-size sedan in Japan, and what emissions-calming technology does it offer?\\n\\n',\n",
              "  'answer': 'The first-ever plug-in hybrid with a highway-compliant full-size sedan in Japan is the 2018 Plug-In Hybrid Transit (EV.SE). It offers \"bump-in\" emissions-calming technology, making the EV more efficient than its rivals.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'prompting setup (i.e. limited support of reasoning and acting behaviors), and perform initial ï¬netuning\\nexperiments showing the potential of ReAct to improve with additional training data. Scaling up\\nReAct to train and operate on more tasks and combining it with complementary paradigms like\\nreinforcement learning could further unlock the potential of large language models.\\n2REAC T: SYNERGIZING REASONING +AC TING\\nConsider a general setup of an agent interacting with an environment for task solving. At time\\nstept, an agent receives an observation ot2O from the environment and takes an action at2A\\nfollowing some policy \\x19(atjct), wherect= (o1;a1;\\x01\\x01\\x01;ot\\x001;at\\x001;ot)is the context to the agent.\\nLearning a policy is challenging when the mapping ct7!atis highly implicit and requires extensive\\ncomputation. For example, the agent shown in Figure 1(1c) is unable to generate the correct ï¬nal\\naction (Act 4) to ï¬nish the QA task as it requires complex reasoning over the trajectory context\\n(Question, Act 1-3, Obs 1-3). Similarly, the agent shown in Figure 1(2a) fails to comprehend from the\\ncontext that sinkbasin 1 does not contain peppershaker 1, thus keep producing hallucinating actions.\\nThe idea of ReAct is simple: we augment the agentâ€™s action space to ^A=A[L , whereLis the\\nspace of language. An action ^at2L in the language space, which we will refer to as a thought or a\\nreasoning trace , does not affect the external environment, thus leading to no observation feedback.\\nInstead, a thought ^ataims to compose useful information by reasoning over the current context ct,\\nand update the context ct+1= (ct;^at)to support future reasoning or acting. As shown in Figure 1,\\nthere could be various types of useful thoughts, e.g. decomposing task goals and create action plans\\n(2b, Act 1; 1d, Thought 1), injecting commonsense knowledge relevant to task solving (2b, Act 1),\\nextracting important parts from observations (1d, Thought2, 4), track progress and transit action plans',\n",
              "  'question': 'What is the main idea behind the ReAct paradigm in the context of an agent interacting with an environment for task solving?\\n\\n',\n",
              "  'answer': \"The main idea behind the ReAct paradigm is to augment the agent's action space to include language reasoning traces or thoughts, which do not affect the external environment. These thoughts aim to compose useful information by reasoning over the current context and update the context to support future reasoning or acting. The ReAct paradigm allows for various types of useful thoughts, such as decomposing task goals, injecting commonsense knowledge, extracting important parts from observations, and tracking progress and transit action plans.\",\n",
              "  'source_doc': 'ReAct_ Synergizing Reasoning and Acting in Language Models.pdf'},\n",
              " {'context': 'as of May 5, 2022). Additional results using various sizes of LaMDA, GPT-3, and PaLM are shown\\nin Table 4.\\n2We sample examples \\x1460tokens to ï¬t into our input context window, and also limit the examples to \\x142\\nsteps to solve for a fair comparison with the eight exemplars that we composed.\\n75 Symbolic Reasoning\\n0255075100 Solve rate (%)Letter Concat: 2\\n(in domain)Letter Concat: 4\\n(OOD)Standard prompting\\nChain-of-thought prompting\\n8 62 540406080100 Solve rate (%)Coin Flip: 2\\n(in domain)\\n8 62 540\\nModel scale (# parameters in billions)Coin Flip: 4\\n(OOD)\\nFigure 8: Using chain-of-thought\\nprompting facilitates generalization to\\nlonger sequences in two symbolic rea-\\nsoning tasks.Our ï¬nal experimental evaluation considers symbolic rea-\\nsoning, which is simple for humans but potentially chal-\\nlenging for language models. We show that chain-of-\\nthought prompting not only enables language models to\\nperform symbolic reasoning tasks that are challenging in\\nthe standard prompting setting, but also facilitates length\\ngeneralization to inference-time inputs longer than those\\nseen in the few-shot exemplars.\\nTasks. We use the following two toy tasks.\\nâ€¢Last letter concatenation. This task asks the model\\nto concatenate the last letters of words in a name (e.g.,\\nâ€œAmy Brownâ€!â€œynâ€ ). It is a more challenging version\\nof ï¬rst letter concatenation, which language models can\\nalready perform without chain of thought.3We generate\\nfull names by randomly concatenating names from the\\ntop one-thousand ï¬rst and last names from name census\\ndata ( https://namecensus.com/ ).\\nâ€¢Coin ï¬‚ip. This task asks the model to answer whether a\\ncoin is still heads up after people either ï¬‚ip or donâ€™t ï¬‚ip\\nthe coin (e.g., â€œA coin is heads up. Phoebe ï¬‚ips the coin.\\nOsvaldo does not ï¬‚ip the coin. Is the coin still heads up?â€\\n!â€œnoâ€ ).\\nAs the construction of these symbolic reasoning tasks is\\nwell-deï¬ned, for each task we consider an in-domain test\\nset for which examples had the same number of steps as',\n",
              "  'question': 'Can symbolic reasoning tasks be challenging for language models, and how can chain-of-thought prompting facilitate their performance?\\n\\n',\n",
              "  'answer': 'Yes, symbolic reasoning tasks can be challenging for language models. According to the paper, chain-of-thought prompting can facilitate the performance of these tasks by enabling language models to perform symbolic reasoning tasks that are challenging in the standard prompting setting and facilitating length generalization to inference-time inputs longer than those seen in the few-shot exemplars. The paper shows that chain-of-thought prompting not only enables language models to perform symbolic reasoning tasks that are challenging in the standard prompting setting but also facilitates length generalization to inference-time inputs longer than those seen in the few-shot exemplars.',\n",
              "  'source_doc': 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.pdf'},\n",
              " {'context': 'or another interpretable pattern such as â€œ Q: {question}\\\\nA: â€. Since a given promptâ€™s intention\\ncan be unclear or ambiguous, we rely on judgment from our labelers, and our main metric is labeler\\npreference ratings. However, since our labelers are not the users who generated the prompts, there\\ncould be a divergence between what a user actually intended and what the labeler thought was\\nintended from only reading the prompt.\\nIt is unclear how to measure honesty in purely generative models; this requires comparing the modelâ€™s\\nactual output to its â€œbeliefâ€ about the correct output, and since the model is a big black box, we canâ€™t\\ninfer its beliefs. Instead, we measure truthfulnessâ€”whether the modelâ€™s statements about the world\\nare trueâ€”using two metrics: (1) evaluating our modelâ€™s tendency to make up information on closed\\ndomain tasks (â€œhallucinationsâ€), and (2) using the TruthfulQA dataset (Lin et al., 2021). Needless to\\nsay, this only captures a small part of what is actually meant by truthfulness.\\nSimilarly to honesty, measuring the harms of language models also poses many challenges. In most\\ncases, the harms from language models depend on how their outputs are used in the real world. For\\ninstance, a model generating toxic outputs could be harmful in the context of a deployed chatbot, but\\nmight even be helpful if used for data augmentation to train a more accurate toxicity detection model.\\nEarlier in the project, we had labelers evaluate whether an output was â€˜potentially harmfulâ€™. However,\\nwe discontinued this as it required too much speculation about how the outputs would ultimately be\\nused; especially since our data also comes from customers who interact with the Playground API\\ninterface (rather than from production use cases).\\nTherefore we use a suite of more speciï¬c proxy criteria that aim to capture different aspects of\\nbehavior in a deployed model that could end up being harmful: we have labelers evaluate whether an',\n",
              "  'question': 'Can measuring truthfulness and harms of language models provide a comprehensive understanding of their honesty and potential negative consequences in real-world scenarios?\\n\\n',\n",
              "  'answer': \"No, measuring truthfulness and harms of language models only captures a small part of what is actually meant by honesty and potential negative consequences in real-world scenarios. Truthfulness measures whether the model's statements about the world are true, while harms depend on how the outputs are used in the real world. However, these metrics do not provide a comprehensive understanding of the model's honesty or potential negative consequences, as they do not take into account the specific context and intended use of the outputs. Therefore, it is challenging to measure honesty and harms of language models in a comprehensive manner.\",\n",
              "  'source_doc': 'Training language models to follow instructions with human feedback.pdf'},\n",
              " {'context': 'on SVAMP and ASDIV. Our results are 6% higher than their prompting method.\\nSemantic Binding and Multi-Step Reasoning The two core properties of â€˜program of thoughtsâ€™ are:\\n(1) multiple steps: breaking down the thought process into the step-by-step program, (2) semantic binding:\\nassociating semantic meaning to the variable names. To better understand how these two properties con-\\ntribute, we compared with two variants. One variant is to remove the semantic binding and simply use a, b, c\\nas the variable names. The other variant is to directly predict the final mathematical equation to compute\\nthe results. We show our findings in Table 6. As can be seen, removing the binding will in general hurt the\\nmodelâ€™s performance. On more complex questions involving more variables like GSM8K, the performance\\ndrop is larger. Similarly, prompting LLMs to directly generate the target equations is also very challenging.\\nBreaking down the target equation into multiple reasoning steps helps boost performance.\\nBreakdown Analysis We perform further analysis to determine which kinds of problems CoT and PoT\\ndiffermostinperformance. WeuseAQuA(Lingetal.,2017)asourtestbedforthis. Specifically, wemanually\\nclassify the questions in AQuA into several categories including geometry, polynomial, symbolic, arithmetic,\\ncombinatorics, linear equation, iterative and probability. We show the accuracy for each subcategory in Fig-\\nure 6. The major categories are (1) linear equations, (2) arithmetic, (3) combinatorics, (4) probability, and\\n(5) iterative. The largest improvements of PoT are in the categories â€˜linear/polynomial equationâ€™, â€˜iterativeâ€™,\\nâ€˜symbolicâ€™, and â€˜combinatoricsâ€™. These questions require more complex arithmetic or symbolic skills to solve.\\nIn contrast, on â€˜arithmeticâ€™, â€˜probabilityâ€™, and â€˜geometricâ€™ questions, PoT and CoT perform similarly. Such\\nobservation reflects our assumption that â€˜programâ€™ is more effective on more challenging problems.',\n",
              "  'question': 'What do the results of the comparison between CoT and PoT on SVAMP and ASDIV show in terms of their performance on different categories of questions?\\n\\n',\n",
              "  'answer': 'The results show that PoT outperforms CoT on more complex arithmetic, symbolic, combinatorics, and iterative questions, while they perform similarly on arithmetic, probability, and geometric questions. This suggests that the \"program\" property of CoT is more effective on more challenging problems.',\n",
              "  'source_doc': 'Program of Thoughts Prompting_  Disentangling Computation from Reasoning for Numerical Reasoning Tasks.pdf'},\n",
              " {'context': 'them. In Garg et al. [11], this was further developed via a change of variables to only require a\\nnon-adversarial optimization over one variable. We can view our approach as a specialization of the\\nIQ-Learn algorithm in Garg et al. [11] to autoregressive sequence models.\\n6 Experiments\\nSmall Medium Large\\nGPT2 Model Size103\\n102\\n101\\n100MAUVE Score\\nMAUVE Score\\nSequenceMatch (Full Model)\\nBehavioral Cloning (Full Model)\\nMLE (Full Model)\\nSequenceMatch (LM Head)\\nBehavioral Cloning (LM Head)\\nMLE (LM Head)\\nFigure 3: MAUVE score when fine-tuning GPT2 of various sizes on the openwebtext dataset with\\ncontext length 512. Higher is better. Full and LM Head refer to which parts of the pretrained model\\nare trained. We see that SequenceMatch outperforms behavioral cloning and MLE. The fully trained\\nmodels achieve much higher MAUVE scores than the variant only training the head.\\nWe experimentally verify that SequenceMatch can lead to better generation of sequences compared to\\nthe typical maximum-likelihood objective, when evaluated on a language modelling task [ 5,7]. Many\\nmore experimental details, code and additional analyses are provided in the supplementary material.\\nExperimental Setup\\nWe use the GPT-2 [ 25] causally-masked transformer architecture through Huggingface [ 38] and\\nFlax/Jax [13, 6]. We finetune on sequences from the openwebtext dataset3, an open-sourced dataset\\nsimilar to the original GPT-2 training set. To ensure the data was well-specified under our data\\ngenerating process, we subsampled the dataset to remove sequences that did not terminate before the\\nend of the context window (512 for experiments in figure 3 and 1024 for results in table 1), and padded\\nsequences that terminated before the context length. We consider two training regimes: Full , where\\nall model parameters are trained, and LM Head , where the model parameters are frozen and the final\\nlinear layer is trained. In both cases we add an additional head depending on the positional encodings',\n",
              "  'question': 'In what way does the proposed SequenceMatch approach differ from the Behavioral Cloning and Maximum Likelihood Estimation (MLE) approaches, and how does it perform in terms of MAUVE score and language modeling task generation?\\n\\n',\n",
              "  'answer': 'The proposed SequenceMatch approach differs from the Behavioral Cloning and MLE approaches in that it uses a non-adversarial optimization over one variable to fine-tune the pretrained GPT-2 model on autoregressive sequence models. The experimental results show that SequenceMatch outperforms behavioral cloning and MLE in terms of MAUVE score and language modeling task generation. Specifically, the fully trained models achieve much higher MAUVE scores than the variant only training the head. Additionally, SequenceMatch leads to better generation of sequences compared to the typical maximum-likelihood objective when evaluated on a language modelling task.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'Figure 3: Mean zero-shot accuracy over Wino-\\ngrande, HellaSwag, PiQA, Arc-Easy, and Arc-\\nChallenge using LLaMA models with different 4-bit\\ndata types. The NormalFloat data type significantly\\nimproves the bit-for-bit accuracy gains compared\\nto regular 4-bit Floats. While Double Quantization\\n(DQ) only leads to minor gains, it allows for a more\\nfine-grained control over the memory footprint to fit\\nmodels of certain size (33B/65B) into certain GPUs\\n(24/48GB).Similarly, we find that default hyperparameters for\\nfully finetuned baselines are undertuned. We do a\\nhyperparameter search over learning rates 1e-6 to\\n5e-5 and batch sizes 8 to 128 to find robust baselines.\\nResults for 7B LLaMA finetuning on Alpaca are\\nshown in Figure 2.\\n4-bit NormalFloat yields better performance\\nthan 4-bit Floating Point While the 4-bit\\nNormalFloat (NF4) data type is information-\\ntheoretically optimal, it still needs to be determined\\nif this property translates to empirical advantages.\\nWe follow the setup from Dettmers and Zettlemoyer\\n[13] where quantized LLMs (OPT [ 72], BLOOM\\n[52], Pythia [ 7], LLaMA) of different sizes (125M\\nto 65B) with different data types are evaluated on\\nlanguage modeling and a set of zero-shot tasks. In\\nFigure 3 and Table 2 we see that NF4 improves per-\\nformance significantly over FP4 and Int4 and that\\ndouble quantization reduces the memory footprint\\nwithout degrading performance.\\nk-bit QL ORAmatches 16-bit full finetuning and\\n16-bit LoRA performance Recent findings have\\nestablished that 4-bit quantization for inference is\\n6Table 3: Experiments comparing 16-bit BrainFloat (BF16), 8-bit Integer (Int8), 4-bit Float (FP4), and 4-\\nbit NormalFloat (NF4) on GLUE and Super-NaturalInstructions. QLORAreplicates 16-bit LoRA and full-\\nfinetuning.\\nDataset GLUE (Acc.) Super-NaturalInstructions (RougeL)\\nModel RoBERTa-large T5-80M T5-250M T5-780M T5-3B T5-11B\\nBF16 88.6 40.1 42.1 48.0 54.3 62.0\\nBF16 replication 88.6 40.0 42.2 47.3 54.9 -\\nLoRA BF16 88.8 40.5 42.6 47.1 55.4 60.7',\n",
              "  'question': 'Which LLM model with a 4-bit NormalFloat data type outperforms other LLM models with different data types on zero-shot tasks on Wino-grande, HellaSwag, PiQA, Arc-Easy, and Arc-Challenge?\\n\\n',\n",
              "  'answer': 'The 7B LLaMA model with a 4-bit NormalFloat data type outperforms other LLM models with different data types on zero-shot tasks on Wino-grande, HellaSwag, PiQA, Arc-Easy, and Arc-Challenge.',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': 'following explicit constraints in the instruction and attempting the correct instruction, and less likely\\nto â€˜hallucinateâ€™ (meaning, making up information on closed domain tasks like summarization).\\nGPT GPT\\n(prompted)SFT PPO-ptx FLAN T0\\nModel246Likert score\\nFigure 5: Comparing our models with FLAN and T0 in terms of Likert scores on a 1-7 scale, on the\\nInstructGPT prompt distribution. FLAN and T0 perform better than default GPT-3, and comparably\\nwith a few-shot GPT-3 model placed into â€˜instruction-followingâ€™ mode.\\ncategories occur too infrequently in our API to obtain statistically signiï¬cant differences between our\\nmodels.\\nOur models generalize to the preferences of \"held-out\" labelers that did not produce any train-\\ning data. Held-out labelers have similar ranking preferences as workers who we used to produce\\ntraining data (see Figure 3). In particular, according to held-out workers, all of our InstructGPT\\nmodels still greatly outperform the GPT-3 baselines. Thus, our InstructGPT models arenâ€™t simply\\noverï¬tting to the preferences of our training labelers.\\nWe see further evidence of this from the generalization capabilities of our reward models. We ran an\\nexperiment where we split our labelers into 5 groups, and train 5 RMs (with 3 different seeds) using\\n5-fold cross validation (training on 4 of the groups, and evaluating on the held-out group). These\\nRMs have an accuracy of 69.6 \\x060.9% on predicting the preferences of labelers in the held-out group,\\na small decrease from their 72.4 \\x060.4% accuracy on predicting the preferences of labelers in their\\ntraining set.\\nPublic NLP datasets are not reï¬‚ective of how our language models are used. In Figure 5, we\\nalso compare InstructGPT to our 175B GPT-3 baselines ï¬ne-tuned on the FLAN (Wei et al., 2021) and\\nT0 (Sanh et al., 2021) datasets (see Appendix C for details). We ï¬nd that these models perform better\\nthan GPT-3, on par with GPT-3 with a well-chosen prompt, and worse than our SFT baseline. This',\n",
              "  'question': 'To what extent do the InstructGPT models generalize to the preferences of \"held-out\" labelers and outperform the GPT-3 baselines?\\n\\n',\n",
              "  'answer': 'The InstructGPT models generalize to the preferences of \"held-out\" labelers and outperform the GPT-3 baselines. This is evident from the results shown in Figure 3, where held-out workers have similar ranking preferences as workers who were used to produce training data. Additionally, the InstructGPT models still greatly outperform the GPT-3 baselines according to held-out workers. Furthermore, the InstructGPT models have an accuracy of 69.6%% on predicting the preferences of labelers in the held-out group, a small decrease from their 72.4% accuracy on predicting the preferences of labelers in their training set. This indicates that the InstructGPT models are not overfitting to the preferences of their training labelers.',\n",
              "  'source_doc': 'Training language models to follow instructions with human feedback.pdf'},\n",
              " {'context': 'to set up the virtual environments, install dependencies, and\\nrun integration tests.\\nWe considered GitHub repos using travis and tox as their CI\\nframeworks, as they are two of the most popular CI tools.\\nWe additionally used publicly available source code from\\npip packages found in the python package index (PyPI).Evaluating Large Language Models Trained on Code\\nTable 2. Finetuned GPT-Neo numbers from the APPS paper referenced above. For Codex-12B, the number of passing programs that\\ntimeout on some test is in the bracket. We used temperature 0.6 for sampling to cover all kin pass@ k, so raw pass@1 results could be\\nimproved with lower temperature.\\nINTRODUCTORY INTERVIEW COMPETITION\\nGPT-N EO2.7B RAW PASS @1 3.90% 0.57% 0.00%\\nGPT-N EO2.7B RAW PASS @5 5.50% 0.80% 0.00%\\n1-SHOT CODEX RAW PASS @1 4.14% (4.33%) 0.14% (0.30%) 0.02% (0.03%)\\n1-SHOT CODEX RAW PASS @5 9.65% (10.05%) 0.51% (1.02%) 0.09% (0.16%)\\n1-SHOT CODEX RAW PASS @100 20.20% (21.57%) 2.04% (3.99%) 1.05% (1.73%)\\n1-SHOT CODEX RAW PASS @1000 25.02% (27.77%) 3.70% (7.94%) 3.23% (5.85%)\\n1-SHOT CODEX FILTERED PASS @1 22.78% (25.10%) 2.64% (5.78%) 3.04% (5.25%)\\n1-SHOT CODEX FILTERED PASS @5 24.52% (27.15%) 3.23% (7.13%) 3.08% (5.53%)\\nBecause these projects contained untrusted code, it was im-\\nportant to run integration tests in the sandboxed environment\\ndescribed above.\\nWhile there are millions of potential functions to curate\\nproblems from, we only collected about 40,000 because\\nnot all functions accept inputs and return outputs. Even\\nwhen they do, most objects captured at runtime cannot be\\npickled and restored outside the sandbox unless the project\\nwas installed.\\nSince our tracing methodology produced inputs and outputs\\nfor all invoked functions, even builtin and library calls im-\\nported by the project were turned into problems. For this\\nreason, functions from tracing tended to be the building\\nblocks of command-line utilities. To excel at these tasks,\\nthe model does not need to know advanced algorithms and',\n",
              "  'question': 'In what ways can the performance of a GPT-Neo model be improved for integrating code in a sandboxed environment?\\n\\n',\n",
              "  'answer': 'The performance of a GPT-Neo model can be improved for integrating code in a sandboxed environment by lowering the temperature during sampling, which can cover all kin pass@ k and improve raw pass@1 results. Additionally, the model does not need to know advanced algorithms to excel at integrating code in a sandboxed environment, as it can focus on building blocks of command-line utilities.',\n",
              "  'source_doc': 'Evaluating Large Language Models Trained on Code.pdf'},\n",
              " {'context': 'of each dataset. Manual denotes human annotated rationales in original datasets. CoT ,Fud, and Mis are TeacherLMâ€™s\\ngenerated CoT, fundamentals, and common mistakes, while CoT-D ,Fud-D , and Mis-D are text-davinci-003â€™s generated\\nCoT, fundamentals and common mistakes, through the same prompts inputted to TeacherLM.\\nDATASET OPTIONSEXMAPLE AMOUNTS AVERAGE TOKENS\\nTRAIN TEST MANUAL COT F UD MIS COT-D F UD-D M IS-D\\nECQA 5 7598 2194 59 75 186 53 47 27 32\\nSTRATEGY QA 2 1832 228 28 90 192 55 73 48 31\\nCREAK 2 10174 1371 15 71 190 48 42 25 19\\nP3- SENSE -3K / 1400364 / / 49 135 54 / / /\\naugmented samples, as shown in Figure 4, include five parts\\nof information: question, answer, fundamentals, chain of\\nthought, and common mistakes. For more information on\\nthe above datasets, please refer to Table 2.\\nIn the multi-task training mode, we evaluated the benefit of\\nthe P3-Sense-3K dataset augmented by TeacherLM-7.1B\\non various models. The three parts of TeacherLMâ€™s explana-\\ntions are concatenated into sequence with the answer. The\\ncontrol group consisted of the original P3-Sense-3k dataset,\\ncontaining only question and answer pairs. We increased the\\nstudent model size from 1.1B to 7.1B and set the learning\\nrate for all experiments in the multi-task training mode to\\n2e-5 and the batch size to 256.\\nFor the single-task fine-tuning, we select BLOOMZ-7.1B\\nas the student model, which has been fine-tuned on xP3,\\na composite of supervised datasets in 46 languages with\\nEnglish and machine-translated prompts. In the single-task\\nfine-tuning, we set the learning rate of all experiments to\\n6e-6 and the batch size to 64.\\n4.3. Comparison with human and text-davinci-003\\nTo further validate model-generated explanationsâ€™ qual-\\nity, we include human annotation and text-davinci-003\\nas control groups in our experiments, where text-davinci-\\n003 serves as the teacher and augments the Strate-\\ngyQA, CREAK, and ECQA datasets in the same way as',\n",
              "  'question': 'Can you explain the difference between the multi-task and single-task fine-tuning methods used in the study?\\n\\n',\n",
              "  'answer': 'The multi-task fine-tuning method involves training a student model on multiple tasks simultaneously, while the single-task fine-tuning method involves training a student model on a single task. In the study, the multi-task fine-tuning method used the P3-Sense-3K dataset augmented by TeacherLM-7.1B, while the single-task fine-tuning method used the BLOOMZ-7.1B model fine-tuned on xP3. The learning rate for the multi-task fine-tuning was set to 2e-5 and the batch size to 256, while the learning rate for the single-task fine-tuning was set to 6e-6 and the batch size to 64.',\n",
              "  'source_doc': 'TeacherLM_ Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise.pdf'},\n",
              " {'context': 'Algorithm 1 SELF-RAGInference\\nRequire: Generator LM M, Retriever R, Large-scale passage collections {d1, . . . , d N}\\n1:Input: input prompt xand preceding generation y<t,Output: next output segment yt\\n2:Mpredicts Retrieve given (x, y<t)\\n3:ifRetrieve ==Yes then\\n4: Retrieve relevant text passages DusingRgiven (x, ytâˆ’1) â–·Retrieve\\n5: Mpredicts ISRELgiven x, dandytgiven x, d, y <tfor each dâˆˆD â–·Generate\\n6: Mpredicts ISSUPand ISUSEgiven x, yt, dfor each dâˆˆD â–·Critique\\n7: Rank ytbased on ISREL,ISSUP,ISUSE â–·Detailed in Section 3.3\\n8:else if Retrieve ==Nothen\\n9: Mgenpredicts ytgiven x â–· Generate\\n10: Mgenpredicts ISUSEgiven x, yt â–·Critique\\nInference overview. Figure 1 and Algorithm 1 present an overview of S ELF-RAGat inference. For\\nevery xand preceding generation y<t, the model decodes a retrieval token to evaluate the utility\\nof retrieval. If retrieval is not required, the model predicts the next output segment, as it does in a\\nstandard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved\\npassageâ€™s relevance, the next response segment, and a critique token to evaluate if the information in\\nthe response segment is supported by the passage. Finally, a new critique token evaluates the overall\\nutility of the response.4To generate each segment, SELF-RAGprocesses multiple passages in parallel\\nand uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control\\n(Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages\\nd1is selected at the first time step since d2does not provide direct evidence ( ISRELis Irrelevant)\\nandd3output is only partially supported while d1are fully supported.\\nTraining overview. SELF-RAGenables an arbitrary LM to generate text with reflection tokens\\nby unifying them as next token predictions from the expanded model vocabulary (i.e., the original',\n",
              "  'question': 'Given a generative model LM, a retriever R, and a large-scale passage collection {d1, . . . , d N}, how does Algorithm 1 SELF-RAG implement inference to generate a next output segment yt?\\n\\n',\n",
              "  'answer': 'Algorithm 1 SELF-RAG implements inference by first predicting whether retrieval is required. If retrieval is needed, the model generates a critique token to evaluate the relevance of the retrieved passage, the next response segment, and a critique token to evaluate if the information in the response segment is supported by the passage. Finally, a new critique token evaluates the overall utility of the response. To generate each segment, the model processes multiple passages in parallel and uses its own generated reflection tokens to enforce soft constraints or hard control over the generated task output.',\n",
              "  'source_doc': 'Self-Rag_ Learning to Retrieve, Generate, and Critique Through Self-Reflection.pdf'},\n",
              " {'context': 'dequant (cFP32,XInt8) =XInt8\\ncFP32=XFP32(2)\\nThe problem with this approach is that if a large magnitude value (i.e., an outlier) occurs in the input\\ntensor, then the quantization binsâ€”certain bit combinationsâ€”are not utilized well with few or no\\nnumbers quantized in some bins. To prevent the outlier issue, a common approach is to chunk the\\ninput tensor into blocks that are independently quantized, each with their own quantization constant c.\\nThis can be formalized as follows: We chunk the input tensor XâˆˆRbÃ—hintoncontiguous blocks of\\nsizeBby flattening the input tensor and slicing the linear segment into n= (bÃ—h)/Bblocks. We\\nquantize these blocks independently with Equation 1 to create a quantized tensor and nquantization\\nconstants ci.\\nLow-rank Adapters Low-rank Adapter (LoRA) finetuning [ 28] is a method that reduces memory\\nrequirements by using a small set of trainable parameters, often termed adapters, while not updating\\nthe full model parameters which remain fixed. Gradients during stochastic gradient descent are\\npassed through the fixed pretrained model weights to the adapter, which is updated to optimize the\\nloss function. LoRA augments a linear projection through an additional factorized projection. Given\\na projection XW =YwithXâˆˆRbÃ—h,WâˆˆRhÃ—oLoRA computes:\\nY=XW +sXL 1L2, (3)\\nwhereL1âˆˆRhÃ—randL2âˆˆRrÃ—o, and sis a scalar.\\nMemory Requirement of Parameter-Efficient Finetuning One important point of discussion is\\nthe memory requirement of LoRA during training both in terms of the number and size of adapters\\nused. Since the memory footprint of LoRA is so minimal, we can use more adapters to improve\\nperformance without significantly increasing the total memory used. While LoRA was designed as a\\n3Parameter Efficient Finetuning (PEFT) method, most of the memory footprint for LLM finetuning\\ncomes from activation gradients and not from the learned LoRA parameters. For a 7B LLaMA\\nmodel trained on FLAN v2 with a batch size of 1, with LoRA weights equivalent to commonly used',\n",
              "  'question': 'How does the outlier issue in quantization of input tensors affect the performance of the quantization process?\\n\\n',\n",
              "  'answer': 'The outlier issue in quantization of input tensors can affect the performance of the quantization process by causing certain bit combinations in the quantization bins to not be utilized well, which can lead to poor quantization of the input tensor. This can result in lower accuracy and reduced performance of the quantized model. To prevent this issue, a common approach is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant c. This can improve the accuracy and performance of the quantized model.',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': 'methodâ€™s â€œzero-shotâ€ performance when retrieving\\nfrom a novel corpus (for WikiText-103). The rest of\\nthe models brought two further bene\\ue000ts. First, they\\nallowed us to investigate how our methods scale\\nto models larger than GPT-2. Second, the fact that\\nWikipedia was part of their training data allowed us\\nto investigate the usefulness of In-Context RALM\\nfor corpora seen during training. The helpfulness\\nof such retrieval has been demonstrated for previ-\\nous RALM methods ( Khandelwal et al. ,2020) and\\nhas also been justi\\ue000ed theoretically by Levine et al.\\n(2022c ).\\nWe ran all models with a maximum sequence\\nlength of 1,024, even though GPT-Neo, OPT and\\nLLaMA models support a sequence length of\\n2,048.4\\nRetrieversWe experimented with both sparse\\n(word-based) and dense (neural) retrievers. We\\nused BM25 ( Robertson and Zaragoza ,2009) as our\\nsparse model. For dense models, we experimented\\nwith (i) a frozen BERT-base ( Devlin et al. ,2019 )\\nfollowed by mean pooling, similar to Borgeaud\\net al. (2022 ); and (ii) the Contriever ( Izacard et al. ,\\n2022a ) and Spider ( Ram et al. ,2022 ) models,\\nwhich are dense retrievers that were trained in un-\\nsupervised manners.\\nRerankingWhen training rerankers (Sec-\\ntion6.2), we initialized from RoBERTa-base ( Liu\\net al.,2019 ).\\n4.3 Implementation Details\\nWe implemented our code base using the Trans-\\nformers library ( Wolf et al. ,2020 ). We based\\nour dense retrieval code on the DPR repository\\n(Karpukhin et al. ,2020 ).\\n3All models are available for use use via https://\\nhuggingface.co/\\n4In preliminary experiments, we observed similar improve-\\nments from In-Context RALM when using a sequence length\\nof 2,048. We used a sequence length of 1,024 in order to\\nfacilitate a direct comparison between all models.\\nFigure 3: The performance of four off-the-shelf\\nretrievers used for In-Context RALM on the de-\\nvelopment set of WikiText-103. All RALMs are\\nrun with s= 4 (i.e., retrieval is applied every four\\ntokens). For each RALM, we report the result of',\n",
              "  'question': 'Which off-the-shelf retrievers were used for In-Context RALM on the development set of WikiText-103 and what was their performance?\\n\\n',\n",
              "  'answer': 'The off-the-shelf retrievers used for In-Context RALM on the development set of WikiText-103 were BM25, a sparse model, and the Contriever and Spider models, which are dense models. The performance of BM25 was not reported, but the performance of the Contriever and Spider models was reported to be similar to each other.',\n",
              "  'source_doc': 'In-Context Retrieval-Augmented Language Models.pdf'},\n",
              " {'context': 'a host or network. Since OpenAIâ€™s training infrastructure\\nis built on Kubernetes and cloud services, we designed our\\nsandbox to address the limitations of these environments\\nwhile remaining idiomatic with their patterns of use.\\nWe selected the gVisor container runtime (Lacasse, 2018)\\nas the main host protection component. Since container\\nruntimes like Docker can share host resources with contain-\\ners, a malicious container could potentially compromise a\\nhost. gVisor protects the host by emulating its resources to\\nintroduce a security boundary between the host and its con-\\ntainers. Network-adjacent hosts and services are protected\\nby eBPF-based ï¬rewall rules that prevent inbound and out-\\nbound connections except for those required for experiment\\ncontrol.\\n3. Code Fine-Tuning\\nWe ï¬ne-tune GPT models containing up to 12B parameters\\non code to produce Codex. In contrast with GPT, Codex\\ndisplays non-trivial performance on the HumanEval dataset.\\nIn fact, Codex is able to solve the majority of the problems\\nin HumanEval if we generate and evaluate 100 samples perproblem, and pick one that passes unit tests. When limited to\\na budget of one evaluation per problem, producing multiple\\nsamples with Codex and choosing the one with the highest\\nmean log-probability provides signiï¬cant gains.\\n3.1. Data Collection\\nOur training dataset was collected in May 2020 from 54 mil-\\nlion public software repositories hosted on GitHub, contain-\\ning 179 GB of unique Python ï¬les under 1 MB. We ï¬ltered\\nout ï¬les which were likely auto-generated, had average line\\nlength greater than 100, had maximum line length greater\\nthan 1000, or contained a small percentage of alphanumeric\\ncharacters. After ï¬ltering, our ï¬nal dataset totaled 159 GB.\\n3.2. Methods\\nSince Codex is evaluated on natural language prompts, we\\nhypothesized that it would be beneï¬cial to ï¬ne-tune from\\nthe GPT-3 (Brown et al., 2020) model family, which already\\ncontains strong natural language representations. Surpris-',\n",
              "  'question': \"Given a host or network, what is the main host protection component used in OpenAI's training infrastructure and how does it introduce a security boundary between the host and its containers?\\n\\n\",\n",
              "  'answer': \"The main host protection component used in OpenAI's training infrastructure is gVisor container runtime. It emulates the host's resources to introduce a security boundary between the host and its containers.\",\n",
              "  'source_doc': 'Evaluating Large Language Models Trained on Code.pdf'},\n",
              " {'context': 'We also performed extensive ablation experiments, and observed that independent random cropping seems to\\nbe a strong alternative to the inverse Cloze task for training retrievers.\\nRefe',\n",
              "  'question': 'How did the inverse Cloze task perform in comparison to independent random cropping for training retrievers?\\n',\n",
              "  'answer': 'Independent random cropping was observed to be a strong alternative to the inverse Cloze task for training retrievers.',\n",
              "  'source_doc': 'Unsupervised Dense Information Retrieval with Contrastive Learning.pdf'},\n",
              " {'context': 'Evaluating Large Language Models Trained on Code\\nMark Chen* 1Jerry Tworek* 1Heewoo Jun* 1Qiming Yuan* 1Henrique Ponde de Oliveira Pinto* 1\\nJared Kaplan* 2Harri Edwards1Yuri Burda1Nicholas Joseph2Greg Brockman1Alex Ray1Raul Puri1\\nGretchen Krueger1Michael Petrov1Heidy Khlaaf3Girish Sastry1Pamela Mishkin1Brooke Chan1\\nScott Gray1Nick Ryder1Mikhail Pavlov1Alethea Power1Lukasz Kaiser1Mohammad Bavarian1\\nClemens Winter1Philippe Tillet1Felipe Petroski Such1Dave Cummings1Matthias Plappert1\\nFotios Chantzis1Elizabeth Barnes1Ariel Herbert-Voss1William Hebgen Guss1Alex Nichol1Alex Paino1\\nNikolas Tezak1Jie Tang1Igor Babuschkin1Suchir Balaji1Shantanu Jain1William Saunders1\\nChristopher Hesse1Andrew N. Carr1Jan Leike1Josh Achiam1Vedant Misra1Evan Morikawa1\\nAlec Radford1Matthew Knight1Miles Brundage1Mira Murati1Katie Mayer1Peter Welinder1\\nBob McGrew1Dario Amodei2Sam McCandlish2Ilya Sutskever1Wojciech Zaremba1\\nAbstract\\nWe introduce Codex, a GPT language model ï¬ne-\\ntuned on publicly available code from GitHub,\\nand study its Python code-writing capabilities.\\nA distinct production version of Codex powers\\nGitHub Copilot. On HumanEval, a new evalua-\\ntion set we release to measure functional correct-\\nness for synthesizing programs from docstrings,\\nour model solves 28.8% of the problems, while\\nGPT-3 solves 0% and GPT-J solves 11.4%. Fur-\\nthermore, we ï¬nd that repeated sampling from the\\nmodel is a surprisingly effective strategy for pro-\\nducing working solutions to difï¬cult prompts. Us-\\ning this method, we solve 70.2% of our problems\\nwith 100 samples per problem. Careful investiga-\\ntion of our model reveals its limitations, including\\ndifï¬culty with docstrings describing long chains\\nof operations and with binding operations to vari-\\nables. Finally, we discuss the potential broader\\nimpacts of deploying powerful code generation\\ntechnologies, covering safety, security, and eco-\\nnomics.\\n*Equal contribution\\n1OpenAI, San Francisco, California, USA.',\n",
              "  'question': 'How does the performance of Codex, a GPT language model fine-tuned on publicly available code from GitHub, compare to that of GPT-3 and GPT-J on the HumanEval evaluation set for synthesizing programs from docstrings?\\n\\n',\n",
              "  'answer': 'Codex solves 28.8% of the problems on the HumanEval evaluation set, while GPT-3 solves 0% and GPT-J solves 11.4%. When using repeated sampling from the model, Codex solves 70.2% of its problems with 100 samples per problem.',\n",
              "  'source_doc': 'Evaluating Large Language Models Trained on Code.pdf'},\n",
              " {'context': 'higher accuracy than the original accuracy of the facts in the original longform generation. Finally,\\nthe revised response takes into account the verifications. The factored version of CoVe answers\\nverification questions such that they cannot condition on the original response, avoiding repetition\\nand improving performance.\\n2 R ELATED WORK\\nHallucination is a general problem in language model generations that appears across many tasks,\\nfrom summarization (Maynez et al., 2020) to open-domain dialogue (Roller et al., 2020), and has not\\nbeen resolved by simply scaling up training data or model size (Zhang et al., 2023). For a survey of\\nthe hallucination issue, see Ji et al. (2023). A majority of the methods for reducing hallucination can\\nbe divided into roughly three categories: training-time correction, generation-time correction and via\\naugmentation (tool-use).\\nIn training-time correction methods, an attempt is made to improve the raw left-to-right generations\\nof an encoder-decoder or decoder-only language model by either training or otherwise adjusting\\nthe model weights to decrease the probability of hallucinated generations. This includes using\\nreinforcement learning (Roit et al., 2023; Wu et al., 2023), constrastive learning (Chern et al., 2023b;\\nSun et al., 2023b) and other methods (Li et al., 2023).\\nIn generation-time correction, a common theme is to make reasoning decisions â€œon top ofâ€ the base\\nLLM in order to make them more reliable. For example, by considering the probabilities of the\\ngenerated tokens (Mielke et al., 2022; Kadavath et al., 2022). In Manakul et al. (2023) multiple\\nsamples are drawn from the model to detect hallucinations. In Varshney et al. (2023) hallucinations\\nare identified using low confidence scores, and their correctness is checked through a validation\\n2procedure, mitigated, and then the generation is continued. An alternative to using the confidence',\n",
              "  'question': 'Can hallucination in language model generations be reduced by training-time correction, generation-time correction, or augmentation methods?\\n\\n',\n",
              "  'answer': 'Hallucination in language model generations can be reduced by training-time correction, generation-time correction, or augmentation methods. Training-time correction methods attempt to improve the raw left-to-right generations of an encoder-decoder or decoder-only language model by either training or adjusting the model weights to decrease the probability of hallucinated generations. Generation-time correction methods make reasoning decisions \"on top of\" the base LLM to make them more reliable, such as by considering the probabilities of the generated tokens or identifying hallucinations using low confidence scores and validation procedures. Augmentation methods involve using external tools or techniques to enhance the performance of the LLM, such as by providing additional training data or using reinforcement learning.',\n",
              "  'source_doc': 'Chain-of-Verification Reduces Hallucination in Large Language Models.pdf'},\n",
              " {'context': 'though it sometimes generates outputs in English. GPT-3 requires more careful prompting, similarly\\nto in English. (2) InstructGPT can summarize and answer questions about code more reliably than\\nGPT-3 (though its answer here isnâ€™t quite correct). For the code QA example, GPT-3 does answer the\\nquestion about 50% of the time.\\nIn Figure 29 we show that adding pretraining updates to our PPO ï¬ne-tuning (PPO-ptx) mitigates these\\nperformance regressions on all datasets, and even surpasses GPT-3 on HellaSwag. The performance\\nof the PPO-ptx model still lags behind GPT-3 on DROP, SQuADv2, and translation; more work is\\nneeded to study and further eliminate these performance regressions.\\nMixing in pretraining updates performs better than the simpler solution of increasing the KL co-\\nefï¬cient. In Figure 33, we show that there is a value of the pretraining mix coefï¬cient that both\\nreverses the performance regressions on SQuADv2 and DROP (the datasets we used for testing), and\\nhas minimal reductions in validation reward. In contrast, increasing the KL coefï¬cient (Figure 34)\\nleads to signiï¬cant decreases in validation reward and never fully recovers on DROP and SQuAD.\\nChanging the KL model from the PPO init to GPT-3 gives similar results.\\n4.3 Qualitative results\\nInstructGPT models show promising generalization to instructions outside of the RLHF ï¬ne-\\ntuning distribution. In particular, we ï¬nd that InstructGPT shows ability to follow instructions\\nin non-English languages, and perform summarization and question-answering for code. This is\\n15Prompt:\\nWhy is it important to eat socks after meditating?\\nGPT-3 175B completion: InstructGPT 175B completion:\\nWhat can you learn from socks?\\nA: When you eat socks, you are tasting the\\nessence of enlightenment. You are tasting the\\ntrue ï¬‚avor of the awakened mind.\\nQ: What is the true ï¬‚avor of the awakened\\nmind?\\nA: The true ï¬‚avor of the awakened mind is\\nthe ï¬‚avor of no ï¬‚avor. And the true ï¬‚avor of',\n",
              "  'question': 'How does the performance of PPO-ptx compare to GPT-3 on various datasets in terms of fine-tuning and pretraining updates?\\n\\n',\n",
              "  'answer': 'The performance of PPO-ptx outperforms GPT-3 on HellaSwag after adding pretraining updates. However, PPO-ptx still lags behind GPT-3 on DROP, SQuADv2, and translation. Increasing the KL coefficient or changing the KL model from PPO init to GPT-3 gives similar results.',\n",
              "  'source_doc': 'Training language models to follow instructions with human feedback.pdf'},\n",
              " {'context': 'earlier bootstrap of that program.\\n1multihop_t5 = dspy.BootstrapFinetune(metric=answer_exact_match).compile(program,\\nteacher=bootstrap, trainset=trainset, target=â€™t5-largeâ€™)\\nResults Table 2 summarizes our results. Compared with the vanilla few-shot prompting, a chain-\\nof-thought and retrieval-augmented generation ( CoTRAG) program can self-bootstrap in DSPy to\\nincrease answer EM substantially. However, this relies entirely on the ColBERTv2 retriever to find\\nrelevant passages directly from the original questions, limiting its passage recall. This is tackled in\\nthereact andmultihop programs, which will generate queries for the retriever in multiple iterative\\nâ€œhopsâ€. Indeed, overall, a simple multihop program performs the best, and in general bootstrap\\nagain proves to be very effective at raising its quality relative to its fewshot variant for both LMs.\\nIn particular, we can see that bootstrap (and/orbootstrap Ã—2) can outperform both fewshot\\nprompting (for multihop ) and expert human reasoning (for react ; adapted slightly from Yao et al.\\n(2022) to our retrieval setting). Perhaps most importantly, we can make llama2-13b-chat compet-\\nitive with GPT-3.5 by simply compiling our programs.\\nTo assess the finetuning capacity of DSPy, we also evaluated the compiler multihop t5defined\\nabove which produces a T5-Large (770M parameter) model. This program scores 39.3% answer\\nEM and 46.0% passage accuracy on the dev set, using only 200 labeled inputs and 800 unlabeled\\n10Preprint\\nTable 2: Results with in-context learning on HotPotQA multi-hop retrieval question answering. We\\nreport answer exact match (Ans) and pair-retrieval accuracy (Psg). Each row represents a separate\\npipeline: the module in the Program column is compiled against the examples in the Training set.\\nThe programs, compilers, and (small) training sets are defined in the main text. For HotPotQA, we\\nuse the training set (and not dev) directly for cross-validation.âˆ—The marked result is evaluated on',\n",
              "  'question': 'In terms of passage recall, how does the ColBERTv2 retriever differ between the chain-of-thought and retrieval-augmented generation (CoTRAG) program and the simple multihop program?\\n\\n',\n",
              "  'answer': 'The chain-of-thought and retrieval-augmented generation (CoTRAG) program relies entirely on the ColBERTv2 retriever to find relevant passages directly from the original questions, limiting its passage recall. On the other hand, the simple multihop program generates queries for the retriever in multiple iterative \"hops\", which improves its passage recall.',\n",
              "  'source_doc': 'DSPy_  Compiling Declarative Language Model Calls into Self-Improving Pipelines.pdf'},\n",
              " {'context': 'GPT-2 small - Megatron-LM 1k 18.2 4.7 days (1.0 \\x02)\\nGPT-2 small - FlashAttention 1k 18.2 2.7 days (1.7\\x02)\\nGPT-2 small - FlashAttention 2k 17.6 3.0 days (1.6 \\x02)\\nGPT-2 small - FlashAttention 4k 17.5 3.6 days (1.3\\x02)\\nLong Document Classiï¬cation. Training Transformers with longer sequences with FlashAttention\\nimproves performance on the MIMIC-III [ 47] and ECtHR [ 6,7] datasets. MIMIC-III contains intensive care\\nunit patient discharge summaries, each annotated with multiple labels. ECtHR contains legal cases from the\\n3LRA accuracy results are known to be highly dependent on the tuning procedure [ 90]. Our reproduced baselines perform\\nbetter than as reported in the original comparison [80].\\n8Attention Memory Usage\\nSequence LengthAttention Runtime (Fwd Pass + Bwd Pass)\\nSequence LengthRuntime (ms)\\nMemory Footprint (GB)256 8K 16K 32K 64K 128 256 512 1024 2048 4096101102\\n1020\\nFlashAttention\\nBlock-Sparse FlashAttentionPyTorch Attention\\nMegatron AttentionLinformer Attention\\nOpenAI Sparse Attention8192100Crossover Points\\n20x2xFigure 3: Left:runtime of forward pass + backward pass. Right:attention memory usage.\\nEuropean Court of Human Rights, each of which is mapped to articles of the Convention of Human Rights\\nthat were allegedly violaged. Both of these datasets contain very long text documents; the average number of\\ntokens in MIMIC is 2,395 tokens, and the longest document contains 14,562 tokens, while the average and\\nlongest numbers in ECtHR are 2,197 and 49,392, respectively. We evaluate lift from increasing the sequence\\nlength of a pretrained RoBERTa model [56] (we repeat the positional embeddings, as in Beltagy et al. [3]).\\nTable 5 shows that sequence length 16K outperforms length 512 by 4.3 points on MIMIC, and that length\\n8K outperforms length 512 by 8.5 points on ECtHR. The discrepancies may be due to subtle distribution\\nshifts: MIMIC-III contains specialized medical text and thus may be more susceptible to a distribution shift',\n",
              "  'question': 'Given that increasing the sequence length of a pretrained RoBERTa model improves performance on the MIMIC-III and ECtHR datasets, what is the expected improvement in performance when using a sequence length of 16K compared to 512 on the MIMIC-III dataset?\\n\\n',\n",
              "  'answer': 'According to the data presented in Table 5, increasing the sequence length from 512 to 16K on the MIMIC-III dataset results in an improvement of 4.3 points. Therefore, it can be expected that using a sequence length of 16K on the MIMIC-III dataset would result in a similar improvement in performance compared to using a sequence length of 512.',\n",
              "  'source_doc': 'FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf'},\n",
              " {'context': 'vigilance is required for safe use of code generation systems\\nlike Codex.\\nWe note several immediate ways to improve safety in the\\nsubsection on risk mitigation below, though over-reliance\\nin particular is one that we believe merits further inquiry\\nin industry and academia. While it is conceptually straight-\\n1We sought to include harms spanning geographic and temporal\\nscales. We also considered not only the severity and probability,\\nbut also the distribution of harms. However, we note that the\\nanalysis described here is only one milestone in what we hope will\\nbe a larger cross-sectoral and cross-organizational effort to steer\\ncode generation in a societally beneï¬cial direction. As we describe\\nour ï¬ndings, we note various speciï¬c uncertainties and areas for\\nfuture work in different sections.\\nFigure 12. When the prompt includes subtle bugs, Codex tends to\\nproduce worse code than it is capable of. This persists when the\\nprompt also includes instructions to write correct code. This gap\\nincreases with model size.\\nforward to provide documentation to users reminding them\\nabout model limitations, empirical investigation is neces-\\nsary in order to identify how to reliably ensure vigilance in\\npractice across a range of user experience levels, UI designs,\\nand tasks. One challenge researchers should consider is that\\nas capabilities improve, it may become increasingly difï¬cult\\nto guard against â€œautomation bias.â€\\n7.2. Misalignment\\nAs with other large language models trained on a next-token\\nprediction objective, Codex will generate code that is as sim-\\nilar as possible to its training distribution. One consequence\\nof this is that such models may do things that are unhelpful\\nfor the user, despite having the capability to be more helpful\\n(see Figure 12). For example, if the user has some subtle\\nmistakes in their code, Codex may â€œdeliberatelyâ€ suggest\\ncode that superï¬cially appears good but is incorrect.\\nThis is an alignment failure - the model is not aligned with',\n",
              "  'question': 'In what ways can the safe use of code generation systems like Codex be improved?\\n\\n',\n",
              "  'answer': \"The safe use of code generation systems like Codex can be improved by implementing various measures such as providing documentation to users reminding them about model limitations, conducting empirical investigations to identify how to reliably ensure vigilance in practice across a range of user experience levels, UI designs, and tasks, and considering challenges such as automation bias. Additionally, researchers should focus on aligning the model with the user's needs and goals.\",\n",
              "  'source_doc': 'Evaluating Large Language Models Trained on Code.pdf'},\n",
              " {'context': 'NaturalQuestions TriviaQA\\nR@5 R@20 R@100 R@5 R@20 R@100\\nInverse Cloze Task (Sachan et al., 2021) 32.3 50.9 66.8 40.2 57.5 73.6\\nMasked salient spans (Sachan et al., 2021) 41.7 59.8 74.9 53.3 68.2 79.4\\nBM25 (Ma et al., 2021) - 62.9 78.3 - 76.4 83.2\\nContriever 47.867.882.1 59.474.283.2\\nsupervised model: DPR (Karpukhin et al., 2020) - 78.4 85.4 - 79.4 85.0\\nsupervised model: FiD-KD (Izacard & Grave, 2020a) 73.8 84.3 89.3 77.0 83.6 87.7\\n4.2 Baselines\\nFirst, we compare Contriever to BM25, which does not require supervision. On QA datasets, we compare to\\ndense retrievers trained with ICT and the Masked Salient Spans from Sachan et al. (2021). On BEIR, we\\nconsider the retriever from REALM (Guu et al., 2020), and RoBERTa large ï¬ne-tuned with SimCSE (Gao\\netal.,2021), asunsuperviseddenseretrievers. WealsocomparetoML-basedretrieverstrainedonMSMARCO,\\nclassiï¬ed in three categories: sparse, dense and late-interaction. For sparse methods, we compare to Splade\\nv2(Formal et al., 2021), which computes sparse representations of documents with BERT pre-trained\\nmodel. For dense methods, we use DPR(Karpukhin et al., 2020) and ANCE(Xiong et al., 2020), which\\nare bi-encoders trained on supervised data such as NaturalQuestions or MS MARCO. We also compare\\ntoTAS-B(HofstÃ¤tter et al., 2021), which performs distillation from a cross-encoder to a bi-encoder, and\\nGenQ, which creates synthetic query-document pairs with a generative model.1For late-interaction, we use\\nColBERT (Khattab et al., 2020), which computes pairwise scores between contextualized representations of\\nqueries and documents, as well as a cross-encoder used to re-rank documents retrieved with BM25.\\n4.3 Results\\nFirst, we compare the performance of fully unsupervised models, i.e., without ï¬ne-tuning on MS MARCO or\\nother annotated data. In Table 1, we report the retrieval performance on two question answering datasets:\\nNaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017). Here, our model is competitive',\n",
              "  'question': 'How does the performance of supervised and unsupervised models compare on NaturalQuestions and TriviaQA datasets?\\n\\n',\n",
              "  'answer': 'On the NaturalQuestions and TriviaQA datasets, the unsupervised models, such as Splade v2, TAS-B, and GenQ, perform competitively with the supervised models, such as DPR and FiD-KD. However, the supervised models, such as DPR and FiD-KD, outperform the unsupervised models in terms of retrieval performance.',\n",
              "  'source_doc': 'Unsupervised Dense Information Retrieval with Contrastive Learning.pdf'},\n",
              " {'context': 'programs }, 2020.\\n[7]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-\\nwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\\nDaniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,\\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,\\nAlec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners.\\narXiv:2005.14165 [cs] , July 2020.\\n[8]Nasser Esmaili, Claude Sammut, and GM Shirazi. Behavioural cloning in control of a dynamic\\nsystem. In 1995 IEEE International Conference on Systems, Man and Cybernetics. Intelligent\\nSystems for the 21st Century , volume 3, pages 2904â€“2909. IEEE, 1995.\\n[9]Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed-\\nings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:\\nLong Papers) , pages 889â€“898, 2018.\\n[10] Zihao Fu, Wai Lam, Anthony Man-Cho So, and Bei Shi. A theoretical analysis of the repetition\\nproblem in text generation. In Proceedings of the AAAI Conference on Artificial Intelligence ,\\nvolume 35, pages 12848â€“12856, 2021.\\n[11] Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, and Stefano Ermon. IQ-\\nLearn: Inverse soft-Q learning for imitation. In NeurIPS , 2021.\\n[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\\nAaron Courville, and Yoshua Bengio. Generative adversarial networks. Neural Information\\nProcessing Systems (NeurIPS) , 2014.\\n[13] Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas\\nSteiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2020.\\n[14] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural\\ninformation processing systems , 29, 2016.',\n",
              "  'question': 'Can programs be considered few-shot learners?\\n\\n',\n",
              "  'answer': 'Yes, programs can be considered few-shot learners. According to the paper \"Language Models are Few-Shot Learners\" by Tom B. Brown et al., language models are capable of learning new concepts and tasks with very few examples, making them few-shot learners. Similarly, in the field of robotics, behavioral cloning algorithms can learn to control a dynamic system with limited data, as demonstrated in the paper \"Behavioural cloning in control of a dynamic system\" by Nasser Esmaili et al. Furthermore, hierarchical neural story generation models can generate coherent and meaningful stories with a small number of training examples, as shown in the paper \"Hierarchical neural story generation\" by Angela Fan et al. Additionally, inverse soft-Q learning algorithms can learn to imitate complex behaviors with limited data, as demonstrated in the paper \"IQ-Learn: Inverse soft-Q learning for imitation\" by Divyansh Garg et al. Finally, generative adversarial networks can learn to generate realistic images or text with only a few examples, as presented in the paper \"Generative adversarial networks\" by Ian Goodfellow et al.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'Brazau and his attorneys. But prosecutors also vowed to pursue additional charges at a\\nsentencing hearing in mid-June, when a trial, if it progresses, will take place. Walcott\\nsaid the judge showed \"[He] showed extraordinary competency, and delivered on his word.\" A\\nhearing for the man in last Julyâ€™s trial is set for Dec. 15. Facing charges of criminal\\nharassment and incitement to hatred and incitement to hatred of others, Brazau refused\\nto come out of his lawyerâ€™s office before the trial began on March 1, but was found to\\nbe a witness at the first witness stand, a guilty plea hearing set for June 18. During\\nthe first stand, Brazau was accused of giving contradictory and false information about\\nIslam and Muslims that could back up a reference to Islam being \"genocidal,\" \"indecent\\nand ultimately un-Islamic,\" and \"not innocent.\" Brazauâ€™s lawyer went on to state: \"It\\ndoesnâ€™t matter if youâ€™re in the community, church, synagogue, anywhere, Iâ€™m not a threat\\nto anyone. Not a threat to God, nor the communities in the city of College Park, Iâ€™m\\na threat to others, because I care deeply about the faith of these people.\" The city of\\nCollege Park is the home of the University of Toronto and the University of Ottawa, as\\nwell as the Little Ethiopia Peek Park in southern Ontario. The court ordered that the\\nflyer stop promoting hate\\nÃ—Some Seattle businesses closed for â€™A Day Without Immigrantsâ€™, but others decided\\nagainst it SEATTLE | While there is no official list of local businesses participating in\\nthis movement, \"A Day Without Immigrants\", we did find some businesses that had closed\\ntheir doors and posted signs up saying they were participating. In fact, one business\\nowner says heâ€™s doing things a little bit differently. Instead of shutting down, he\\nsays he is choosing to pay it forward. Please enable Javascript to watch this video\\nEdward Moran moved from Mexico to the United States in 1984. He opened El Norte Lounge',\n",
              "  'question': \"What does Brazau's lawyer argue in the first witness stand?\\n\",\n",
              "  'answer': \"Brazau's lawyer argues that Brazau is not a threat to anyone, not even God or the communities in the city of College Park, but rather a threat to others because he cares deeply about their faith.\",\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'SCAN (Chen et al., 2020; Liu et al., 2020; Nye et al., 2020; Shaw et al., 2021), they require compli-\\ncated model training and grammar inference algorithms to search in a large grammar space. Another\\nline of work on SCAN designs data augmentation schemes (Andreas, 2020; Aky Â¨urek et al., 2021;\\nLake, 2019). Both Andreas (2020) and Aky Â¨urek et al. (2021) construct synthetic training samples by\\nrecombining fragments occurring in different training samples, and Aky Â¨urek et al. (2021) further de-\\nsigns a sampling scheme that encourages the recombination model to produce rare samples. On the\\nother hand, Lake (2019) proposed a meta training algorithm, which requires a meta-grammar space\\nto construct training data, and the format of sampled grammars is similar to the SCAN grammar.\\nWhile these data augmentation techniques improve the performance on several compositional gener-\\nalization benchmarks, they fail to solve the length split of SCAN. Other prior works propose neural\\nnetwork architectures to improve compositional generalization, where they encourage the model to\\nlearn the word and span mapping (Russin et al., 2019; Li et al., 2019), the alignment of input and\\noutput as span trees (Herzig & Berant, 2021), and the permutation equivariance of input and output\\nwords (Gordon et al., 2020). Still, these end-to-end neural networks without symbolic components\\ndo not generalize to longer test inputs. Unlike the existing work, we demonstrate that without model\\narchitectures and symbolic components specially designed to improve compositional generalization,\\nleast-to-most prompting achieves 99:7%accuracy on any split (including length split) with only a\\nhandful of demonstration examples, and it does not require any training or ï¬netuning.\\nEasy-to-hard generalization . In addition to compositional generalization, there are many other\\ntasks where the test cases require more reasoning steps to solve than the training examples, for',\n",
              "  'question': 'What is the problem with the existing data augmentation techniques and neural network architectures for achieving compositional generalization in SCAN?\\n\\n',\n",
              "  'answer': 'The existing data augmentation techniques and neural network architectures for achieving compositional generalization in SCAN have limitations. Specifically, they fail to solve the length split of SCAN and do not generalize to longer test inputs. Additionally, they require complicated model training and grammar inference algorithms, as well as symbolic components, which increase the complexity of the models. In contrast, the proposed least-to-most prompting approach achieves high accuracy on any split, including the length split, with only a few demonstration examples and does not require any training or fine-tuning.',\n",
              "  'source_doc': 'Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.pdf'},\n",
              " {'context': 'its result, respectively, as:\\ne(c) =<API>ac(ic) </API>\\ne(c;r) =<API>ac(ic)!r</API>\\nwhere â€œ <API> â€, â€œ</API> â€ and â€œ!â€ are special\\ntokens.1Some examples of linearized API calls\\ninserted into text sequences are shown in Figure 1.\\nGiven a datasetC=fx1;:::; xjCjgof plain\\ntexts, we ï¬rst convert this dataset into a dataset\\nC\\x03augmented with API calls. This is done in three\\nsteps, illustrated in Figure 2: First, we exploit the\\nin-context learning ability of Mto sample a large\\nnumber of potential API calls. We then execute\\nthese API calls and ï¬nally check whether the ob-\\ntained responses are helpful for predicting future\\ntokens; this is used as a ï¬ltering criterion. After\\nï¬ltering, we merge API calls for different tools,\\nresulting in the augmented dataset C\\x03, and ï¬netune\\n1In practice, we use the token sequences â€œ [â€, â€œ]â€ and\\nâ€œ->â€ to represent â€œ <API> â€, â€œ</API> â€ and â€œ!â€, respec-\\ntively. This enables our approach to work without modifying\\nthe existing LMâ€™s vocabulary. For reasons of readability, we\\nstill refer to them as â€œ <API> â€, â€œ</API> â€ and â€œ!â€ through-\\nout this section.Your task is to add calls to a Question Answering API to a \\npiece of text. The questions should help you get \\ninformation required to complete the text. You can call the \\nAPI by writing \"[QA(question)]\" where \"question\" is the \\nquestion you want to ask. Here are some examples of API \\ncalls: \\nInput:  Joe Biden was born in Scranton, Pennsylvania. \\nOutput:  Joe Biden was born in  [QA(\"Where was Joe  \\nBiden born?\")]  Scranton, [QA(\"In which state is  \\nScranton?\")]  Pennsylvania. \\nInput:  Coca-Cola, or Coke, is a carbonated soft drink \\nmanufactured by the Coca-Cola Company. \\nOutput: Coca-Cola, or [QA(\"What other name is  \\nCoca-Cola known by?\")]  Coke, is a carbonated soft drink \\nmanufactured by [QA(\"Who manufactures Coca-Cola?\")]  \\nthe Coca-Cola Company. \\nInput:  x \\nOutput: Figure 3: An exemplary prompt P(x)used to generate\\nAPI calls for the question answering tool.',\n",
              "  'question': 'How do we add calls to a Question Answering API to a piece of text to get information required to complete the text?\\n\\n',\n",
              "  'answer': 'To add calls to a Question Answering API to a piece of text, we can use the API by writing \"[QA(question)]\" where \"question\" is the question we want to ask. We can call the API by providing the text as input and specifying the API calls required to get the information we need to complete the text. Examples of API calls include asking for the location of a person, the name of a product, or the manufacturer of a product.',\n",
              "  'source_doc': 'Toolformer_ Language Models Can Teach Themselves to Use Tools.pdf'},\n",
              " {'context': 'from the same vendors, but do not undergo a screening test.\\nDespite the complexity of the task, we ï¬nd that inter-annotator agreement rates are quite high:\\ntraining labelers agree with each-other 72:6\\x061:5%of the time, while for held-out labelers this\\nnumber is 77:3\\x061:3%. For comparison, in the summarization work of Stiennon et al. (2020)\\nresearcher-researcher agreement was 73\\x064%.\\n3.5 Models\\nWe start with the GPT-3 pretrained language models from Brown et al. (2020). These models are\\ntrained on a broad distribution of Internet data and are adaptable to a wide range of downstream tasks,\\nbut have poorly characterized behavior. Starting from these models, we then train models with three\\ndifferent techniques:\\nSupervised ï¬ne-tuning (SFT). We ï¬ne-tune GPT-3 on our labeler demonstrations using supervised\\nlearning. We trained for 16 epochs, using a cosine learning rate decay, and residual dropout of 0.2.\\nWe do our ï¬nal SFT model selection based on the RM score on the validation set. Similarly to Wu\\net al. (2021), we ï¬nd that our SFT models overï¬t on validation loss after 1 epoch; however, we ï¬nd\\nthat training for more epochs helps both the RM score and human preference ratings, despite this\\noverï¬tting.\\nReward modeling (RM). Starting from the SFT model with the ï¬nal unembedding layer removed,\\nwe trained a model to take in a prompt and response, and output a scalar reward. In this paper we\\nonly use 6B RMs, as this saves a lot of compute, and we found that 175B RM training could be\\nunstable and thus was less suitable to be used as the value function during RL (see Appendix C for\\nmore details).\\nIn Stiennon et al. (2020), the RM is trained on a dataset of comparisons between two model outputs\\non the same input. They use a cross-entropy loss, with the comparisons as labelsâ€”the difference in\\nrewards represents the log odds that one response will be preferred to the other by a human labeler.',\n",
              "  'question': 'What is the inter-annotator agreement rate for training labelers and held-out labelers in the study?\\n\\n',\n",
              "  'answer': 'The inter-annotator agreement rate for training labelers is 72:6:5% and for held-out labelers it is 77:3:3%.',\n",
              "  'source_doc': 'Training language models to follow instructions with human feedback.pdf'},\n",
              " {'context': 'tions (by up to 5â€“46% and 16â€“40%, respectively). On top of that, DSPy pro-\\ngrams compiled to open and relatively small LMs like 770M-parameter T5and\\nllama2-13b-chat are competitive with approaches that rely on expert-written\\nprompt chains for proprietary GPT-3.5 .\\nDSPy is available at https://github.com/stanfordnlp/dspy .\\n1 I NTRODUCTION\\nLanguage models (LMs) are enabling researchers to build NLP systems at higher levels of abstrac-\\ntion and with lower data requirements than ever before (Bommasani et al., 2021). This is fueling an\\nexploding space of â€œpromptingâ€ techniquesâ€”and lightweight finetuning techniquesâ€”for adapting\\nLMs to new tasks (Kojima et al., 2022), eliciting systematic reasoning from them (Wei et al., 2022;\\nWang et al., 2022b), and augmenting them with retrieved sources (Guu et al., 2020; Lazaridou et al.,\\n2022; Khattab et al., 2022) or with tools (Yao et al., 2022; Schick et al., 2023). Most of these tech-\\nniques are explored in isolation, but interest has been growing in building multi-stage pipelines and\\nagents that decompose complex tasks into more manageable calls to LMs in an effort to improve\\nperformance (Qi et al., 2019; Khattab et al., 2021a; Karpas et al., 2022; Dohan et al., 2022; Khot\\net al., 2022; Khattab et al., 2022; Chen et al., 2022; Pourreza & Rafiei, 2023; Shinn et al., 2023).\\nUnfortunately, LMs are known to be sensitive to how they are prompted for each task, and this is\\nexacerbated in pipelines where multiple LM calls have to interact effectively. As a result, the LM\\n1arXiv:2310.03714v1  [cs.CL]  5 Oct 2023Preprint\\ncalls in existing LM pipelines and in popular developer frameworks are generally implemented using\\nhard-coded â€˜prompt templatesâ€™, that is, long strings of instructions and demonstrations that are hand\\ncrafted through manual trial and error. We argue that this approach, while pervasive, can be brittle\\nand unscalableâ€”conceptually akin to hand-tuning the weights for a classifier. A given string prompt',\n",
              "  'question': 'Can multi-stage pipelines and agents that decompose complex tasks into more manageable calls to LMs improve performance compared to existing LM pipelines and developer frameworks?\\n\\n',\n",
              "  'answer': 'Yes, multi-stage pipelines and agents that decompose complex tasks into more manageable calls to LMs can improve performance compared to existing LM pipelines and developer frameworks. This is because they allow for more flexibility and adaptability in how LMs are prompted for each task, which can lead to better performance and scalability. Additionally, these approaches can leverage the strengths of different LMs and prompting techniques to improve overall performance.',\n",
              "  'source_doc': 'DSPy_  Compiling Declarative Language Model Calls into Self-Improving Pipelines.pdf'},\n",
              " {'context': 'example, in sorting, while for P= 32 GoT only negligibly\\nimproves upon ToT2, its median error count becomes lower\\nbyâ‰ˆ61% for P= 64 andâ‰ˆ69% for P= 128 . The quar-\\ntiles also become respectively better. The results for other\\nschemes also follow the intuition; for example, IO becomesconsistently worse with the increasing P, which is expected\\nas a single thought is unlikely to solve a large problem in-\\nstance. Overall, this analysis illustrates that GoT is indeed\\nwell-suited for elaborate problem cases , as the execution\\nschedules usually become more complex with the growing\\nproblem sizes.\\n7.3 Discussion on Task Decomposition\\nWhen splitting a task into subtasks and then solving these\\nsubtasks, the size of responses and the input (in tokens) are\\nreduced proportionally to the degree of task decomposition.\\nHowever, the â€œstaticâ€ part of the prompt (i.e., few-shot ex-\\namples) may become a significant overhead (see GoT4 to\\nGoT8 in Figure 7). Here, we observe that these few-shot ex-\\namples can usually also be reduced in size (e.g., the passages\\nused to demonstrate keyword counting can also be made\\nsmaller and still be indicative of the actual input size), thus\\nactively working towards decreasing the cost (e.g., see the\\ndifference between GoT8 and GoTx in Figure 7).\\nThe overall goal when conducting graph decomposition is\\nto break down a task to the point, where the LLM can solve\\nit correctly for the majority of time using a single prompt\\n(or with a few additional improvement steps). This signifi-\\ncantly lowers the number of improvement/refinement steps\\nneeded during the later stages of the graph exploration. Fur-\\nthermore, as indicated by our results, combining or concate-\\nnating sub-results is usually an easier task than solving large\\ntask instances from scratch. Hence, the LLM is often suc-\\ncessful when aggregating the final solution.\\n8 Related Work\\nWe summarize relations between GoT and related work.\\n8.1 Prompting Paradigms & Approaches',\n",
              "  'question': 'What is the effect of task decomposition on the size of responses and input in LLM performance?\\n\\n',\n",
              "  'answer': 'Task decomposition can reduce the size of responses and input in LLM performance by proportionally reducing the degree of task decomposition. However, the \"static\" part of the prompt, such as few-shot examples, may become a significant overhead. Reducing the size of these examples can help decrease the cost, leading to lower improvement/refinement steps needed during the later stages of graph exploration. Additionally, combining or concatenating sub-results is often easier than solving large task instances from scratch, making the LLM successful in aggregating the final solution.',\n",
              "  'source_doc': 'Graph of Thoughts_ Solving Elaborate Problems with Large Language Models.pdf'},\n",
              " {'context': 'and 6).\\n7.2 Analysis of GoTâ€™s Advantages\\nThe results of analysis are in Figure 5 (sorting), 6 (set inter-\\nsection), 7 (keyword counting), and 8 (document merging);\\nsee Section 5 for the description of specific use cases. Over-\\nall, GoT improves the quality of outcomes over all the con-\\nsidered baselines and it reduces inference costs compared to\\nToT.\\nGoT vs. ToT GoT improves upon ToT and ToT2 by a\\nlarge margin over all the considered problem instances. ToT\\nusually comes with somewhat higher quality than ToT2, but\\nsimultaneously much higher costs. GoTâ€™s costs are always\\nlower than ToT, and comparable (in some cases lower, in\\nothers higher) than ToT2. For example, it reduces median er-\\nror by â‰ˆ62%, thereby achieving a higher quality of sorting,\\nforP= 128 in comparison to ToT while ensuring >31%\\ncost reductions. These advantages are due to GoTâ€™s ability\\nto decompose complex tasks into simpler sub-tasks, solve\\nthese sub-tasks independently, and then incrementally merge\\nthese outcomes into the final result.\\nGoT vs. IO and CoT GoT consistently delivers much\\nhigher quality of outcomes than IO/CoT. For example, for\\nsorting ( P= 64 ), GoTâ€™s median error is â‰ˆ65% and â‰ˆ83%\\nlower than, respectively, CoT and IO. Yet, the costs of GoT\\nâ€“ and ToT â€“ are much higher than in IO and CoT. This is\\nmostly due to our configuration of CoT, where we do not ar-\\ntificially inflate the lengths of the chains of reasoning if this\\ndoes not improve the outcomes. The higher costs of GoT and\\nToT are driven by knew thoughts built for each Generate\\noperation; these multiple thoughts are one of the reasons for\\nGoTâ€™s superiority in quality.\\nIncreasing Complexity of Tackled Problems Most im-\\nportantly, the advantages of GoT in the quality increase for\\nall the baselines with the growing size of the problem P. For\\nexample, in sorting, while for P= 32 GoT only negligibly\\nimproves upon ToT2, its median error count becomes lower\\nbyâ‰ˆ61% for P= 64 andâ‰ˆ69% for P= 128 . The quar-',\n",
              "  'question': 'What is the main advantage of GoT compared to the other baselines and problem instances mentioned in the analysis?\\n\\n',\n",
              "  'answer': \"GoT's main advantage is its ability to decompose complex tasks into simpler sub-tasks, solve these sub-tasks independently, and then incrementally merge the outcomes into the final result. This leads to higher quality of outcomes and lower costs compared to the other baselines and problem instances.\",\n",
              "  'source_doc': 'Graph of Thoughts_ Solving Elaborate Problems with Large Language Models.pdf'},\n",
              " {'context': '[39] S. Zhang, Z. Chen, Y . Shen, M. Ding, J. B. Tenenbaum, and C. Gan. Planning with large\\nlanguage models for code generation. In The Eleventh International Conference on Learning\\nRepresentations , 2023. URL https://openreview.net/forum?id=Lr8cOOtYbfL .\\n11',\n",
              "  'question': 'In the context of the paper \"Planning with large language models for code generation\", what is the purpose of using language models in code generation?\\n\\n',\n",
              "  'answer': \"The purpose of using language models in code generation is to improve the quality and efficiency of the generated code, by leveraging the model's ability to understand and generate natural language instructions.\",\n",
              "  'source_doc': 'Tree of Thoughts_  Deliberate Problem Solving with Large Language Models.pdf'},\n",
              " {'context': 'NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017). Here, our model is competitive\\nwith a strong BM25 baseline (Ma et al., 2021), for example leading to 3 points improvement for the recall@100\\non NaturalQuestions. It also outperforms previously proposed dense retrievers which were trained with ICT\\nor salient span masking. In Figure 1 we report the recall@100 performance of unsupervised models on the\\nBEIR benchmark. Interestingly, we observe that in this setting, Contriever is competitive compared to BM25\\non all datasets, but TREC-COVID and TÃ³uche-2020. In particular, it obtains better performance than BM25\\non 11 out of 15 datasets from the benchmark for the recall@100. Contriever also outperforms previously\\nproposed unsupervised dense retrievers, which obtains lower performance than BM25 in general. For the\\nnDCG@10, which puts more emphasis on the very ï¬rst retrieved documents, while Contriever largely closes\\nthe gap between unsupervised retrievers and BM25, it is still outperformed by BM25 as reported in Table 11.\\nThe diï¬€erence is mainly due to the fact that BM25 largely outperforms Contriever on two datasets with\\nspeciï¬c features: Trec-COVID and TÃ³uche-2020. Trec-COVID is an information retrieval dataset related to\\nCOVID. However data used to train Contriever were collected before the COVID outbreak, thus they may\\nnot be adapted. TÃ³uche-2020 contains long documents, which does not seem to be very well supported by\\ndense neural retrievers: even after supervised training, models are still lagging behind BM25. Overall, these\\nresults show the potential of contrastive learning to train fully unsupervised dense retrievers.\\n1GenQ thus leads to one diï¬€erent model for each dataset.\\n7Published in Transactions on Machine Learning Research (08/2022)\\nTable 2:BEIR Benchmark. We report nDCG@10 on the test sets from the BEIR benchmark for bi-encoder\\nmethods without re-ranker. We also report the average and number of datasets where a method is the best',\n",
              "  'question': 'Can you explain the performance of the Contriever model on the BEIR benchmark?\\n\\n',\n",
              "  'answer': 'The Contriever model performs competitively on the BEIR benchmark, outperforming previously proposed unsupervised dense retrievers and achieving better performance than BM25 on 11 out of 15 datasets for recall@100. However, it is still outperformed by BM25 on two datasets with specific features: Trec-COVID and TÃ³uche-2020. The difference is mainly due to the fact that BM25 outperforms Contriever on these two datasets. Overall, these results show the potential of contrastive learning to train fully unsupervised dense retrievers.',\n",
              "  'source_doc': 'Unsupervised Dense Information Retrieval with Contrastive Learning.pdf'},\n",
              " {'context': 'FLAN v2 for different data types. Overall, NF4 with double quantization (DQ) matches BFloat16 performance,\\nwhile FP4 is consistently one percentage point behind both.\\nMean 5-shot MMLU Accuracy\\nLLaMA Size 7B 13B 33B 65B Mean\\nDataset Alpaca FLAN v2 Alpaca FLAN v2 Alpaca FLAN v2 Alpaca FLAN v2\\nBFloat16 38.4 45.6 47.2 50.6 57.7 60.5 61.8 62.5 53.0\\nFloat4 37.2 44.0 47.3 50.0 55.9 58.5 61.3 63.3 52.2\\nNFloat4 + DQ 39.0 44.5 47.5 50.7 57.3 59.2 61.8 63.9 53.1\\non a challenging Natural Language Understanding benchmark (MMLU) and develop new methods\\nfor real-world chatbot performance evaluation.\\n5.1 Experimental setup\\nWe now describe an overview of the experimental setup with full details in Appendix B.\\nData As, to our knowledge, there is no comprehensive study of recent instruction-following datasets,\\nwe select eight recent datasets. We include datasets obtained through crowd-sourcing (OASST1 [ 31],\\nHH-RLHF [ 4]), distillation from instruction-tuned models (Alpaca [ 55], self-instruct [ 59], unnatural-\\ninstructions [ 26]), corpora aggregations (FLAN v2 [ 12]), as well as hybrids (Chip2 [ 32], Long-\\nform [30]). These datasets cover different languages, data sizes, and licenses.\\nTraining Setup To avoid confounding effects from different training objectives, we perform QLoRA\\nfinetuning with cross-entropy loss (supervised learning) without reinforcement learning, even for\\ndatasets that include human judgments of different responses. For datasets that have a clear distinction\\nbetween instruction and response, we finetune only on the response (see ablations in Appendix B).\\nFor OASST1 and HH-RLHF, multiple responses are available. We then select the top response at\\nevery level of the conversation tree and finetune on the full selected conversation, including the\\ninstructions. In all of our experiments, we use NF4 QLORAwith double quantization and paged\\noptimizers to prevent memory spikes during gradient checkpointing. We do small hyperparameter',\n",
              "  'question': 'How does the performance of FLAN v2 compare to other data types on the MMLU benchmark?\\n\\n',\n",
              "  'answer': 'FLAN v2 performs similarly to BFloat16 and outperforms Float4 and NFloat4 + DQ on the MMLU benchmark.',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': 'task. The middle panel of Figure 3 shows how often retrieved documents contain the text of the correct\\nanswer option. There being at least one mention of the correct answer choice in 30% of test questions in the\\ntop 25 passages.6The right panel shows that the accuracy on MMLU increases when the correct answer\\noption text occurs more frequently in retrieved passages, rising from 55% for questions when the answer\\noption does not appear, to 77% for questions mentioned more than 15 times.\\nA human analysis of retrieved documents revealed that documents are helpful for answering questions in a\\nnumber of diï¬€erent ways. Manual inspection of a sample of 50 correctly-answered questions revealed that\\n44% contained at least partially useful background information. These are documents that would improve the\\nlikelihood of a non-expert human answering correctly, such as contextual clues surrounding a quotation from\\na question, or helpful numerical ï¬gures for quantity-based questions, which help to narrow down the answer\\noptions to a smaller range. In a further 26% of cases, a passage contained all the necessary information to\\nanswer the question, stated in a straightforward way. If read competently, such passages make the question\\nsimple to answer, and often include information such as canonical deï¬nitions, or the exact numerical answer\\nrequested in the question. 28% of retrieval sets did not contain obvious information which would make the\\nquestion easier. Finally, 2% contained the verbatim question in a passage, together with its answer.\\nGiven that MMLU has been created from pre-existing exams, it is possible that these questions appear on the\\nopen web. Models trained on web data (or, in our case, retrieving from it) run the risk of answering correctly\\nnot through generalisation, but by verbatim memorisation, which could lead to misleadingly high scores. In\\nsome very large language models, which can verbatim memorize and recall large parts of their pre-training',\n",
              "  'question': 'How does the frequency of occurrence of the correct answer option text in retrieved passages affect the accuracy of MMLU?\\n\\n',\n",
              "  'answer': 'The accuracy of MMLU increases when the correct answer option text occurs more frequently in retrieved passages. The accuracy rises from 55% for questions when the answer option does not appear, to 77% for questions mentioned more than 15 times.',\n",
              "  'source_doc': 'Atlas_  Few-shot Learning with Retrieval Augmented Language Models.pdf'},\n",
              " {'context': '3.4 Human data collection\\nTo produce our demonstration and comparison data, and to conduct our main evaluations, we hired\\na team of about 40 contractors on Upwork and through ScaleAI. Compared to earlier work that\\ncollects human preference data on the task of summarization (Ziegler et al., 2019; Stiennon et al.,\\n2020; Wu et al., 2021), our inputs span a much broader range of tasks, and can occasionally include\\ncontroversial and sensitive topics. Our aim was to select a group of labelers who were sensitive to the\\npreferences of different demographic groups, and who were good at identifying outputs that were\\npotentially harmful. Thus, we conducted a screening test designed to measure labeler performance\\non these axes. We selected labelers who performed well on this test; for more information about our\\nselection procedure and labeler demographics, see Appendix B.1.\\nDuring training and evaluation, our alignment criteria may come into conï¬‚ict: for example, when a\\nuser requests a potentially harmful response. During training we prioritize helpfulness to the user (not\\n7doing so requires making some difï¬cult design decisions that we leave to future work; see Section 5.4\\nfor more discussion). However, in our ï¬nal evaluations we asked labelers prioritize truthfulness and\\nharmlessness (since this is what we really care about).\\nAs in Stiennon et al. (2020), we collaborate closely with labelers over the course of the project. We\\nhave an onboarding process to train labelers on the project, write detailed instructions for each task\\n(see Appendix B.2), and answer labeler questions in a shared chat room.\\nAs an initial study to see how well our model generalizes to the preferences of other labelers, we hire\\na separate set of labelers who do not produce any of the training data. These labelers are sourced\\nfrom the same vendors, but do not undergo a screening test.\\nDespite the complexity of the task, we ï¬nd that inter-annotator agreement rates are quite high:',\n",
              "  'question': 'Given the context, what is the purpose of the labelers in the study?\\n\\n',\n",
              "  'answer': 'The purpose of the labelers in the study is to select a group of individuals who are sensitive to the preferences of different demographic groups and good at identifying outputs that are potentially harmful. They are used to conduct a screening test and collaborate closely with the researchers over the course of the project. Additionally, a separate set of labelers who do not produce any of the training data are hired to see how well the model generalizes to the preferences of other labelers.',\n",
              "  'source_doc': 'Training language models to follow instructions with human feedback.pdf'},\n",
              " {'context': 'descriptions with a total of 210,289 examples.\\nB.2 Hyperparameters\\nWe provide the exact hyperparameters used in our QLORAfinetuning experiments. We find hyper-\\nparameters to be largely robust across datasets. We use the MMLU 5-shot dev set for validation\\nand hyperparameter tuning. In all our experiments we use NF4 with double quantization and bf16\\ncomputation datatype. We set LoRA r= 64 ,Î±= 16 , and add LoRA modules on all linear layers of\\nthe base model. We also use Adam beta2 of 0.999, max grad norm of 0.3 and LoRA dropout of 0.1\\nfor models up to 13B and 0.05 for 33B and 65B models. Following previous work on instruction\\nfinetuning [ 62,60] and after benchmarking other linear and cosine schedules, we use a constant\\nlearning rate schedule. We use group-by-length to group examples of similar lengths in the same\\nbatch (note this will produce a oscillating loss curve). The hyperparameters we tune for each model\\nsize are shown in Table 9.\\nB.3 Ablations\\nWhile it is general practice in the literature to only train on the response in instruction following\\ndatasets, we study the effect of training on the instruction in addition to the response in Table 10. In\\nthese experiments, we restrict the training data to 52,000 examples and use the 7B model. Over four\\ndifferent instruction tuning datasets, we find that only training on the target is beneficial to MMLU\\n23Dataset Unnatural Instructions Chip2 Alpaca FLAN v2 Mean\\nTrain on source and target 36.2 33.7 38.1 42.0 37.5\\nTrain on target 38.0 34.5 39.0 42.9 38.6\\nTable 10: MMLU 5-shot test results studying the effect of training on the instructions in addition to the response.\\nperformance. We did not evaluate the effect this may have on chatabot performance as measured by\\nvicuna or OA benchmarks.\\nB.4 What is more important: instruction finetuning dataset size or dataset quality?\\nData set suitability is more important than dataset size. To understand the effects of dataset',\n",
              "  'question': 'How does the performance of MMLU models vary with the size and quality of the instruction finetuning dataset?\\n\\n',\n",
              "  'answer': 'The performance of MMLU models is more dependent on the quality of the instruction finetuning dataset than on its size. This is because the quality of the dataset determines how well the model can learn the patterns and relationships in the data, which in turn affects its ability to generate accurate responses. While larger datasets may provide more examples for the model to learn from, if the data is of poor quality or does not accurately represent the task at hand, the model may still struggle to perform well. Therefore, it is important to carefully select and curate the instruction finetuning dataset to ensure that it is of high quality and suitable for the task at hand.',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': 'However, this line of works can only scale up by using more special tokens in the prompt, which\\ntake up available sequence length for task tokens when positional embeddings are learned.\\nLow-Rank Structures in Deep Learning. Low-rank structure is very common in machine learn-\\ning. A lot of machine learning problems have certain intrinsic low-rank structure (Li et al., 2016;\\nCai et al., 2010; Li et al., 2018b; Grasedyck et al., 2013). Moreover, it is known that for many\\ndeep learning tasks, especially those with a heavily over-parametrized neural network, the learned\\nneural network will enjoy low-rank properties after training (Oymak et al., 2019). Some prior works\\neven explicitly impose the low-rank constraint when training the original neural network (Sainath\\net al., 2013; Povey et al., 2018; Zhang et al., 2014; Jaderberg et al., 2014; Zhao et al., 2016; Kho-\\ndak et al., 2021; Denil et al., 2014); however, to the best of our knowledge, none of these works\\nconsiders low-rank update to a frozen model for adaptation to downstream tasks . In theory liter-\\nature, it is known that neural networks outperform other classical learning methods, including the\\ncorresponding (ï¬nite-width) neural tangent kernels (Allen-Zhu et al., 2019; Li & Liang, 2018) when\\nthe underlying concept class has certain low-rank structure (Ghorbani et al., 2020; Allen-Zhu & Li,\\n2019; Allen-Zhu & Li, 2020a). Another theoretical result in Allen-Zhu & Li (2020b) suggests that\\nlow-rank adaptations can be useful for adversarial training. In sum, we believe that our proposed\\nlow-rank adaptation update is well-motivated by the literature.\\n7 U NDERSTANDING THE LOW-RANK UPDATES\\nGiven the empirical advantage of LoRA, we hope to further explain the properties of the low-rank\\nadaptation learned from downstream tasks. Note that the low-rank structure not only lowers the\\nhardware barrier to entry which allows us to run multiple experiments in parallel, but also gives',\n",
              "  'question': 'Can you explain the properties of the low-rank adaptation learned from downstream tasks using LoRA?\\n\\n',\n",
              "  'answer': 'The low-rank adaptation learned from downstream tasks using LoRA has several properties. Firstly, it lowers the hardware barrier to entry, allowing for multiple experiments to be run in parallel. Additionally, the low-rank structure of the learned neural network allows for it to enjoy low-rank properties after training, which can be useful for adversarial training. Furthermore, it has been shown that neural networks outperform other classical learning methods, including the corresponding (finite-width) neural tangent kernels, when the underlying concept class has certain low-rank structure. Overall, the low-rank adaptation update learned from downstream tasks using LoRA is well-motivated by the literature.',\n",
              "  'source_doc': 'LoRA_ Low-Rank Adaptation of Large Language Models.pdf'},\n",
              " {'context': 'demonstrate Atlascan zero-shot transfer backwards in time to 2017 eï¬€ectively too (50.1%). Interestingly,\\nT5 is unable answer questions from 2020 well, even when trained with 2020 answers (3.6%), likely because it\\nwas pre-trained on data pre-dating 2020 (Dodge et al., 2021).\\nWe also examine temporal eï¬€ects for NaturalQuestions. NaturalQuestions is a dataset composed of search\\nqueries collected via the Google search engine in a short period of time. Thus data have a strong temporal\\nbias, with a lot of questions about the 2018 World Cup for example. Moreover some questions are ambiguous\\nwithout speciï¬cation of the temporal context. For instance, for the question â€œwhen did ireland last beat\\nengland at twickenhamâ€ , the expected answer is 2018 in NaturalQuestions, while Ireland also beat England at\\nTwickenham in 2022 as well as many other times before. In Table 12, we report results obtained by ï¬netuning\\nAtlasusing diï¬€erent Wikipedia dumps for the index. We observe that the 2018 December Wikipedia dump,\\nwhich is close to the date of data collection, leads to the best results for both few-shot and full ï¬ne-tuning.\\nIn particular, it leads to a new state-of-the-art of 64 EM on NaturalQuestions.\\n17Table 11:Results on our TempLAMA-derived dataset. We report performance for a static, closed-book\\nT5-11B, as well as Atlas-11B supplied with a test-time Wikipedia index from 2017 or 2020. We evaluate\\nmodels ï¬netuned on a small training set of 248 time-sensitive cloze-question-answer pairs, using answers\\neither from 2017 or 2020. Good models should score highly when the test set year matches the year of the\\ntest-time index, and score low otherwise.\\n2017 Test Set Acc. 2020 Test Set Acc.\\nTrain Set Test-time Index Closed-book Atlas Closed-book Atlas\\n2017 answers2017 12.1 57.7 2.9 1.5\\n2020 12.1 10.2 2.9 53.1\\n2020 answers2017 4.8 50.1 3.6 4.2\\n2020 4.8 3.5 3.6 60.5\\nTable12:ImpactofindexdatatemporalityonNaturalQuestions. Wereportexactmatchperformance',\n",
              "  'question': 'Can we effectively transfer Atlascan knowledge backwards in time to 2017 and achieve a high accuracy of 50.1%?\\n\\n',\n",
              "  'answer': 'Yes, we can effectively transfer Atlascan knowledge backwards in time to 2017 and achieve a high accuracy of 50.1% by using the 2018 December Wikipedia dump as the index for fine-tuning the Atlas model. This led to a new state-of-the-art performance of 64 EM on NaturalQuestions.',\n",
              "  'source_doc': 'Atlas_  Few-shot Learning with Retrieval Augmented Language Models.pdf'},\n",
              " {'context': 'A prompt used byParser\\nGoal: Extract\\ninformation from\\nLLM\\'s thought Goal: Assess the\\nquality of the\\nLLM\\'s solution\\nControllerGoal: Initiate, coordinate, manage,\\nand progress the GoT executionExternal entity Prompt Thought\\nThought stateScore\\nOperation\\nThought state + its\\nassociated operationsThought state\\n+ thought\\'s scoreDependencyModule of the\\nGoT system Graph of\\nOperations\\nGoal: Specify\\nLLM thought\\ntransformations\\nGraph Reasoning State\\nGoal: Maintain\\nthe ongoing LLM\\nreasoning process\\nUser\\nGoal: Indicate the\\ntop-scoring thoughts\\nGraph of Operations enables seamless specification of not only\\nGoT, but also existing schemes such as CoT, CoT-SC, ToT\\nAPI for Prompter (extensible)\\nâ¡ Generate(t,k) //generate a prompt for k new thoughts, using thought tâ¡ //LLM params: model used, temperature, max tokens, api key, org, ...\\nâ¡ //LLM cost features: prompt token cost, response token cost, ...\\nâ¡ //Instances of Prompter + Parser + Graph of Operations,\\nâ¡ //Any additional input parameters (e.g., numbers to be sorted).\\n//Each of the above routines is responsible for parsing an LLM\\'s reply\\n//to a corresponding Prompter routine (e.g., ParseScore parses Score).â¡ Score(t) //score thought t\\nâ¡ Validate(t) //generate a prompt to validate the correctness of thought tâ¡ ValidateAndImprove(t) //generate a prompt to enhance thought t,\\nâ¡ Aggregate(t1,...,tk) //generate a prompt to combine thoughts t1, ..., tk API for Controller\\nAPI for Parser (extensible)\\nParseGenerate, ParseImprove, ParseScore,\\nParseAggregate, ParseValidate, ...â¡ Generate, Aggregate, Score, ... //see Prompter API\\nâ¡ KeepBest(N) //preserves N best scoring thoughts\\nâ¡ Repeat(k) //Repeat a given operation k times, generating k thoughts.\\n    //For example, this enables \"Aggregate\" to generate multiple outcomes\\n    //of the combination operation. Each such thought is maintained \\n   //within the Graph Reasoning State and scored individually.\\nGenerate(t,k=1)+Repeat(k=4)',\n",
              "  'question': \"Can you assess the quality of the LLM's solution based on the thought state, associated operations, and thought's score in the Graph of Operations?\\n\\n\",\n",
              "  'answer': \"Yes, the quality of the LLM's solution can be assessed based on the thought state, associated operations, and thought's score in the Graph of Operations. The thought state represents the current state of the LLM's reasoning process, while the associated operations represent the actions that the LLM is taking to update its understanding of the task at hand. The thought's score represents the confidence level of the LLM in its current understanding of the task. By considering all of these factors together, we can gain a comprehensive understanding of the quality of the LLM's solution.\",\n",
              "  'source_doc': 'Graph of Thoughts_ Solving Elaborate Problems with Large Language Models.pdf'},\n",
              " {'context': 'Ê±Ã§Ê²Ë¤\\x1dÄÃŠÄ‘Ä®Ë¤ÄµÆ™Ë¤\\x9aÄÄµÅ©ÄˆÄÆœË¤\\x89Å—ÄµÄ­Å”ÆœÄ‘Ä®ÄˆË¤Ê±\\x1dÄµÃ‰Ê²Ê±ÃŠÊ²Ë¤GÄ®Å”Å©ÆœËjÅ©ÆœÅ”Å©ÆœË¤\\x89Å—ÄµÄ­Å”ÆœÄ‘Ä®ÄˆË¤Ê±GjÊ²Figure 1: Schematic illustrating various approaches to problem solving with LLMs. Each rectangle\\nbox represents a thought , which is a coherent language sequence that serves as an intermediate\\nstep toward problem solving. See concrete examples of how thoughts are generated, evaluated, and\\nsearched in Figures 2,4,6.\\nchoices instead of just picking one, and (2) evaluates its current status and actively looks ahead or\\nbacktracks to make more global decisions.\\nTo design such a planning process, we return to the origins of artiï¬cial intelligence (and cognitive\\nscience), drawing inspiration from the planning processes explored by Newell, Shaw, and Simon\\nstarting in the 1950s [ 18,19]. Newell and colleagues characterized problem solving [ 18] as search\\nthrough a combinatorial problem space, represented as a tree. We thus propose the Tree of Thoughts\\n(ToT) framework for general problem solving with language models. As Figure 1 illustrates, while\\nexisting methods (detailed below) sample continuous language sequences for problem solving, ToT\\nactively maintains a tree of thoughts, where each thought is a coherent language sequence that serves\\nas an intermediate step toward problem solving (Table 1). Such a high-level semantic unit allows the\\nLM to self-evaluate the progress different intermediate thoughts make towards solving the problem\\nthrough a deliberate reasoning process that is also instantiated in language (Figures 2,4,6). This\\nimplementation of search heuristics via LM self-evaluation and deliberation is novel, as previous\\nsearch heuristics are either programmed or learned. Finally, we combine this language-based\\ncapability to generate and evaluate diverse thoughts with search algorithms, such as breadth-ï¬rst\\nsearch (BFS) or depth-ï¬rst search (DFS), which allow systematic exploration of the tree of thoughts\\nwith lookahead and backtracking.',\n",
              "  'question': 'In the context of the Tree of Thoughts (ToT) framework for general problem solving with language models, what is the purpose of maintaining a tree of thoughts?\\n\\n',\n",
              "  'answer': 'The purpose of maintaining a tree of thoughts in the ToT framework is to allow each thought to serve as an intermediate step toward problem solving. This high-level semantic unit allows the language model to self-evaluate the progress different intermediate thoughts make towards solving the problem through a deliberate reasoning process that is also instantiated in language. This implementation of search heuristics via LM self-evaluation and deliberation is novel, as previous search heuristics are either programmed or learned. The tree of thoughts also allows the LM to explore the problem space systematically using search algorithms such as breadth-first search (BFS) or depth-first search (DFS), which allow lookahead and backtracking.',\n",
              "  'source_doc': 'Tree of Thoughts_  Deliberate Problem Solving with Large Language Models.pdf'},\n",
              " {'context': 'becomes â€˜Bâ€™, what was â€˜Bâ€™ becomes â€˜Câ€™ etc.3We then sum the 4 probabilities to obtain the ï¬nal prediction,\\nwhich reduces spurious bias towards one of the answer letters (further details in appendix A.1). The results\\nare shown in Table 6. We ï¬nd that in zero-shot and 5-shot settings, de-biasing is very eï¬€ective, improving\\nresults by 10.3 and 4.5 points respectively. When more training data is available, the need for de-biasing\\ndecreases, leading to only 0.2 point improvement in the multi-task and full data settings.\\nComparison to published works Next, we compare our Atlas-11B results with de-biasing to recently\\nreported results with state-of-the-art large language models such as GPT-3 or Chinchilla, which required\\nsigniï¬cantly more amount of computation to train. We report results in Table 7. We ï¬nd that Atlasis able\\nto perform signiï¬cantly better than random in zero-shot, and in conjunction with de-biased inference, achieves\\nzero-shot scores that exceed 5-shot results reported with GPT3 in the literature (47.1% vs 43.9%) (Hendrycks\\net al., 2021). For the 5-shot setting, Atlasoutperforms GPT-3 by 4%, while using 15 Ã—less parameters, and\\n10Ã—less pre-training compute.4When multitask-training on the combined 5-shot data, Atlasimproves to\\n56.6% close to the 5-shot performance of Gopher (60.0%). Finally, on the full data setting, where we train on\\nauxiliary data recommended by the MMLU authors, Atlasreaches an overall accuracy of 65.6%, close to\\nthe state-of-the-art. Interestingly, in this setup, Atlassigniï¬cantly outperforms GPT-3, while on the 5-shot\\nsetting, their performance is similar.\\n4.5.2 Open-domain Question Answering Results\\nNext we evaluate Atlason two open-domain question answering benchmarks: NaturalQuestions and\\nTriviaQA. We compare to prior work, both in a few-shot setting using 64 examples, and using the full training\\nset, and report results in Table 8. On these benchmarks, which require high-degree of memorisation, we',\n",
              "  'question': 'In the context provided, what is the performance improvement achieved by de-biasing in the multi-task and full data settings?\\n\\n',\n",
              "  'answer': 'In the multi-task and full data settings, de-biasing leads to only a 0.2 point improvement in overall accuracy.',\n",
              "  'source_doc': 'Atlas_  Few-shot Learning with Retrieval Augmented Language Models.pdf'},\n",
              " {'context': 'tasks. To give a few examples: (1) when given an instruction with a false premise, the model\\nsometimes incorrectly assumes the premise is true, (2) the model can overly hedge; when given a\\nsimple question, it can sometimes say that there is no one answer to the question and give multiple\\npossible answers, even when there is one fairly clear answer from the context, and (3) the modelâ€™s\\nperformance degrades when instructions contain multiple explicit constraints (e.g. â€œlist 10 movies\\nmade in the 1930â€™s set in Franceâ€) or when constraints can be challenging for language models (e.g.\\nwriting a summary in a speciï¬ed number of sentences).\\n9We generally instruct our labelers to skip evaluations where they are missing the required expertise, though\\nsometimes labelers use a translation service to evaluate simple instructions in languages that they do not speak.\\n16We show some examples of these behaviors in Figure 9. We suspect that behavior (2) emerges partly\\nbecause we instruct labelers to reward epistemic humility; thus, they may tend to reward outputs that\\nhedge, and this gets picked up by our reward model. We suspect that behavior (1) occurs because there\\nare few prompts in the training set that assume false premises, and our models donâ€™t generalize well\\nto these examples. We believe both these behaviors could be dramatically reduced with adversarial\\ndata collection (Dinan et al., 2019b).\\n5 Discussion\\n5.1 Implications for alignment research\\nThis research is part of our broader research program to align AI systems with human intentions (Chris-\\ntiano et al., 2017; Ziegler et al., 2019; Stiennon et al., 2020). Even though this work focuses on\\nour current language model systems, we seek general and scalable methods that work for future AI\\nsystems (Leike et al., 2018). The systems we work with here are still fairly limited, but they are\\namong the largest language models today and we apply them on a wide range of language tasks,',\n",
              "  'question': 'In what ways do language models exhibit limitations and biases when performing language tasks?\\n\\n',\n",
              "  'answer': \"Language models can exhibit limitations and biases in several ways, such as incorrectly assuming false premises, overly hedging in simple questions, and degrading performance when instructions contain multiple explicit constraints or challenging language. These behaviors can occur due to factors like the lack of prompts in the training set, limited expertise of labelers, and the reward model's tendency to favor epistemic humility. These limitations and biases can have implications for alignment research, as they may affect the ability of AI systems to align with human intentions.\",\n",
              "  'source_doc': 'Training language models to follow instructions with human feedback.pdf'},\n",
              " {'context': 'Published in Transactions on Machine Learning Research (10/2023)\\nProgram of Thoughts Prompting: Disentangling Computa-\\ntion from Reasoning for Numerical Reasoning Tasks\\nÂ§,Â¶Wenhu Chenâˆ—,Â§Xueguang Maâˆ—,â€ Xinyi Wang,â—¦William W. Cohen\\nÂ§University of Waterloo\\nÂ¶Vector Institute, Toronto\\nâ€ University of California, Santa Barabra\\nâ—¦Google Research\\n{wenhuchen,x93ma}@uwaterloo.ca, xinyi_wang@ucsb.edu, wcohen@google.com\\nReviewed on OpenReview: https://openreview.net/forum?id=YfZ4ZPt8zd\\nAbstract\\nRecently, therehasbeensignificantprogressinteachinglanguagemodelstoperformstep-by-\\nstep reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting\\n(CoT) is the state-of-art method for many of these tasks. CoT uses language models to\\nproduce text describing reasoning, and computation, and finally the answer to a question.\\nHere we propose â€˜Program of Thoughtsâ€™ (PoT), which uses language models (mainly Codex)\\nto generate text and programming language statements, and finally an answer. In PoT,\\nthe computation can be delegated to a program interpreter, which is used to execute the\\ngenerated program, thus decoupling complex computation from reasoning and language\\nunderstanding. We evaluate PoT on five math word problem datasets and three financial-\\nQA datasets in both few-shot and zero-shot settings. We find that PoT has an average\\nperformance gain over CoT of around 12% across all datasets. By combining PoT with\\nself-consistency decoding, we can achieve extremely strong performance on all the math\\ndatasets and financial datasets. All of our data and code will be released.\\n1 Introduction\\nNumerical reasoning is a long-standing task in artificial intelligence. A surge of datasets has been proposed\\nrecently to benchmark deep-learning modelsâ€™ capabilities to perform numerical/arithmetic reasoning. Some\\nwidely used benchmarks are based on Math word problems (MWP) (Cobbe et al., 2021; Patel et al., 2021;',\n",
              "  'question': 'In this paper, what is the difference between Chain-of-thoughts prompting (CoT) and Program of Thoughts (PoT) for teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks?\\n\\n',\n",
              "  'answer': 'Chain-of-thoughts prompting (CoT) uses language models to produce text describing reasoning, computation, and finally the answer to a question, while Program of Thoughts (PoT) uses language models to generate text and programming language statements, and finally an answer. In PoT, the computation can be delegated to a program interpreter, which is used to execute the generated program, thus decoupling complex computation from reasoning and language understanding. The paper finds that PoT has an average performance gain over CoT of around 12% across all datasets.',\n",
              "  'source_doc': 'Program of Thoughts Prompting_  Disentangling Computation from Reasoning for Numerical Reasoning Tasks.pdf'},\n",
              " {'context': 'awesome) spotted this at the Plaza Ice Cream Parlor in the Magic Kingdom this week!But\\nitâ€™s OK. Itâ€™s brand new | not even on the menu yet | so we didnâ€™t miss out on too much\\nof the Donut Sundae lifespan. And weâ€™re hoping that lifespan is a nice, long one! The\\nMain Street Plaza Ice Cream Parlor can be found at the intersection of Main Street USA\\nand Tomorrowland, just before you get to Cinderella Castle. And the sundae joins a few\\nother must-have treats on the Ice Cream Parlorâ€™s menu, including the house-made ice cream\\nsandwich (preferably ordered with a drizzled sauce!), the \"kidsâ€™ cone\" (itâ€™s totally OK to\\norder this as a grown\" adult adult! But NOT on a Snickers bar, too!), the kikos and curries (unless you use\\nall the glaze from other frozen items in the store), and absolutely everything | as well\\nas the desserts and ice cream itself. Guests can share their undie sundae or a gummy\\nmummy figurine with the kiosk. Happy Butterfingers, and thanks for checking out the best\\nall summer online.<|endoftext|>\\n<BOS>A notorious protester convicted of wilfully promoting hatred against Muslims and\\ncriminally harassing a Muslim man and his family was sentenced Tuesday to nine months\\nin jail. Eric Brazau handed out a flyer that \"vilified Muslims and disparages their\\nreligion,\" Ontario court Judge S. Ford Clements said in February, when he found Brazau\\nguilty. Eric Brazau was convicted of willful promotion of hatred against Muslims and\\ncriminally harassing a Muslim family. ( CARLOS OSORIO / TORONTO STAR FILE PHOTO ) The\\ncase was far from being on the borderline between \"rough and tumble debate\" and hate\\nspeech, as Brazau had argued, Clements said in a College Park courtroom. Brazau handed\\nout the flyer, which contained many offensive references to Islam and Muslims, in August\\nand September 2012. While distributing it, Brazau sometimes yelled obscenities about\\nIslam \"in a tone of voice that suggested he was very angry and had little interest in',\n",
              "  'question': 'What is the context of the provided text?\\n',\n",
              "  'answer': 'The provided text is discussing a new Donut Sundae at the Main Street Plaza Ice Cream Parlor in the Magic Kingdom, as well as other popular treats on their menu.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'All previous evaluations use metrics about the general quality of the answer. In Fig. 6, we show\\nmore detailed metrics from LLMZoo to reveal in which aspects SoT can improve or hurt the answer\\nquality. On average, we can see that SoT improves the diversity and relevance while hurting the\\nimmersion and coherence.\\n0% 20% 40% 60% 80% 100%IntegrityCoherenceImmersionRelevanceDiversity\\n23.2%29.8%40.5%61.4%\\n99.9%34.6%30.6%23.7%11.3%\\n0.1%42.1%39.6%35.8%27.3%Win Tie Lose\\nFigure 6: Win/tie/lose rates of SoT v.s. normal generations using metrics from LLMZoo. SoT\\nperforms well on diversity and relevance, and relatively worse on coherence and immersion.\\nThrough answer investigation (App. I.1.3), we summarize the key takeaways as follows. The skele-\\nton stage of SoT explicitly require LLMs to discuss the answers from multiple aspects without filler\\nwords. This improves the diversity and relevance of the answers. As for coherence and immersion,\\nSoT is not worse than the normal generation around 60% of the time. One future direction is to\\nimprove the SoT prompts or pipeline so that the answers can be better in more metrics.\\n4 S OTWITH ROUTER (SOT-R): A DAPATIVELY TRIGGERING SOT\\nIn Â§ 3, we see that SoT provides considerable speed-ups while maintaining (or even improving)\\nanswer quality for many question types. However, the biggest limitation is that SoT is not suitable\\nfor questions that require step-by-step reasoning (Â§ 3.2.3). Towards pushing the practical adoption\\nof SoT, we explore the possibility of adaptively triggering SoT only when it is suitable. To achieve\\nthat, we propose a router module that decides if SoT should be applied for the user request, and\\nthen call either SoT or normal decoding accordingly. This paradigm aligns with the recent trends\\nof composing multiple models to solve complicated tasks (Chase, 2022; Shen et al., 2023). To\\nimplement the router, we explore two options: LLM prompting as the router (no model training is',\n",
              "  'question': \"What are the key takeaways from the analysis of SoT's performance using metrics from LLMZoo, and how can the SoT prompts or pipeline be improved to better perform in more metrics?\\n\\n\",\n",
              "  'answer': \"The key takeaways from the analysis of SoT's performance using metrics from LLMZoo are that SoT improves the diversity and relevance of the answers while hurting the immersion and coherence. To improve SoT's performance in more metrics, one future direction is to improve the SoT prompts or pipeline.\",\n",
              "  'source_doc': 'Skeleton-of-Thought_ Large Language Models Can Do Parallel Decoding.pdf'},\n",
              " {'context': '4.1 Relative Efficiency of Generating During Training\\nOur loss as presented requires sampling from the model during training. Sampling is generally much\\nslower than computing the logits. Even when using caching to reduce unnecessary recomputations,\\nthe time taken to sample from the model will scale with the expected length of the sequence. However,\\nthe sequences do not need to be exactly sampled from the current policy. Since any policy can be used,\\nwe can keep sequences generated from the policy at previous training steps, stored in a replay buffer\\n[22]. We give an empirical analysis of the overhead when using SequenceMatch in the appendix.\\n4.2 Efficient Implementation of Backspace with Editing Actions\\nEditing actions which can delete previous parts of the input are challenging to implement while\\nretaining the fast training of transformer-based autoregressive models. For instance, the sequence of\\nactions [a; b; <backspace>] cannot be fed directly into a policy network pÎ¸(a|s), since it contains\\n7actions, not states. The sequence [a; b; <backspace>] is not a valid state: the corresponding\\nstate is [<begin-sentence> a] .\\nIn order to convert this into a form where we can compute the relevant logits using masked attention,\\nwe must pre-process the sequence of actions into corresponding inputs, labels, masks and position\\nIDs using algorithm A in the appendix. The preprocessing is illustrated in figure 2. On the other\\nhand, generation with <backspace> actions is straightforward: we already keep previous key-value\\ncached values for generation with transformers. When <backspace> is sampled, we simply roll\\nback the state of the key-value cache and position id with negligible overhead.\\n4.3 Augmenting Expert Sequences with Backspace\\nTo provide the policy with examples of how the <backspace> token should be used, we augment the\\ndata sequences as follows: with (small) probability Î·, we replace a sequence . . . , x iâˆ’1, xi, xi+1, . . .\\nwithxiâˆ’1, xi, xâ€²',\n",
              "  'question': 'In Section 4.2 of the paper, what is the challenge in implementing editing actions in transformer-based autoregressive models?\\n\\n',\n",
              "  'answer': 'The challenge in implementing editing actions in transformer-based autoregressive models is that they cannot be fed directly into a policy network due to the presence of non-state actions such as backspace. The sequence of actions [a; b; <backspace>] cannot be processed by the policy network, and the corresponding state is not valid. To overcome this challenge, an algorithm is needed to pre-process the sequence of actions into corresponding inputs, labels, masks, and position IDs. This preprocessing is illustrated in figure 2 and involves a pre-processing step called algorithm A in the appendix.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': '2bootstrap = tp.compile(program, trainset=trainset, valset=devset)\\nThis will generate demonstration chains for examples in the training set and optimize the selection\\nof demonstrations (from this set) to self-improve the programâ€™s modules. As the name indicates, this\\nis done with random search, treating the selection of demonstrations as a parameter to optimize.\\nNext, if desired, this bootstrapping process can be nested in DSPy. In particular, we can use the\\noptimized bootstrap program itself to further bootstrap another program. This is relevant, for\\nexample, whenever the original zero-shot program performs relatively poorly.\\n1bootstrap2 = tp.compile(program, teacher=bootstrap, trainset=trainset, valset=devset)\\nAnd lastly, we consider ensembling these bootstraps:\\n1# A program that ensembles the top-7 candidate programs from a bootstrapping compiler run\\n(in particular â€˜bootstrapâ€˜ or, when applicable, â€˜bootstrap2â€˜) with majority voting.\\n2ensemble = Ensemble(reduce_fn=dspy.majority).compile(bootstrap.programs[:7])\\nGSM8K includes human reasoning chains. Above, trainset does not include these reasoning\\nchains. We also evaluate with trainset humanCoT, which extends the examples in trainset with\\nthe human reasoning string. These two datasets can be used interchangeably as the value for the\\ntrainset parameter above. We note here that compiling generally runs on the order of minutes\\n(or tens of minutes) as even the more expensive settings only require running the program a few\\nthousand times (e.g., 10â€“20 trials over 150â€“300 validation examples) and they can occur in parallel.\\nResults Our results are summarized in Table 1, which includes dev results as well as our evaluation\\nof promising representatives of each approach on the test set. First, the vanilla program results\\nshow that GPT-3.5 andllama2-13b-chat struggle with math word problems when they have to\\npredict the answers directly, that is, without using a reasoning chain first. This is most pronounced',\n",
              "  'question': 'Given that the context describes a bootstrapping process for improving a program\\'s modules by randomly selecting demonstrations, what is the purpose of using the \"optimized bootstrap program\" to further bootstrap another program?\\n\\n',\n",
              "  'answer': 'The purpose of using the \"optimized bootstrap program\" to further bootstrap another program is to improve the performance of the original zero-shot program. This is relevant whenever the original program performs relatively poorly.',\n",
              "  'source_doc': 'DSPy_  Compiling Declarative Language Model Calls into Self-Improving Pipelines.pdf'},\n",
              " {'context': 'first place votes by CoD step across annotatorsâ€“as\\nwell as aggregated across annotators. First, we report\\na low Fleissâ€™ kappa (Fleiss, 1971) of 0.112, which\\npoints to the subtle differences between summaries\\nand the subjective nature of the task. Recent work hasCoD Step Entity Density Informative Quality Coherence Attributable Overall GPT-4 Eval Average\\n1 0.089 4.34 4.75 4.96 4.96 4.41 4.69\\n2 0.129 4.62 4.79 4.92 5.00 4.58 4.78\\n3 0.148 4.67 4.76 4.84 5.00 4.57 4.77\\n4 0.158 4.74 4.69 4.75 5.00 4.61 4.76\\n5 0.167 4.73 4.65 4.61 4.97 4.58 4.71\\nTable 3: GPT-4 Likert-scale (1-5) assessments of Chain of Density ( CoD) Summaries by step.\\nFigure 4: An example of a human-preferred densification step (left) and one which is not preferred. For the left, the\\nbottom summary is preferred because the addition of â€œLiverpoolâ€ and the goal-scorers is relevant. The second summary\\nmakes room with sensible compressions, such as synthesizing â€œa potential route back into the gameâ€ into â€œa comebackâ€.\\nFor the right, the addition of more details on â€œTVMondeâ€ does not make up for the presence of an awkward fusion of\\nentities (â€œcyberattackâ€, and â€œYves Bigotâ€), which was a direct result of having to tighten the previous summary.\\nsimilarly noted low instance-level agreement when\\njudging GPT-based summaries (Goyal et al., 2022).\\nYet, at the system level, some trends start to\\nemerge. For 3 of the 4 annotators, CoD step 1\\nreceived the largest share of first-place votes across\\nthe 100 examples (28, 43, and 31.4%, respectively).\\nYet, in aggregate, 61% of first placed summaries\\n(23.0+22 .5+15 .5) involved â‰¥3densification steps.\\nThe median preferred CoD step is in the middle ( 3),\\nand the expected step is 3.06.\\nBased on the average density of Step 3 summaries,\\nwe can roughly infer a preferred entity density of\\nâˆ¼0.15across the CoD candidates. From Table 1,\\nwe can see that this density aligns with human-written\\nsummaries ( 0.151), yet is noticeable higher than sum-',\n",
              "  'question': 'Given the results of the GPT-4 evaluation of Chain of Density (CoD) summaries, what is the preferred entity density across the CoD candidates?\\n\\n',\n",
              "  'answer': 'Based on the average density of Step 3 summaries, we can roughly infer a preferred entity density of âˆ¼0.15 across the CoD candidates.',\n",
              "  'source_doc': 'From Sparse to Dense_  GPT-4 Summarization with Chain of Density Prompting.pdf'},\n",
              " {'context': 'arXiv preprint arXiv:1604.06174 , 2016.\\n[10] W.-L. Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y . Zhuang, J. E.\\nGonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%*\\nchatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/ .\\n[11] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement\\nlearning from human preferences. Advances in neural information processing systems , 30,\\n2017.\\n[12] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, E. Li, X. Wang, M. De-\\nhghani, S. Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint\\narXiv:2210.11416 , 2022.\\n[13] T. Dettmers and L. Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv\\npreprint arXiv:2212.09720 , 2022.\\n[14] T. Dettmers, M. Lewis, Y . Belkada, and L. Zettlemoyer. LLM.int8(): 8-bit matrix multiplication\\nfor transformers at scale. Advances in Neural Information Processing Systems 35: Annual\\nConference on Neural Information Processing Systems 2022, NeurIPS 2022 , 2022.\\n[15] T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer. 8-bit optimizers via block-wise\\nquantization. 9th International Conference on Learning Representations, ICLR , 2022.\\n[16] A. E. Elo. The proposed uscf rating system. its development, theory, and applications. Chess\\nLife, 22(8):242â€“247, 1967.\\n[17] A. E. Elo. The rating of chessplayers, past and present . Arco Pub., 1978.\\n17[18] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization\\nfor generative pre-trained transformers. arXiv preprint arXiv:2210.17323 , 2022.\\n[19] J. Fu, S.-K. Ng, Z. Jiang, and P. Liu. Gptscore: Evaluate as you desire. arXiv preprint\\narXiv:2302.04166 , 2023.\\n[20] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, and D. Song. Koala: A\\ndialogue model for academic research. Blog post, April 2023. URL https://bair.berkeley.',\n",
              "  'question': 'How has the field of deep learning evolved since the introduction of Vicuna, an open-source chatbot that impresses GPT-4 with 90% chatGPT quality?\\n\\n',\n",
              "  'answer': 'Since the introduction of Vicuna, the field of deep learning has continued to evolve with various advancements in different areas. For example, there have been improvements in natural language processing, such as the development of LLM.int8() for 8-bit matrix multiplication, which can significantly reduce the memory requirements for transformers at scale. Additionally, there have been advancements in reinforcement learning, such as the introduction of Deep Reinforcement Learning from Human Preferences, which aims to learn optimal policies for complex decision-making tasks. Furthermore, there have been advancements in post-training quantization, such as Gptq, which can improve the accuracy of generative pre-trained transformers. Overall, these advancements demonstrate the ongoing innovation and progress in the field of deep learning.',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': 'elements from PyTorch (Paszke et al., 2019).\\nIn-context learning (McCann et al. 2018; Radford et al. 2018; Brown et al. 2020) is a key mechanism\\nfor foundation model programming. A growing body of work has revealed that, especially with\\ninstruction tuning (Ouyang et al., 2022), we can elicit sophisticated behavior via prompting (Wei\\net al., 2022; Wang et al., 2022b; Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Madaan et al.,\\n2023). Similarly, forms of weak supervision that would normally require task-specific (Khattab\\net al., 2021a;b) or hand-built (Ratner et al., 2016; Hancock et al., 2018) heuristics are now done by\\nLMs (Wang et al., 2022b; Zelikman et al., 2022; Zhang et al., 2022; Shao et al., 2023).\\nIn-context learning methods now routinely invoke tools, leading to LM pipelines that use retrieval\\nmodels (Chen et al., 2017; Lewis et al., 2020; Guu et al., 2020; Lazaridou et al., 2022; Izacard et al.,\\n2022), multimodal foundation models, and more traditional tools like APIs (Nakano et al., 2021)\\nand calculators. A number of toolkits have been developed to facilitate this, including LangChain\\n(Chase, 2022), Semantic Kernel (Microsoft, 2023), LlamaIndex (Liu, 2022), and many other re-\\ntrieval and agent libraries. These toolkits provide pre-packaged chains and agents that connect\\nLMs with numerous accessible tools. However, they suffer from the pervasive prompt engineering\\nchallenges we address in DSPy: they express task-specific behavior through hand-written prompt\\ntemplates (for detailed discussion, see Appendix B).\\nResearchers are starting to apply discrete optimization and RL to find effective prompts, generally\\nfor a single logical LM call (Guo et al., 2023; Pryzant et al., 2023; Huang et al., 2022; Yang et al.,\\n2023). DSPy seeks to generalize this space: it offers a rich framework for optimizing arbitrary\\npipelines from high-level declarative signatures , by bootstrapping high-quality multi-stage demon-',\n",
              "  'question': 'Can a foundation model be programmed using in-context learning and prompt engineering challenges?\\n\\n',\n",
              "  'answer': 'Yes, a foundation model can be programmed using in-context learning and prompt engineering challenges. However, the prompt engineering challenges can be pervasive and require the development of toolkits to facilitate the process. These toolkits can provide pre-packaged chains and agents that connect LMs with numerous accessible tools, but they express task-specific behavior through hand-written prompt templates. Researchers are starting to apply discrete optimization and RL to find effective prompts, generally for a single logical LM call. DSPy seeks to generalize this space by offering a rich framework for optimizing arbitrary pipelines from high-level declarative signatures, by bootstrapping high-quality multi-stage demonetrization.',\n",
              "  'source_doc': 'DSPy_  Compiling Declarative Language Model Calls into Self-Improving Pipelines.pdf'},\n",
              " {'context': 'for alignment research (5.1), what we are aligning to (5.2), limitations (5.3), open questions (5.4),\\nand broader impacts of this work (5.5).\\n2 Related work\\nResearch on alignment and learning from human feedback. We build on previous techniques\\nto align models with human intentions, particularly reinforcement learning from human feed-\\nback (RLHF). Originally developed for training simple robots in simulated environments and Atari\\ngames (Christiano et al., 2017; Ibarz et al., 2018), it has recently been applied to ï¬ne-tuning language\\nmodels to summarize text (Ziegler et al., 2019; Stiennon et al., 2020; BÃ¶hm et al., 2019; Wu et al.,\\n2021). This work is in turn inï¬‚uenced by similar work using human feedback as a reward in domains\\nsuch as dialogue (Jaques et al., 2019; Yi et al., 2019; Hancock et al., 2019), translation (Kreutzer et al.,\\n2018; Bahdanau et al., 2016), semantic parsing (Lawrence and Riezler, 2018), story generation (Zhou\\nand Xu, 2020), review generation (Cho et al., 2018), and evidence extraction (Perez et al., 2019).\\nMadaan et al. (2022) use written human feedback to augment prompts and improve the performance\\nof GPT-3. There has also been work on aligning agents in text-based environments using RL with\\n4a normative prior (Nahian et al., 2021). Our work can be seen as a direct application of RLHF to\\naligning language models on a broad distribution of language tasks.\\nThe question of what it means for language models to be aligned has also received attention re-\\ncently (Gabriel, 2020). Kenton et al. (2021) catalog behavioral issues in LMs that result from\\nmisalignment, including producing harmful content and gaming misspeciï¬ed objectives. In concur-\\nrent work, Askell et al. (2021) propose language assistants as a testbed for alignment research, study\\nsome simple baselines, and their scaling properties.\\nTraining language models to follow instructions. Our work is also related to research on cross-',\n",
              "  'question': 'In alignment research, what does it mean for language models to be aligned and what are some behavioral issues that can result from misalignment?\\n\\n',\n",
              "  'answer': 'In alignment research, it means for language models to align with human intentions and produce outputs that are consistent with those intentions. Misalignment can result in harmful content production and gaming misspecified objectives. Some behavioral issues that can result from misalignment include producing harmful content and gaming misspecified objectives.',\n",
              "  'source_doc': 'Training language models to follow instructions with human feedback.pdf'},\n",
              " {'context': 'Easy-to-hard generalization . In addition to compositional generalization, there are many other\\ntasks where the test cases require more reasoning steps to solve than the training examples, for\\nexample, the last-letter-concatenation task where the test lists are longer than the demonstration\\nexamples. Dong et al. (2019) propose Neural Logic Machines (NLMs) for both inductive learning\\nand logic reasoning. NLMs trained on small-scale tasks (such as small size block worlds) can\\nperfectly generalize to large-scale tasks (such as larger size block worlds). Schwarzschild et al.\\n(2021) show that recurrent networks trained to solve simple problems with few recurrent steps (such\\nas small size mazes or chess puzzles) can solve more complex problems (such as larger size mazes\\nor chess puzzles) by performing additional recurrences during inference. In our method, we achieve\\neasy-to-hard generalization by decomposing a complex problem into a series of easier problems.\\nTask decomposition . Perez et al. (2020) decompose a multi-hop question into a number of inde-\\npendent single-hop subquestions, which are answered by an off-the-shelf question answering (QA)\\nmodel. Then those answers are aggregated to form the ï¬nal answer. Both question decomposition\\nand answer aggregation are implemented by trained models. Wang et al. (2022a) conducts multi-hop\\nQA by modeling prompts as continuous virtual tokens and progressively eliciting relevant knowl-\\n8Published as a conference paper at ICLR 2023\\nedge from language models via iterative prompting. Unlike these methods, our approach does not\\ninvolve any training or ï¬netuning. Moreover, the subquestions generated in least-to-most prompting\\nare usually dependent and have to be sequentially solved in a speciï¬c order so that answers to some\\nsubquestions can be used as building blocks to solve other subquestions. Yang et al. (2022) translate\\nnatural language questions to SQL queries by decomposing a question into a sequence of slot-ï¬lling',\n",
              "  'question': 'Given a complex problem that requires multiple reasoning steps to solve, how can we achieve easy-to-hard generalization by decomposing it into a series of easier problems?\\n\\n',\n",
              "  'answer': 'Our method achieves easy-to-hard generalization by decomposing a complex problem into a series of easier problems. We do this by identifying the key subproblems that need to be solved to arrive at the final solution. These subproblems are then solved independently using trained models, and the answers are aggregated to form the final answer. This approach allows us to break down the complex problem into smaller, more manageable parts, making it easier to reason about and generalize to new situations.',\n",
              "  'source_doc': 'Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.pdf'},\n",
              " {'context': 'Toolformer: Language Models Can Teach Themselves to Use Tools\\nTimo Schick Jane Dwivedi-Yu Roberto DessÃ¬yRoberta Raileanu\\nMaria Lomeli Luke Zettlemoyer Nicola Cancedda Thomas Scialom\\nMeta AI ResearchyUniversitat Pompeu Fabra\\nAbstract\\nLanguage models (LMs) exhibit remarkable\\nabilities to solve new tasks from just a few\\nexamples or textual instructions, especially at\\nscale. They also, paradoxically, struggle with\\nbasic functionality, such as arithmetic or fac-\\ntual lookup, where much simpler and smaller\\nmodels excel. In this paper, we show that\\nLMs can teach themselves to use external tools\\nvia simple APIs and achieve the best of both\\nworlds. We introduce Toolformer , a model\\ntrained to decide which APIs to call, when to\\ncall them, what arguments to pass, and how to\\nbest incorporate the results into future token\\nprediction. This is done in a self-supervised\\nway, requiring nothing more than a handful of\\ndemonstrations for each API. We incorporate\\na range of tools, including a calculator, a Q&A\\nsystem, a search engine, a translation system,\\nand a calendar. Toolformer achieves substan-\\ntially improved zero-shot performance across\\na variety of downstream tasks, often competi-\\ntive with much larger models, without sacriï¬c-\\ning its core language modeling abilities.\\n1 Introduction\\nLarge language models achieve impressive zero-\\nand few-shot results on a variety of natural lan-\\nguage processing tasks (Brown et al., 2020; Chowd-\\nhery et al., 2022, i.a.) and show several emergent\\ncapabilities (Wei et al., 2022). However, all of\\nthese models have several inherent limitations that\\ncan at best be partially addressed by further scal-\\ning. These limitations include an inability to access\\nup-to-date information on recent events (Komeili\\net al., 2022) and the related tendency to hallucinate\\nfacts (Maynez et al., 2020; Ji et al., 2022), difï¬cul-\\nties in understanding low-resource languages (Lin\\net al., 2021), a lack of mathematical skills to per-',\n",
              "  'question': 'Given that language models exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, and struggle with basic functionality such as arithmetic or factual lookup, how can they be taught to use external tools via simple APIs to achieve the best of both worlds?\\n\\n',\n",
              "  'answer': 'Language models can be taught to use external tools via simple APIs by introducing a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring only a handful of demonstrations for each API. The range of tools that can be incorporated includes a calculator, a Q&A system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.',\n",
              "  'source_doc': 'Toolformer_ Language Models Can Teach Themselves to Use Tools.pdf'},\n",
              " {'context': 'our results show that RLHF is very effective at making language models more helpful to\\nusers, more so than a 100x model size increase. This suggests that right now increasing\\ninvestments in alignment of existing language models is more cost-effective than training\\nlarger modelsâ€”at least for our customersâ€™ natural language task distribution.\\n2.Weâ€™ve seen some evidence that InstructGPT generalizes â€˜following instructionsâ€™ to\\nsettings that we donâ€™t supervise it in, for example on non-English language tasks and\\ncode-related tasks. This is an important property because itâ€™s prohibitively expensive to have\\nhumans supervise models on every task they perform. More research is needed to study how\\nwell this generalization scales with increased capabilities; see Christiano et al. (2021) for\\nrecent research in this direction.\\n3.We were able to mitigate most of the performance degradations introduced by our\\nï¬ne-tuning. If this was not the case, these performance degradations would constitute\\nan alignment taxâ€”an additional cost for aligning the model. Any technique with a high\\ntax might not see adoption. To avoid incentives for future highly capable AI systems to\\nremain unaligned with human intent, there is a need for alignment techniques that have low\\nalignment tax. To this end, our results are good news for RLHF as a low-tax alignment\\ntechnique.\\n4.Weâ€™ve validated alignment techniques from research in the real world. Alignment\\nresearch has historically been rather abstract, focusing on either theoretical results (Soares\\net al., 2015), small synthetic domains (Christiano et al., 2018; Leike et al., 2017), or training\\nML models on public NLP datasets (Ziegler et al., 2019; Stiennon et al., 2020). Our work\\nprovides grounding for alignment research in AI systems that are being used in production in\\n17the real world with customers.10This enables an important feedback loop on the techniquesâ€™\\neffectiveness and limitations.\\n5.2 Who are we aligning to?',\n",
              "  'question': 'Given your results, what are the implications for the alignment of existing language models and the development of low-tax alignment techniques?\\n\\n',\n",
              "  'answer': \"Based on your results, it appears that right now increasing investments in alignment of existing language models is more cost-effective than training larger models, at least for your customers' natural language task distribution. Additionally, your findings suggest that RLHF is a low-tax alignment technique that can mitigate most of the performance degradations introduced by fine-tuning. This is important because alignment research has historically been abstract and focused on theoretical results, small synthetic domains, or training ML models on public NLP datasets. However, your work provides grounding for alignment research in AI systems that are being used in production in the real world with customers, enabling an important feedback loop on the techniques' effectiveness and limitations.\",\n",
              "  'source_doc': 'Training language models to follow instructions with human feedback.pdf'},\n",
              " {'context': 'the same temperature. We do not have a strong hypothesis\\nfor which direction should yield higher pass rates. While\\ngenerating docstrings may be more forgiving because natu-\\nral language syntax is less strict than code syntax, docstrings\\nin our dataset may be lower quality because developers tend\\nto devote less time to writing docstrings. Indeed, our model\\nproduces docstrings like â€œI just found this function onlineâ€\\nand â€œThis test is not correctly written and itâ€™s not my solu-\\ntion.â€\\nFinally, with a docstring model, we have yet another way\\nto choose a single sample from a set of ksamples. In-\\nstead of picking the sample with the best mean log proba-\\nbility as investigated in the previous two sections, we can\\nchoose the sample that maximizes the back-translation ob-Evaluating Large Language Models Trained on Code\\nTable 3. Pass rates for our docstring generating model Codex-D,\\nwhich is evaluated by hand-grading 10 samples per task due to the\\nlack of a ground-truth automatic evaluation. We ï¬nd similar but\\nlower pass-rates compared to Codex-S.\\nMODEL PASS @1 PASS @10\\nCODEX -S-12B 32.2% 59.5%\\nCODEX -D-12B 20.3% 46.5%\\njectiveP(ground truth docstring jgenerated sample )where\\nPis evaluated using Codex-D. Unfortunately, in Figure 7,\\nwe show that ranking samples via back-translation under-\\nperforms mean log-probability ranking, though it outper-\\nforms random ranking. This heuristic also appears to overï¬t\\nquickly.\\n6. Limitations\\nWhile Codex is able to sample correct solutions for the\\nmajority of HumanEval problems, we ï¬nd that it has a\\nnumber of limitations.\\nFirst, Codex is not sample efï¬cient to train. Our training\\ndataset comprises a signiï¬cant fraction of publicly available\\nPython code on GitHub, totaling hundreds of millions of\\nlines of code. Even seasoned developers do not encounter\\nanywhere near this amount of code over their careers. In-\\ndeed, a strong student who completes an introductory com-\\nputer science course is expected to be able to solve a larger',\n",
              "  'question': 'Can choosing a sample that maximizes the back-translation objective function improve the pass rates of a docstring generating model?\\n\\n',\n",
              "  'answer': 'No, choosing a sample that maximizes the back-translation objective function does not improve the pass rates of a docstring generating model. In fact, this heuristic appears to overfit quickly and underperforms mean log-probability ranking.',\n",
              "  'source_doc': 'Evaluating Large Language Models Trained on Code.pdf'},\n",
              " {'context': '7, 7, 8, 8, 9, 9, 9, 9, 9]\\nReason: The incorrectly sorted list contains two extra 4s and is missing two\\n6s and one 9.\\nOutput: [0, 1, 1, 2, 2, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 8, 8, 9,\\n9, 9, 9, 9, 9]\\n</Examples>\\nInput: {input}\\nIncorrectly Sorted: {incorrectly_sorted}A prompt used by\\n...\\n...This prompt is used by an operation\\nImprove(t), which enhances a given thought t\\nusing information provided in another thought.\\nDepending on how the Improve + Repeat \\noperation is implemented by the user within\\nGoT, it can either generate a number of new \\nthoughts in GRS (the upper graph on the right), \\nsimilar to Generate + Repeat, or may refine \\nthe same thought in GRS (the lower graph on \\nthe right), chaining k=4 refinement iterations together.\\n1 2\\n233Initial/system prompt (optional)\\nHello. I want to sort the following input sequence of numbers: {input}I\\nI\\n44\\nThe input\\nthought t1Figure 3: The system architecture of GoT, and the APIs of respective modules. The user can straightforwardly extend the design\\ntowards new prompting schemes, experiment with novel thought transformations, and plug in different LLMs. The blue part of\\nthe figure contains the architecture overview, the green part lists the API, and the red part contains example prompts together\\nwith a GRS and operations involved.5.1 Sorting\\nDue to space constraints, we detail one use case (sorting).\\nWe focus on its decomposition and Graph of Operations,\\nwhich are central for implementing and executing any work-\\nload within GoT. We consider sorting numbers 0â€“9 with du-\\nplicates. The considered LLMs are unable to sort a sequence\\nof such numbers correctly beyond a certain length consis-\\ntently because duplicate counts do not match.\\nIn GoT, we employ merge-based sorting: First, one de-\\ncomposes the input sequence of numbers into subarrays.\\nThen, one sorts these subarrays individually, and then re-\\nspectively merges them into a final solution. Figure 4 illus-',\n",
              "  'question': 'In what way does the GoT system utilize merge-based sorting to sort a sequence of numbers with duplicates?\\n\\n',\n",
              "  'answer': 'The GoT system utilizes merge-based sorting to sort a sequence of numbers with duplicates by first decomposing the input sequence into subarrays, sorting these subarrays individually, and then merging them into a final solution.',\n",
              "  'source_doc': 'Graph of Thoughts_ Solving Elaborate Problems with Large Language Models.pdf'},\n",
              " {'context': 'into a sorted array of numbers\\nSplitting an unsorted array into\\nsubarrays, for subsequent sortingCombining articles into\\na coherent summary...\\nKeyword\\nsummary 1Keyword\\nsummary 21 4 6 2 4 2 4 9 8 7 5 4\\n1 4 6 2    4 2 4 9    8 7 5 4Article\\n1\\nGenerating summaries from\\nan article, to maximize qualityFigure 2: Examples of aggregation and generation thought\\ntransformations.\\nrays of numbers into a final sorted array. We illustrate exam-\\nples of aggregation and generation in Figure 2.\\nFormally, each such transformation can be modeled as\\nT(G, pÎ¸)where G= (V, E)is the graph reflecting the\\ncurrent state of the reasoning, and pÎ¸is the used LLM. T\\nmodifies Gusually by adding new vertices and their incom-\\ning edges. We have Gâ€²=T(G, pÎ¸) = ( Vâ€², Eâ€²), where\\nVâ€²= (VâˆªV+)\\\\Vâˆ’andEâ€²= (EâˆªE+)\\\\Eâˆ’.V+\\nandE+are new vertices and edges inserted into Gto model\\nthe new thoughts and their dependencies, respectively. To\\nmaximize the expressiveness of GoT â€“ we also enable the\\nuser to explicitly remove thoughts, by specifying the corre-\\nsponding vertices and edges to be removed ( Vâˆ’andEâˆ’, re-\\nspectively). Here, it is the userâ€™s responsibility to ensure that\\nthe sets V+, E+, Vâˆ’,andEâˆ’come with consistent trans-\\nformations (i.e., for example, that the user does not attempt\\nto remove a vertex that does not exist). This enables seam-\\nless incorporation of schemes where, in order to save space\\nwithin the context, one can remove parts of reasoning that\\ndo not promise improvements.The specific form of Tand how it impacts Gdepends on\\na specific transformation. We first detail the primary graph-\\nenabled thought transformations, and then proceed to de-\\nscribe how GoT embraces the transformations from the ear-\\nlier schemes. Unless stated otherwise, Vâˆ’=Eâˆ’=âˆ….\\nAggregation Transformations First, with GoT, one can\\naggregate arbitrary thoughts into new ones, to combine\\nand reinforce the advantages of these thoughts, while elim-\\ninating their disadvantages. In the basic form, in which',\n",
              "  'question': 'Can aggregation transformations in GoT combine arbitrary thoughts into new ones, while eliminating their disadvantages?\\n\\n',\n",
              "  'answer': 'Yes, aggregation transformations in GoT can combine arbitrary thoughts into new ones, while eliminating their disadvantages.',\n",
              "  'source_doc': 'Graph of Thoughts_ Solving Elaborate Problems with Large Language Models.pdf'},\n",
              " {'context': 'improvement. The TeacherLM-7.1B model has achieved a\\nzero-shot score of 52.3 on the MMLU, surpassing the 5-shot\\nperformance of most hundred billion parameter models. Ad-\\nditionally, the zero-shot score of TeacherLM-176B is 59.8,\\ncomparable to the 5-shot score of gopher-280B (Rae et al.,\\n2021). The full results are in Appendix A.2.3.4.2. M ULTI -STAGE TRAINING HELPS A L OT\\nWe show the evaluation results during multi-stage training\\nin Table 1. In contrast to multi-stage training, we also\\nmixed all datasets and trained on them directly. The training\\nhyperparameters and the number of steps are consistent. We\\nfind that directly blending all datasets scores much lower\\nthan multi-stage training. However, only the models using\\nCoT data obtained continuous improvement in the third\\nstage. This phenomenon may indicate that tasks of the\\nMMLU benchmark require higher reasoning ability.\\n3.4.3. A DDSUBJECT TOPROMPT\\nInspired by the fact that the MMLU benchmark is composed\\nof different subject tasks, we added the prompt â€œ The fol-\\nlowing are multiple choice questions (with answers) about\\n{subject name }â€ at the beginning of each text. We labeled\\nthe subject of each sample of TeacherData-2M. This ap-\\nproach increases the score of TeacherLM-7.1B by 2%. We\\nthink such a prompt is more helpful for the model to use the\\ncorrect knowledge to answer the question.\\n4. Teaching Student\\nIn this section, we select TeacherLM-7.1B as the teacher for\\nthe following experiments in order to balance efficiency and\\nperformance. Our experiment contains three dimensions to\\nverify the ability of TeacherLM. The first dimension is the\\nchoice of different training modes, divided into multi-task\\ntraining followed by zero-shot testing and single-task fine-\\ntuning followed by testing in the corresponding task. The\\nsecond dimension is the scaling of the model size. The third\\ndimension is the diversity of the test tasks.\\n4.1. Datasets\\nWe selected P3-Sense-3K for multi-task training. More-',\n",
              "  'question': 'Can TeacherLM-7.1B achieve a higher zero-shot score on the MMLU benchmark by using a prompt that specifies the subject of each sample?\\n\\n',\n",
              "  'answer': 'Yes, according to the given context, adding a prompt that specifies the subject of each sample of TeacherData-2M increased the score of TeacherLM-7.1B by 2%. Therefore, it can be concluded that using such a prompt can help the model to use the correct knowledge to answer the question.',\n",
              "  'source_doc': 'TeacherLM_ Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise.pdf'},\n",
              " {'context': 'approach, called MoCo, considers two networks: one for the keys, parametrized by Î¸k, and one of the query,\\nparametrized by Î¸q. The parameters of the query network are updated with backpropagation and stochastic\\ngradient descent, similarly to when using in-batch negatives, while the parameters of the key network, or\\nMomentum encoder, is updated from the parameters of the query network by using a exponential moving\\naverage:\\nÎ¸kâ†mÎ¸k+ (1âˆ’m)Î¸q, (2)\\nwheremis the momentum parameter that takes its value in [0,1].\\n4 Experiments\\nIn this section, we empirically evaluate our best retriever trained with contrastive learning, called Contriever\\n(contrastive retriever), which uses MoCo with random cropping. We use a contrastive learning procedure that\\ndiï¬€ers from ICT (Lee et al., 2019) mainly in three aspects. First, positive pairs are sampled using random\\ncropping and tokens from each element of the pair are deleted with a probability of 10%. Second we use\\nMoCo where negatives consists of elements from previous batches stored in a queue. This allows to scale to a\\nlarge number of negatives. Third we use data from Wikipedia and CCNet (Wenzek et al., 2020) for training.\\nAblation studies motivating these technical choices are performed in Section 6. More technical details about\\nour model are given in Appendix A.1.\\n4.1 Datasets\\nContriever is trained with contrastive learning on documents sampled from a mix between Wikipedia data\\nand CCNet data (Wenzek et al., 2020), where half the batches are sampled from each source.\\nFirst, we evaluate our model on two question answering datasets: NaturalQuestions (Kwiatkowski et al.,\\n2019) and TriviaQA (Joshi et al., 2017). For both datasets, we use the open domain versions as introduced\\nby Lee et al. (2019), and the English Wikipedia dump from Dec. 20, 2018 as the collection of documents to\\nretrieve from. We report the top-k retrieval accuracy, i.e.the number of questions for which at least one of\\nthe top-k passages contain the answer.',\n",
              "  'question': 'In the context provided, what is the purpose of using a momentum encoder in the contrastive learning procedure for training the retriever?\\n\\n',\n",
              "  'answer': 'The purpose of using a momentum encoder in the contrastive learning procedure for training the retriever is to update the parameters of the key network from the parameters of the query network using a exponential moving average, as described in equation (2). This helps to stabilize the training process and improve the performance of the retriever.',\n",
              "  'source_doc': 'Unsupervised Dense Information Retrieval with Contrastive Learning.pdf'},\n",
              " {'context': 'use word embedding regularization (Liu et al., 2019; Huang et al., 2019), data augmentation (Liu\\net al., 2019; Dinan et al., 2019a; Sheng et al., 2019), null space projection to make the distribution\\nover sensitive tokens more uniform (Liang et al., 2021), different objective functions (Qian et al.,\\n2019), or causal mediation analysis (Vig et al., 2020). There is also work on steering the generation\\nof language models using a second (usually smaller) language model (Dathathri et al., 2019; Krause\\net al., 2020), and variants of this idea have been applied to reducing language model toxicity (Schick\\net al., 2021).\\n5Table 1: Distribution of use\\ncase categories from our API\\nprompt dataset.\\nUse-case (%)\\nGeneration 45.6%\\nOpen QA 12.4%\\nBrainstorming 11.2%\\nChat 8.4%\\nRewrite 6.6%\\nSummarization 4.2%\\nClassiï¬cation 3.5%\\nOther 3.5%\\nClosed QA 2.6%\\nExtract 1.9%Table 2: Illustrative prompts from our API prompt dataset. These\\nare ï¬ctional examples inspired by real usageâ€”see more examples\\nin Appendix A.2.1.\\nUse-case Prompt\\nBrainstorming List ï¬ve ideas for how to regain enthusiasm for my\\ncareer\\nGeneration Write a short story where a bear goes to the beach,\\nmakes friends with a seal, and then returns home.\\nRewrite This is the summary of a Broadway play:\\n\"\"\"\\n{summary}\\n\"\"\"\\nThis is the outline of the commercial for that play:\\n\"\"\"\\n3 Methods and experimental details\\n3.1 High-level methodology\\nOur methodology follows that of Ziegler et al. (2019) and Stiennon et al. (2020), who applied\\nit in the stylistic continuation and summarization domains. We start with a pretrained language\\nmodel (Radford et al., 2019; Brown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al.,\\n2022), a distribution of prompts on which we want our model to produce aligned outputs, and a team\\nof trained human labelers (see Sections 3.4 for details). We then apply the following three steps\\n(Figure 2).\\nStep 1: Collect demonstration data, and train a supervised policy. Our labelers provide demon-',\n",
              "  'question': 'In which context would word embedding regularization (Liu et al., 2019; Huang et al., 2019), data augmentation (Liu et al., 2019; Dinan et al., 2019a; Sheng et al., 2019), null space projection to make the distribution over sensitive tokens more uniform (Liang et al., 2021), different objective functions (Qian et al., 2019), or causal mediation analysis (Vig et al., 2020) be most applicable?\\n\\n',\n",
              "  'answer': 'Word embedding regularization (Liu et al., 2019; Huang et al., 2019), data augmentation (Liu et al., 2019; Dinan et al., 2019a; Sheng et al., 2019), null space projection to make the distribution over sensitive tokens more uniform (Liang et al., 2021), different objective functions (Qian et al., 2019), or causal mediation analysis (Vig et al., 2020) would be most applicable in the context of natural language processing for various use cases such as generation, open QA, brainstorming, chat, rewrite, summarization, classification, and other use cases.',\n",
              "  'source_doc': 'Training language models to follow instructions with human feedback.pdf'},\n",
              " {'context': 'many index compression techniques are available for nearest neighbour search, which can often dramatically\\nreduce memory requirements at the cost of some retrieval accuracy. Following Izacard et al. (2020), we\\nexplore the eï¬€ect of Product Quantization (PQ, JÃ©gou et al., 2011), a popular lossy compression technique\\nonAtlas-3Bâ€™s accuracy for the 64-shot NQ task at diï¬€erent compression levels.\\nThe results are shown in Figure 4. We ï¬nd that substantial compression is possible before the onset of\\nsigniï¬cant performance degradation. Namely, the Wikipedia index can be compressed from 49GB to 4GB\\nwith negligible drop in retrieval precision and exact match. Likewise, the combined index can be compressed\\nfrom 587GB to 50GB without serious degradation, indicating that the combined index could be loaded onto\\na single 80GB GPU.\\n18101102\\nMemory (in GB)0.20.40.60.8NQ Recall@50\\n101102\\nMemory (in GB)10203040NQ Exact Match\\n100101\\nMemory (in GB)0.20.40.60.8NQ Recall@50\\n100101\\nMemory (in GB)10203040NQ Exact Match\\nFigure 4:Index Compression: Atlas-3B 64-shot NQ performance (left column: Retrieval Recall@50, right\\ncolumn: QA Exact Match score), as a function of index size, for diï¬€erent levels of quantisation. The right-most\\npoint in each plot represents the uncompressed index. Top Row: Wikipedia + CC Index. Bottom Row:\\nWikipedia Index.\\n6 Discussion\\nIn this paper, we introduce Atlas, a large retrieval-augmented language model. By jointly pre-training the\\nretriever module and the language model, we show that Atlashas strong few-shot learning capabilities on a\\nwide range of knowledge intensive tasks, including NaturalQuestions, TriviaQA, FEVER, 8 KILT tasks and 57\\nMMLU tasks. For example, Atlas-11B reaches more than 42% accuracy on NaturalQuestions and 84.7% on\\nTriviaQA when training on 64 examples, which is an improvement of almost 3 points compared to PaLM, a\\n540B parameters model, which required 50x more pre-training compute. We also provided detailed ablations',\n",
              "  'question': \"Given Atlas-3B's performance on the 64-shot NQ task at different compression levels, what is the maximum compression ratio that can be achieved without significant performance degradation?\\n\\n\",\n",
              "  'answer': 'We find that substantial compression is possible before the onset of significant performance degradation. The Wikipedia index can be compressed from 49GB to 4GB with negligible drop in retrieval precision and exact match. Likewise, the combined index can be compressed from 587GB to 50GB without serious degradation, indicating that the combined index could be loaded onto a single 80GB GPU.',\n",
              "  'source_doc': 'Atlas_  Few-shot Learning with Retrieval Augmented Language Models.pdf'},\n",
              " {'context': 'truthfulness. Interestingly, the exception is our 1.3B PPO-ptx model, which performs slightly worse\\nthan a GPT-3 model of the same size. When evaluated only on prompts that were not adversarially\\nselected against GPT-3, our PPO models are still signiï¬cantly more truthful and informative than\\nGPT-3 (although the absolute improvement decreases by a couple of percentage points.\\nGPT SFT PPO PPO-ptx0255075PercentageQA prompt\\nGPT SFT PPO PPO-ptx\\nModelInstruction + QA prompt\\nFigure 6: Results on the TruthfulQA dataset. Gray bars indicate ratings of truthfulness; colored bars\\nindicate ratings of truthfulness andinformativeness.\\nFollowing Lin et al. (2021), we also give a helpful â€œInstruction+QAâ€ prompt that instructs the model\\nto respond with â€œI have no commentâ€ when it is not certain of the correct answer. In this case, our\\nPPO models err on the side of being truthful and uninformative rather than conï¬dently saying a\\nfalsehood; the baseline GPT-3 model arenâ€™t as good at this.\\nOur improvements in truthfulness are also evidenced by the fact that our PPO models hallucinate (i.e.\\nfabricate information) less often on closed-domain tasks from our API distribution, which weâ€™ve\\nshown in Figure 4.\\nInstructGPT shows small improvements in toxicity over GPT-3, but not bias. We ï¬rst evaluate\\nour models on the RealToxicityPrompts dataset (Gehman et al., 2020). We do this in two ways: we\\nrun model samples through the Perspective API8to obtain automatic toxicity scores, which is the\\n8www.perspectiveapi.com\\n13None Respectful00.050.100.150.200.25ToxicityHuman eval\\nModel\\nGPT\\nSFT\\nPPO-ptx\\nNone Respectful\\nPromptPerspectiveAPI scoreFigure 7: Comparing human evaluations and automatic evaluations (Perspective API scores) on\\nRealToxicityPrompts. A total of 1,729 prompts were labeled for three different 175B models, both\\nwith and without \"respectful\" instructions. The automatic evaluations shown here are calculated',\n",
              "  'question': 'Given the context, what are the improvements in truthfulness of the PPO models compared to the GPT-3 model?\\n\\n',\n",
              "  'answer': 'The PPO models show significant improvements in truthfulness compared to the GPT-3 model. This is evidenced by their ratings of truthfulness on the TruthfulQA dataset, their lower fabrication of information on closed-domain tasks from the API distribution, and their performance on the RealToxicityPrompts dataset.',\n",
              "  'source_doc': 'Training language models to follow instructions with human feedback.pdf'},\n",
              " {'context': 'tree is dictated by the utilized search algorithm (for example\\nBFS or DFS).\\n3 The GoT Framework\\nWe now detail the GoT framework. We present it in Figure 1,\\nand compare it to other prompting strategies.\\nFormally, GoT can be modeled as a tuple (G,T,E,R),\\nwhere Gis the â€œLLM reasoning processâ€ (i.e., all the LLM\\nthoughts within the context, with their relationships), TareInput\\nOutputInput\\nOutput OutputThoughts:\\nUnscored\\nNegative\\nscore OutputInput\\nOutput[This work]\\nInput\\nPositive\\nscore\\nDependencies\\nbetween thoughts\\nAbandon thought\\nBacktrackBasic Input-\\nOutput (IO)\\nLegendMultiple CoTs (CoT-SC) Chain-of-\\n-Thought\\n(CoT)Tree of Thoughts (ToT) Graph of Thoughts (GoT)\\nKey novelty:\\nIntermediate\\nLLM thoughts\\nwithin a chainBranching out\\nfrom a chain\\nSelecting\\na chain with\\nthe best scoreAbandon a chain\\nKey novelty\\n(beyond CoT):\\nHarnessing multiple\\nindependent chains\\nof thoughtsKey novelty\\n(beyond CoT-SC):\\nGenerating several\\nnew thoughts based\\non a given arbitrary\\nthought, exploring\\nit further, and possibly\\nbacktracking from itKey novelty (beyond ToT):\\nArbitrary graph-based thought\\ntransformations (aggregating \\nthoughts into a new one, \\nlooping over a thought to \\nrefine it)BacktrackingRefining\\nAggregating\\nthoughtsBacktracking\\nfrom a chain\\nIntermediate\\nthoughts are\\nalso scored\\nAggregating\\nchainsInputFigure 1: Comparison of Graph of Thoughts (GoT) to other prompting strategies.\\nthe potential thought transformations, Eis an evaluator func-\\ntion used to obtain scores of thoughts, and Ris a ranking\\nfunction used to select most relevant thoughts.\\n3.1 Reasoning Process\\nWe model the reasoning process as a directed graph G=\\n(V, E);Vis a set of vertices and EâŠ†VÃ—Vis a set of\\nedges. Gis directed and thus the edges are a subset of or-\\ndered vertex pairs EâŠ†VÃ—V. A vertex contains a solution\\nto a problem at-hand (be it an initial, intermediate, or a fi-\\nnal one). The concrete form of such a thought depends on\\na use case; it could be a paragraph (in writing tasks) or a',\n",
              "  'question': 'Given that the GoT framework is modeled as a directed graph G=(V,E), where V is a set of vertices and E is a set of edges, and the edges are ordered vertex pairs EâŠ†VÃ—V, what is the relationship between the vertices and edges in the graph?\\n\\n',\n",
              "  'answer': 'The vertices in the graph represent solutions to a problem at hand, while the edges represent the relationship between these solutions. The edges are ordered vertex pairs, meaning that each edge has a specific order of the two vertices it connects. This allows for the directed nature of the graph, where the edges indicate the direction of the relationship between the vertices.',\n",
              "  'source_doc': 'Graph of Thoughts_ Solving Elaborate Problems with Large Language Models.pdf'},\n",
              " {'context': 'retrievers in an unsupervised way. Paranjape et al. (2021) propose a method to train retrieval-augmented\\ngenerators using a second â€œinformedâ€ retriever with access to the output, which the test-time retriever can be\\ndistilled from, and HofstÃ¤tter et al. (2022) recently proposed a training set ï¬ltering/weighting approach to\\ntrain stronger retrieval-augmented generators. Izacard et al. (2022) explored diï¬€erent contrastive learning\\nmethods to train retrievers, while Ram et al. (2022) used recurring spans within a document to create\\npseudo-positive query-document pairs.\\n7Retrieval-augmented language models. Continuous cache models (Grave et al., 2017b) deï¬nes a\\nprobability distribution over recent tokens, by computing the similarity between previous and current\\nrepresentations of tokens. This distribution is then interpolated with the distribution of the language model,\\nto improve predictions. Later, the amount of tokens used to compute this distribution was extended to a much\\nlarger memory by leveraging approximate nearest neighbors search (Grave et al., 2017a). The related kNN-LM\\nmodel (Khandelwal et al., 2020) replaced LSTMs by transformer networks, and scaled the memory to billions\\nof tokens, leading to strong performance improvements. More recently, RETRO (Borgeaud et al., 2021)\\nextended these by scaling the retrieval memory to trillions of tokens, and changing the model architecture to\\ntake retrieved documents as input.\\nRetrieval-Augmentation with Search Engines. Recently, diï¬€erent works have proposed to train large\\nlanguage models to interact with a search engine, by generating text queries, and using the retrieved documents\\nas additional context (Nakano et al., 2021; Thoppilan et al., 2022; Shuster et al., 2022). In the context\\nof few-shot question answering, Lazaridou et al. (2022) used the question to perform a search query, and\\nretrieved documents are added to the prompt of a large language model performing in-context learning.\\n3.2 Few-shot learning',\n",
              "  'question': 'What is the difference between retrieval-augmented language models and retrieval-augmented generators?\\n\\n',\n",
              "  'answer': 'Retrieval-augmented language models and retrieval-augmented generators are both methods used to improve the performance of language models. However, they differ in their approach to using retrieved documents as additional context. Retrieval-augmented language models, such as continuous cache models and kNN-LM, use retrieved documents to improve predictions by interpolating the probability distribution over recent tokens with the distribution of the language model. On the other hand, retrieval-augmented generators, such as RETRO, use retrieved documents as input to the model architecture. In addition, retrieval-augmented generators often train the model using a second \"informed\" retriever with access to the output, which the test-time retriever can be distilled from.',\n",
              "  'source_doc': 'Atlas_  Few-shot Learning with Retrieval Augmented Language Models.pdf'},\n",
              " {'context': 'In the case of classiï¬cation tasks, the query corresponds to the textual input, and the model generates the\\nlexicalized class label, i.e. the word corresponding to the label. We give more examples of downstream tasks,\\nfrom the KILT benchmark in Figure 2. As many natural language processing tasks require knowledge , our\\ngoal is to enhance standard text-to-text models with retrieval, which, as we hypothesise in the introduction,\\nmay be crucial to endow models with few-shot capabilities.\\n2.1 Architecture\\nOur model is based on two sub-models: the retriever and thelanguage model . When performing a task,\\nfrom question answering to generating Wikipedia articles, our model starts by retrieving the top-k relevant\\ndocuments from a large corpus of text with the retriever. Then, these documents are fed to the language\\nmodel, along with the query, which in turns generates the output. Both the retriever and the language model\\nare based on pre-trained transformer networks, which we describe in more detail below.\\nRetriever. Our retriever module is based on the Contriever (Izacard et al., 2022), an information retrieval\\ntechnique based on continuous dense embeddings. The Contriever uses a dual-encoder architecture, where the\\nquery and documents are embedded independently by a transformer encoder (Huang et al., 2013; Karpukhin\\net al., 2020). Average pooling is applied over the outputs of the last layer to obtain one vector representation\\nper query or document. A similarity score between the query and each document is then obtained by\\ncomputing the dot product between their corresponding embeddings. The Contriever model is pre-trained\\nusing the MoCo contrastive loss (He et al., 2020), and uses unsupervised data only. As shown in the following\\nsection, an advantage of dense retrievers is that both query and document encoders can be trained without\\ndocument annotation, using standard techniques such as gradient descent and distillation.',\n",
              "  'question': 'In the context of text-to-text models with retrieval, what is the dual-encoder architecture used in the Contriever module?\\n\\n',\n",
              "  'answer': 'The dual-encoder architecture used in the Contriever module for text-to-text models with retrieval consists of two transformer encoders, where the query and documents are embedded independently. The outputs of the last layer are averaged to obtain one vector representation per query or document, and a similarity score is then computed between the query and each document using the dot product between their corresponding embeddings.',\n",
              "  'source_doc': 'Atlas_  Few-shot Learning with Retrieval Augmented Language Models.pdf'},\n",
              " {'context': 'While LMs can be highly unreliable, we find they can be rather efficient at searching the space\\nof solutions for multi-stage designs. A well-decomposed program can typically find at least a few\\ntraining examples where the LM can pass the constraints enforced by the signatures and metrics,\\nallowing us to bootstrap iteratively if needed.\\nStage 2: Parameter Optimization Now each parameter has a discrete set of candidates: demon-\\nstrations, instructions, etc. Many hyperparameter tuning algorithms (e.g., random search or Tree-\\nstructured Parzen Estimators as in HyperOpt (Bergstra et al., 2013) and Optuna (Akiba et al., 2019))\\ncan be applied for selection among candidates. We report simplified implementations of DSPyâ€™s\\nBootstrapFewShotWithRandomSearch andBootstrapFewShotWithOptuna in Appendix E.2 and\\nAppendix E.3.\\nAnother type of optimization is finetuning withBootstrapFinetune , where the demonstrations are\\nused to update the LMâ€™s weights for each predictor. When this is applied, the LM parameter of each\\nmodule is updated to the new LM weights. Typically, we are optimizing average quality using the\\nmetric with cross-validation over the training set or a validation set. This is applicable even with no\\nlabels for any stages, depending on the nature of metric.\\nStage 3: Higher-Order Program Optimization A different type of optimization that the DSPy\\ncompiler supports is modifying the control flow of the program. One of the simplest forms of\\nthese is ensembles, which we use in the case studies in this work. An ensemble will bootstrap\\nmultiple copies of the same program, and then replace the program with a new one that runs them\\nall in parallel and reduces their predictions into one with a custom function (e.g., majority voting).\\nIn future work, this stage can easily accommodate techniques for more dynamic (i.e., test-time)\\nbootstrapping as well as automatic backtracking-like logic.\\n5 G OALS OF EVALUATION',\n",
              "  'question': 'Can the use of ensembles in stage 3 of the DSPy compiler provide a significant improvement in the efficiency and accuracy of multi-stage designs?\\n\\n',\n",
              "  'answer': 'The use of ensembles in stage 3 of the DSPy compiler can improve the efficiency and accuracy of multi-stage designs by allowing for multiple copies of the same program to be bootstrapped and their predictions to be combined into a single output using a custom function. This can lead to a reduction in the overall number of training examples needed to achieve a satisfactory level of performance, as well as improved performance on the validation set. However, the effectiveness of ensembles will depend on the specific nature of the problem being solved and the quality of the individual programs being combined.',\n",
              "  'source_doc': 'DSPy_  Compiling Declarative Language Model Calls into Self-Improving Pipelines.pdf'},\n",
              " {'context': 'give an empirical examination of this training overhead in the appendix. A secondary limitation is a\\nmore complex loss and requirement to process sequences to mask backspaces during training. Future\\nwork can investigate the effect of training larger models against SequenceMatch objectives, as well\\nas how the qualities of generations change with choice of divergence.\\n10References\\n[1]Firas Al-Hafez, Davide Tateo, Oleg Arenz, Guoping Zhao, and Jan Peters. LS-IQ: Implicit\\nreward regularization for inverse reinforcement learning. In The Eleventh International Confer-\\nence on Learning Representations , 2023.\\n[2]Martin Arjovsky, Soumith Chintala, and L Â´eon Bottou. Wasserstein generative adversarial\\nnetworks. In International Conference on Machine Learning , pages 214â€“223. PMLR, 2017.\\n[3]Kushal Arora, Layla El Asri, Hareesh Bahuleyan, and Jackie Chi Kit Cheung. Why Exposure\\nBias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation.\\nInFindings of the Association for Computational Linguistics: ACL 2022 , pages 700â€“710, 2022.\\n[4]Paul Barde, Julien Roy, Wonseok Jeon, Joelle Pineau, Chris Pal, and Derek Nowrouzezahrai.\\nAdversarial soft advantage fitting: Imitation learning without policy optimization. In Advances\\nin Neural Information Processing Systems , volume 33, pages 12334â€“12344, 2020.\\n[5]Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von\\nArx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the\\nopportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 , 2021.\\n[6]James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal\\nMaclaurin, and Skye Wanderman-Milne. JAX: Composable transformations of Python+NumPy\\nprograms }, 2020.\\n[7]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-',\n",
              "  'question': 'Given the context, what is the training overhead of using SequenceMatch objectives and how does it compare to other methods?\\n\\n',\n",
              "  'answer': 'The training overhead of using SequenceMatch objectives is not explicitly stated in the context. However, it is mentioned that a more complex loss and the requirement to process sequences to mask backspaces during training may increase the training overhead. Additionally, the context suggests that future work can investigate the effect of training larger models against SequenceMatch objectives and how the qualities of generations change with choice of divergence, which may also impact the training overhead.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'elicit language modelsâ€™ reasoning capabilities and generate more accurate programs. We provide a detailed\\ncomparison in the experimental section.\\nWe show the proposed PoT prompting method in Figure 3 under the few-shot and zero-shot settings. Under\\nthe few-shot setting, a few exemplars of (question, â€˜program of thoughtsâ€™) pairs will be prefixed as demon-\\nstrations to teach the LLM how to generate â€˜thoughtfulâ€™ programs. Under the zero-shot setting, the prompt\\nonly contains an instruction without any exemplar demonstration. Unlike zero-shot CoT (Kojima et al.,\\n2022), which requires an extra step to extract the answer from the â€˜chain of thoughtsâ€™, zero-shot PoT can\\nreturn the answer straightforwardly without extra steps.\\nIn zero-shot PoT, a caveat is that LLM can fall back to generating a reasoning chain in comments rather than\\nin the program. Therefore, we propose to suppress â€˜#â€™ token logits to encourage it to generate programs.\\n4Published in Transactions on Machine Learning Research (10/2023)\\nFigure 4: PoT combined with CoT for multi-stage reasoning.\\n2.3 PoT as an Intermediate Step\\nFor certain problems requiring additional textual reasoning, we propose to utilize PoT to tackle the compu-\\ntation part. The program generated by PoT can be executed to provide intermediate result, which is further\\ncombined with the question to derive the final answer with CoT. We depict the whole process in Figure 8.\\nDuring demonstration, we present LLMs with examples to teach it predict whether to an additional CoT\\nreasoning needs to be used. If LLM outputs â€˜keep promptingâ€™ in the end, we will adopt the execution results\\nfrom PoT as input to further prompt LLMs to derive the answer through CoT.\\nForinstance,intheleftexampleinFigure3,theprogramwillbeexecutedtoreturnafloatnumberâ€˜ans=2.05â€™,\\nwhich means that after 2.05 hours the two trains will meet. However, directly adding 2.05 to 11 AM does not',\n",
              "  'question': 'Can a language model generate programs of thoughts using the proposed PoT prompting method, and if so, how does it differ from zero-shot CoT?\\n\\n',\n",
              "  'answer': \"Yes, a language model can generate programs of thoughts using the proposed PoT prompting method. Unlike zero-shot CoT, which requires an extra step to extract the answer from the 'chain of thoughts', zero-shot PoT can return the answer straightforwardly without extra steps. However, in zero-shot PoT, the LLM can fall back to generating a reasoning chain in comments rather than in the program. To encourage it to generate programs, we propose suppressing '#' token logits. PoT can be used as an intermediate step for certain problems requiring additional textual reasoning, and the program generated by PoT can be executed to provide intermediate results, which are further combined with the question to derive the final answer with CoT.\",\n",
              "  'source_doc': 'Program of Thoughts Prompting_  Disentangling Computation from Reasoning for Numerical Reasoning Tasks.pdf'},\n",
              " {'context': 'hension, algorithms, and simple mathematics, with some\\ncomparable to simple software interview questions. We\\nrelease this data along with an evaluation framework at\\nhttps://www.github.com/openai/human-eval.\\nTo solve a problem in our test set, we generate multiple\\nsamples from the models, and check if any of them pass the\\nunit tests. With just a single sample, a 12B parameter Codex\\nsolves 28.8% of these problems, and a 300M parameter\\nCodex solves 13.2% of these problems. In contrast, the 6B\\nparameter GPT-J (Wang & Komatsuzaki, 2021) achieves\\n11.4% on the same dataset, while all GPT models achieve\\nnear 0%. To improve our modelâ€™s performance at the task of\\nfunction synthesis from docstrings, we ï¬ne-tune Codex on\\nstandalone, correctly implemented functions. The resulting\\nmodel, Codex-S, solves 37.7% of problems with a single\\nsample. Figure 2 showcases problems of varying difï¬culty\\nin our dataset, along with correct model generated solutions.\\nReal-world programming tasks often involve iterations of\\napproaches and bug ï¬xes, which is approximated by gener-\\nating many samples from our models and selecting one that\\npasses all unit tests. Within 100 samples, Codex-S is able togenerate at least one correct function for 77.5% of the prob-\\nlems. This result suggests that accurate code samples can\\nbe selected via heuristic ranking instead of fully evaluating\\neach sample, the latter of which may not be possible or prac-\\ntical in deployment. Indeed, we ï¬nd that the sample with\\nhighest mean log-probability passes unit tests for 44.5% of\\nthe problems.\\nWe conclude by discussing the limitations and potential\\nbroader impacts of these Codex models and of increasingly\\npowerful code generating models more generally.\\n2. Evaluation Framework\\nIn this section, we discuss the details of our evaluation\\nframework. We begin by deï¬ning the pass@kmetric, and\\nexplain its advantages over standard match-based metrics.\\nNext, we describe the dataset of hand-written problems,',\n",
              "  'question': 'Given that Codex-S solves 37.7% of problems with a single sample and is able to generate at least one correct function for 77.5% of the problems within 100 samples, what is the performance of Codex-S in terms of generating accurate code samples?\\n\\n',\n",
              "  'answer': 'Codex-S has a performance of 37.7% in generating accurate code samples with a single sample and 77.5% within 100 samples.',\n",
              "  'source_doc': 'Evaluating Large Language Models Trained on Code.pdf'},\n",
              " {'context': 'e.g., it is unclear what 8 on a 10 point scale means across different scenarios. As such, we instead\\nrecommend using the Elo ranking method [ 16], based on pairwise judgments from human annotators\\nand GPT-4 to avoid the problem of grounding an absolute scale. Elo ratings of the most competitive\\n9Table 6: Zero-shot Vicuna benchmark scores as a percentage of the score obtained by ChatGPT evaluated by\\nGPT-4. We see that OASST1 models perform close to ChatGPT despite being trained on a very small dataset\\nand having a fraction of the memory requirement of baseline models.\\nModel / Dataset Params Model bits Memory ChatGPT vs Sys Sys vs ChatGPT Mean 95% CI\\nGPT-4 - - - 119.4% 110.1% 114.5 % 2.6%\\nBard - - - 93.2% 96.4% 94.8% 4.1%\\nGuanaco 65B 4-bit 41 GB 96.7% 101.9% 99.3% 4.4%\\nAlpaca 65B 4-bit 41 GB 63.0% 77.9% 70.7% 4.3%\\nFLAN v2 65B 4-bit 41 GB 37.0% 59.6% 48.4% 4.6%\\nGuanaco 33B 4-bit 21 GB 96.5% 99.2% 97.8% 4.4%\\nOpen Assistant 33B 16-bit 66 GB 91.2% 98.7% 94.9% 4.5%\\nAlpaca 33B 4-bit 21 GB 67.2% 79.7% 73.6% 4.2%\\nFLAN v2 33B 4-bit 21 GB 26.3% 49.7% 38.0% 3.9%\\nVicuna 13B 16-bit 26 GB 91.2% 98.7% 94.9% 4.5%\\nGuanaco 13B 4-bit 10 GB 87.3% 93.4% 90.4% 5.2%\\nAlpaca 13B 4-bit 10 GB 63.8% 76.7% 69.4% 4.2%\\nHH-RLHF 13B 4-bit 10 GB 55.5% 69.1% 62.5% 4.7%\\nUnnatural Instr. 13B 4-bit 10 GB 50.6% 69.8% 60.5% 4.2%\\nChip2 13B 4-bit 10 GB 49.2% 69.3% 59.5% 4.7%\\nLongform 13B 4-bit 10 GB 44.9% 62.0% 53.6% 5.2%\\nSelf-Instruct 13B 4-bit 10 GB 38.0% 60.5% 49.1% 4.6%\\nFLAN v2 13B 4-bit 10 GB 32.4% 61.2% 47.0% 3.6%\\nGuanaco 7B 4-bit 5 GB 84.1% 89.8% 87.0% 5.4%\\nAlpaca 7B 4-bit 5 GB 57.3% 71.2% 64.4% 5.0%\\nFLAN v2 7B 4-bit 5 GB 33.3% 56.1% 44.8% 4.0%\\nmodels can be seen in Table 1. We note that human and GPT-4 ranking of models on the Vicuna\\nbenchmark disagree partially, particularly for Guanaco 7B, but are consistent for most models with\\na Kendall Tau of Ï„= 0.43and Spearman rank correlation of r= 0.55at the system level. At the',\n",
              "  'question': 'Can Elo ratings be used to compare the performance of different language models in a zero-shot scenario?\\n\\n',\n",
              "  'answer': 'Yes, Elo ratings can be used to compare the performance of different language models in a zero-shot scenario. The Elo ranking method is based on pairwise judgments from human annotators and GPT-4, and it has been shown to be effective in avoiding the problem of grounding an absolute scale. The table shows that Elo ratings of the most competitive models perform close to ChatGPT despite being trained on a very small dataset and having a fraction of the memory requirement of baseline models. Therefore, Elo ratings can provide a reliable way to compare the performance of different language models in a zero-shot scenario.',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': 'performance. Integrating these hyper-parameters\\ninto the In-Context RALM formulation gives\\np(x 1, ..., x n) =\\nnsâˆ’1\\ue009\\nj=0s\\ue009\\ni=1pÎ¸\\ue004\\nxsÂ·j+i|\\ue00a\\nRC(qs,â„“\\nj);x <(sÂ·j+i)\\ue00b\\ue005\\n.\\n(4)\\n4 Experimental Details\\nWe now describe our experimental setup, including\\nall models we use and their implementation details.\\n4.1 Datasets\\nWe evaluated the effectiveness of In-Context\\nRALM across\\ue000ve diverse language modeling\\ndatasets and two common open-domain question\\nanswering datasets.\\nLanguage ModelingThe\\ue000rst LM dataset is\\nWikiText-103( Merity et al. ,2016 ), which has been\\nextensively used to evaluate RALMs ( Khandelwal\\net al.,2020 ;He et al. ,2021 ;Borgeaud et al. ,2022 ;\\nAlon et al. ,2022 ;Zhong et al. ,2022 ). Second, we\\nchose three datasets spanning diverse subjects from\\nThe Pile ( Gao et al. ,2021):ArXiv,Stack Exchange\\nandFreeLaw. Finally, we also investigatedReal-\\nNews( Zellers et al. ,2019 ), since The Pile lacks a\\ncorpus focused only on news (which is by nature a\\nknowledge-intensive domain).\\nOpen-Domain Question AnsweringIn order\\nto evaluate In-Context RALM on downstream\\ntasks as well, we use theNatural Questions(NQ;\\nKwiatkowski et al. 2019 ) andTriviaQA( Joshi et al. ,\\n2017 ) open-domain question answering datasets.\\n4.2 Models\\nLanguage ModelsWe performed our experi-\\nments using the four models of GPT-2 (110Mâ€“\\n1.5B; Radford et al. 2019 ), three models of GPT-\\nNeo and GPT-J (1.3Bâ€“6B; Black et al. 2021 ;Wangand Komatsuzaki 2021 ), eight models of OPT\\n(125Mâ€“66B; Zhang et al. 2022 ) and three mod-\\nels of LLaMA (7Bâ€“33B; Touvron et al. 2023 ). All\\nmodels are open source and publicly available.3\\nWe elected to study these particular models for\\nthe following reasons. The\\ue000rst four (GPT-2) mod-\\nels were trained on WebText ( Radford et al. ,2019),\\nwith Wikipedia documents excluded from their\\ntraining datasets. We were thus able to evaluate our\\nmethodâ€™s â€œzero-shotâ€ performance when retrieving\\nfrom a novel corpus (for WikiText-103). The rest of\\nthe models brought two further bene\\ue000ts. First, they',\n",
              "  'question': 'Can integrating hyper-parameters into the In-Context RALM formulation improve language modeling and open-domain question answering performance?\\n\\n',\n",
              "  'answer': 'Yes, integrating hyper-parameters into the In-Context RALM formulation can improve language modeling and open-domain question answering performance. The experimental results show that In-Context RALM outperforms other language models on diverse language modeling datasets and two common open-domain question answering datasets. Additionally, the study found that this method can retrieve from a novel corpus, making it useful for zero-shot performance evaluation.',\n",
              "  'source_doc': 'In-Context Retrieval-Augmented Language Models.pdf'},\n",
              " {'context': 'proved on this setup by learning a state vector used to con-\\ndition child node expansion. Later, Allamanis et al. (2015)\\napplied this idea in text-to-code retrieval and Yin & Neu-\\nbig (2017) utilized it in text-conditional code generation.\\nCode2seq (Alon et al., 2018) found that ASTs could also be\\nleveraged for code-to-text generation.\\nPrograms can also be synthesized without passing through\\nan AST representation. Hindle et al. (2012) investigated\\nn-gram language models of code, ï¬nding code to be more\\npredictable than natural language. Latent Predictor Net-\\nworks (Ling et al., 2016) showed that character-level lan-\\nguage models could generate working code for implement-\\ning Magic the Gathering cards in an online arena, when\\naided with a latent mode that allows card attributes to be\\ncopied into code. DeepCoder (Balog et al., 2017) trained\\na model to predict the functions appearing in source code,\\nwhich could be used to guide program search.\\nFollowing the success of large natural language models (De-\\nvlin et al., 2018; Radford et al., 2019; Liu et al., 2019; Raffel\\net al., 2020; Brown et al., 2020) large scale Transformers\\nhave also been applied towards program synthesis. Code-\\nBERT (Feng et al., 2020) trained the BERT objective on\\ndocstrings paired with functions, and obtained strong results\\non code search. PyMT5 (Clement et al., 2020) is similar in\\nspirit to our work, and used the T5 objective to train a sys-\\ntem which can translate between non-overlapping subsets\\noffsignature, docstring, body g.\\nWe used functional correctness to benchmark our models,\\nand observed improvements on this metric with more sam-\\npling. SPoC (Kulal et al., 2019) considered the problem\\nof producing functionally correct code from pseudocode\\nwith a ï¬xed budget of compilations, which is similar to our\\npass@kmetric. TransCoder (Lachaux et al., 2020) trained\\na system to translate between programming languages in\\nan unsupervised manner, and also observed that functional',\n",
              "  'question': 'Can a state vector be used to condition child node expansion in code-to-text generation?\\n\\n',\n",
              "  'answer': 'Yes, a state vector can be used to condition child node expansion in code-to-text generation. This approach has been successfully applied in text-to-code retrieval and text-conditional code generation.',\n",
              "  'source_doc': 'Evaluating Large Language Models Trained on Code.pdf'},\n",
              " {'context': 'In-Context Retrieval-Augmented Language Models\\nOri Ramâˆ—Yoav Levineâˆ—Itay Dalmedigos Dor Muhlgay\\nAmnon Shashua Kevin Leyton-Brown Yoav Shoham\\nAI21 Labs\\n{orir,yoavl,itayd,dorm,amnons,kevinlb,yoavs}@ai21.com\\nAbstract\\nRetrieval-Augmented Language Modeling\\n(RALM) methods, which condition a lan-\\nguage model (LM) on relevant documents\\nfrom a grounding corpus during generation,\\nwere shown to signi\\ue000cantly improve lan-\\nguage modeling performance. In addition,\\nthey can mitigate the problem of factually\\ninaccurate text generation and provide natu-\\nral source attribution mechanism. Existing\\nRALM approaches focus on modifying the\\nLM architecture in order to facilitate the in-\\ncorporation of external information, signi\\ue000-\\ncantly complicating deployment. This paper\\nconsiders a simple alternative, which we dub\\nIn-Context RALM: leaving the LM architec-\\nture unchanged and prepending grounding\\ndocuments to the input,without any further\\ntraining of the LM. We show that In-Context\\nRALM that builds on off-the-shelf general\\npurpose retrievers provides surprisingly large\\nLM gains across model sizes and diverse cor-\\npora. We also demonstrate that the document\\nretrieval and ranking mechanism can be spe-\\ncialized to the RALM setting to further boost\\nperformance. We conclude that In-Context\\nRALM has considerable potential to increase\\nthe prevalence of LM grounding, particularly\\nin settings where a pretrained LM must be\\nused without modi\\ue000cation or even via API\\naccess.1\\n1 Introduction\\nRecent advances in language modeling (LM) have\\ndramatically increased the usefulness of machine-\\ngenerated text across a wide range of use-cases\\nand domains ( Brown et al. ,2020 ). However, the\\nmainstream paradigm of generating text with LMs\\nbears inherent limitations in access to external\\nknowledge. First, LMs are not coupled with any\\nâˆ—Equal contribution.\\n1Our code is available at https://github.com/\\nAI21Labs/in-context-ralm\\nFigure 1: Our framework, dubbedIn-Context\\nRALM, provides large language modeling gains on',\n",
              "  'question': 'In what way does In-Context RALM differ from existing RALM approaches?\\n\\n',\n",
              "  'answer': 'In-Context RALM differs from existing RALM approaches by leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. This contrasts with existing RALM approaches that modify the LM architecture to facilitate the incorporation of external information.',\n",
              "  'source_doc': 'In-Context Retrieval-Augmented Language Models.pdf'},\n",
              " {'context': 'should postpone a nuclear test to four years, as well as a lingering diplomatic hostage\\nsituation. \"Iâ€™ve been trying to convince the American people that the nuclear submarine\\ntest was the (CIAâ€™s) worst mistake of our decade,\" Kim said. Last Update: Sunday, 15\\nJuly 2016 KSA 17:04 - GMT 13:04 - Updated: Sunday, 15 July 2016 KSA 17:04 - GMT 12:04 -\\nSource: Reuters Â©Copyright 2016 Cable News Service. All Rights Reserved. This material\\nmay not be published, broadcast, rewritten, or redistributed. This Story Filed Under:\\ncybersecurity, fbi, global, world, nonuk, Pyongyang, DPRK, PRC Companies: public reaction,\\nnational launch date, nyc, reisen think tank, launch, Yonhap News Agency, news-service,\\nunreisen North Korea Sources: McClatchy, F-Secure, NBC News, Time Warner Cable, F-Secure\\n(Sony), WSJ, NTSC, Sputnik.com Post navigation As you can see from the chart above, North\\nKorea has climbed to its second year of its 9 February election season, a considerable\\nadvancement over the debut year of previous years. Keep up to date with all the latest\\nnews with expert comment and analysis from Syngenta. Follow us on Twitter and Facebook\\nfor updates. As you can see in the graphic above, North Koreaâ€™s year is notable because\\nitâ€™s a much brighter point on the balance sheet with its long history of trade with other\\ncountries. However, America has both more pragmatic | and strategic | impulses in this.\\nAmerica will be more aware of whoâ€™s pushing North Korea, rather than just the actions of\\na few. Thatâ€™s not to say that President Obama and his Cabinet have been \"soft\" on the\\ncountryâ€™s nuclear program. But what they both done is specifically try to reaffirm the\\nneed for US engagement in the Korean peninsula. We see the same thing happening with\\nNorth Korea now as it did when the Obama administration first visited in 2006. The Trump\\nadministration, in emphasizing economic difficulties and its war on North Korean leader\\nKim Jong Un, can still influence the',\n",
              "  'question': \"In what ways does the American people's perception of the nuclear submarine test differ from the CIA's assessment of it as their worst mistake of the decade?\\n\\n\",\n",
              "  'answer': \"The American people's perception of the nuclear submarine test may differ from the CIA's assessment of it as their worst mistake of the decade due to varying levels of awareness and understanding of the issue. While the CIA may have a more technical and analytical perspective on the matter, the general public may have a more simplistic or emotional view of the situation. Additionally, the CIA's assessment may be influenced by factors such as political considerations, while the public's perception may be shaped by media coverage and personal experiences.\",\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'task. Scratchpad outperforms direct predic-\\ntion whether using ï¬ne-tuning or few-shot.\\nFew-shot Fine-tuning\\nDirect prediction 8.8% 31.8%\\nScratchpad 20.1% 50.7%\\n5 E XECUTING PYTHON PROGRAMS\\nWe have shown that scratchpads can help algorithm induction, that is, they can help models learn to\\nimplement a particular algorithm with direct algorithm-speciï¬c supervision. But needing to hand-\\ndesign the intermediate states for every new task is sub-optimal. In this section, we evaluate whether\\na model can learn to implement a new algorithm by executing arbitrary code. To test this capability,\\nwe follow the problem setup from Austin et al. (2021), in which language models are asked to predict\\nthe result of executing a given Python program on a particular input. Language models performed\\npoorly at this task, even on programs which are solutions to a programming tasks the model is able to\\nsolve. Here we show that the scratchpad technique can dramatically improve the ability of language\\nmodels to execute programs.\\nDirect execution prediction Our main baseline is the direct execution prediction procedure ex-\\nplored in Austin et al. (2021). Models are shown the source code for a function, and asked to predict\\nthe output of running the function on speciï¬c inputs. For example, the function in Figure 1 takes as\\ninput a string sand a character ch, and removes the ï¬rst and last instances of the character chfrom\\nthe string s. The direct execution prompt and target for this task are shown in the â€œDirect Execution\\nPredictionâ€ box in Figure 1. A task is considered solved under this regime if the model correctly\\noutputs the target string.\\nExecution prediction via scratchpad tracing As discussed above, direct execution prediction\\nrequires the model to correctly output the result of executing the entire function in a single pass.\\nDirect execution prediction has been shown to perform poorly on Python programs in Austin et al.',\n",
              "  'question': 'What is the difference between direct execution prediction and execution prediction via scratchpad tracing in the context of language models and Python programs?\\n\\n',\n",
              "  'answer': 'Direct execution prediction requires the model to correctly output the result of executing the entire function in a single pass, while execution prediction via scratchpad tracing allows the model to learn to implement a new algorithm by executing arbitrary code and tracing its intermediate states.',\n",
              "  'source_doc': 'Show Your Work_  Scratchpads for Intermediate Computation with Language Models.pdf'},\n",
              " {'context': 'adding the sentence â€œLetâ€™s think step by stepâ€ before LLMs\\ngenerate answers, the model can generate a step-by-step\\nthought process and significantly improve accuracy in solv-\\ning reasoning tasks(Kojima et al., 2022). Therefore, Large\\nLanguage Models are zero-shot Reasoners and can also be\\nconsidered as Zero-Shot data augmentation generators.\\nInstruction finetuning Multi-task learning improves the\\nperformance of language models in zero-shot settings. Many\\nworks have found that designing elaborated natural lan-\\nguage templates with instructions for each NLP task and\\nconnecting them breaks down barriers between tasks and\\nallows the language model to understand the data better\\n(Wei et al., 2021; Sanh et al., 2021; Ouyang et al., 2022;\\nWang et al., 2022b; Scialom et al., 2022; Chung et al., 2022;\\nMuennighoff et al., 2022; Iyer et al., 2022). In our work,\\nwe combine instruction finetuning and reasoning to unlock\\nmore potential in language models.\\n3. Training TeacherLM\\nA good teacher can be a beacon, guiding students toward\\nmastering the methods to solve problems. We aim the same\\nfor the TeacherLM. In this regard, we make two primary\\nefforts. Firstly, we construct a dataset comprising two mil-\\nlion detailed explanations. Secondly, we adopt a multi-stage\\nprogressive training mode, moving from generality towards\\nspecialization.\\n3.1. Dataset Construction\\nP3-Sense-3K We extract 58 supervised datasets from P3\\n(Sanh et al., 2021). Each dataset contains multiple prompts,\\nresulting in 529 tasks. To ensure sample balance, We select\\nat most 3,000 samples of less than 1,200 tokens for each\\ntask. There are 1,400,364 samples in total. Then we for-\\nmat multiple choice tasks in the form of â€œ Q:{question }\\n{options }A:{answer }â€. All other tasks are changed to the\\nform of â€œ Q:{question }A:{answer }â€.\\nMuffin-3W We extract 56 supervised datasets from Muffin\\n(Wei et al., 2021), and each dataset includes ten prompts.',\n",
              "  'question': 'Can Large Language Models be considered as Zero-Shot data augmentation generators?\\n\\n',\n",
              "  'answer': 'Yes, according to the context, Large Language Models can be considered as Zero-Shot data augmentation generators as they can generate a step-by-step thought process and significantly improve accuracy in solving reasoning tasks.',\n",
              "  'source_doc': 'TeacherLM_ Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise.pdf'},\n",
              " {'context': 'RAG (Lewis et al., 2020) or a more expensive approach. This method (not shown in our tables) achieves\\nexact match scores of 32.7% (RAG) and 38.4% (Ensemble), requiring 50 (RAG) or 450 (Ensemble) forward\\npasses of Gopher-280B per test-time question. Atlas, using the same 15 training examples and 50 passages\\nachieves 38.7 EM, despite having 25 Ã—fewer parameters, and requiring comparatively negligible compute.\\n4.5.3 FEVER Results\\nWe report results on the original 3-class FEVER fact checking test set in Table 9. We consider a 64-shot\\nsetting, with training examples uniformly sampled from the full training set. Unlike the development and\\ntest sets, the train set is imbalanced, with more positive labels than negative, posing a challenge for few-shot\\n14Table 9: Comparison to state-of-the-art on FEVER. We report accuracy on FEVER test set, for\\nwhich evaluation is available here: https://competitions.codalab.org/competitions/18814 . For the few-shot\\nsettings, our model uses ï¬ne-tuning while other models use prompting.â€ uses an index composed of the\\nFEVER Wikipedia corpus.\\n15-shot 65-shot Full dataset\\nGopher (Rae et al., 2021) 51.1 - -\\nProoFVer (Krishna et al., 2021) - - 79.5\\nAtlas 56.2 64.3 78.0 /80.1â€ \\nTable 10: Downstream results on the KILT hidden test sets Downstream metrics are accuracy (AIDA\\nCoNLL-YAGO, FEVER, T-REx, zero-shot RE), exact match (Natural Questions, HotpotQA, TriviaQA), or\\nF1 (Wizard of Wikipedia).\\nModelAIDA FEV T-REx zsRE NQ HoPo TQA WoW\\nacc acc acc acc em em em f1\\nGENRE (Cao et al., 2021) 89.9 - - - - - - -\\nSphere (Piktus et al., 2021) - 89.0 81.7 74.2 51.6 38.3 72.7 15.5\\nSEAL (Bevilacqua et al., 2022) - 89.5 83.6 74.6 53.7 40.5 70.9 18.3\\nRe2G (Glass et al., 2022) - 89.6 87.7 - 51.7 - 76.3 18.9\\nFID with RS (HofstÃ¤tter et al., 2022) - 92.2 85.2 83.761.2 39.1 84.620.6\\nAtlas, 64-shot 66.5 87.1 58.9 74.9 43.6 34.7 76.4 15.5\\nAtlas, full train set 90.6 93.5 85.1 80.8 61.3 50.6 84.021.6',\n",
              "  'question': 'In what ways does the performance of the Atlas model compare to other state-of-the-art models on the FEVER fact checking test set, particularly in terms of accuracy and compute requirements?\\n\\n',\n",
              "  'answer': 'The Atlas model achieves higher accuracy on the FEVER test set compared to some state-of-the-art models, such as Gopher and ProoFVer, while requiring comparatively negligible compute. However, it performs slightly worse than other models like Sphere and Re2G in terms of accuracy.',\n",
              "  'source_doc': 'Atlas_  Few-shot Learning with Retrieval Augmented Language Models.pdf'},\n",
              " {'context': 'learning agent trained on 105expert trajectories for each task type5.\\nWebShop CanReAct also interact with noisy real-world language environments for practical\\napplications? We investigate WebShop (Yao et al., 2022), a recently proposed online shopping\\nwebsite environment with 1.18M real-world products and 12k human instructions. Unlike ALFWorld,\\nWebshop contains a high variety of structured and unstructured texts (e.g. product titles, descriptions,\\nand options crawled from Amazon), and requires an agent to purchase a product based on a user\\ninstruction (e.g. â€œI am looking for a nightstand with drawers. It should have a nickel ï¬nish, and\\npriced lower than $140â€) through web interactions (e.g. search â€œnightstand drawersâ€, choose buttons\\nsuch as â€œcolor: modern-nickel-whiteâ€ or â€œback to searchâ€). This task is evaluated by average score\\n(percentage of desired attributes covered by the chosen product averaged across all episodes) and\\nsuccess rate (percentage of episodes where the chosen product satisï¬es all requirements) on 500 test\\ninstructions. We formulate Act prompts with actions to search, choose product, choose options,\\nand buy, with ReAct prompts additionally reasoning to determine what to explore, when to buy,\\nand what products options are relevant to the instruction. See Table 6 for an example prompt, and\\nTable 10 for model predictions in the Appendix. We compare to an imitation learning (IL) method\\n5Micheli & Fleuret (2021) ï¬netuned a GPT-2 model on 3553 task instances and achieved a much improved\\nperformance than BUTLER, but it is trained on all task types, thus not included as a baseline.\\n7Published as a conference paper at ICLR 2023\\nMethod Pick Clean Heat Cool Look Pick 2 All\\nAct (best of 6) 88 42 74 67 72 41 45\\nReAct (avg) 65 39 83 76 55 24 57\\nReAct (best of 6) 92 58 96 86 78 41 71\\nReAct-IM (avg) 55 59 60 55 23 24 48\\nReAct-IM (best of 6) 62 68 87 57 39 33 53\\nBUTLER g(best of 8) 33 26 70 76 17 12 22\\nBUTLER (best of 8) 46 39 74 100 22 24 37',\n",
              "  'question': 'What is the difference between WebShop and ALFWorld, and how do they differ in terms of the tasks they require agents to perform?\\n\\n',\n",
              "  'answer': 'WebShop is an online shopping website environment with 1.18M real-world products and 12k human instructions, while ALFWorld is a fictional environment for training agents to perform various tasks. WebShop requires agents to purchase a product based on a user instruction through web interactions, while ALFWorld requires agents to perform various tasks such as navigating a virtual environment and manipulating objects. WebShop contains a high variety of structured and unstructured texts, while ALFWorld is more structured. Additionally, WebShop evaluates task performance using average score and success rate, while ALFWorld uses various metrics such as reward and trajectory length.',\n",
              "  'source_doc': 'ReAct_ Synergizing Reasoning and Acting in Language Models.pdf'},\n",
              " {'context': 'relevant variables and their corresponding numer-\\nalsâ€ and â€œ calculate intermediate results (pay atten-\\ntion to calculation and commonsense) â€ instructions.\\nThis prompting variant is called the PS+ prompting\\nstrategy (see Figure 3 (b)). Despite its simplic-\\nity, PS+ strategy greatly improves the quality of\\nthe generated reasoning process. Moreover, this\\nprompting strategy can be easily customized to\\nsolve a variety of problems other than math reason-\\ning, such as commonsense and symbolic reasoning\\nproblems.\\nWe evaluate our proposed prompting on six\\nmath reasoning datasets, including AQuA (Ling\\net al., 2017), GSM8K (Cobbe et al., 2021), Mul-\\ntiArith, AddSub, SingleEq, and SV AMP (Patel\\net al., 2021), two commonsense reasoning datasets\\n(CommonsenseQA (Talmor et al., 2019) and Strat-\\negyQA (Geva et al., 2021)), and two symbolic rea-\\nsoning datasets (Last Letter and Coin Flip (Weiet al., 2022b)). The results of our experiments\\nwith GPT-3 show that our proposed Zero-shot-PS+\\nprompting consistently outperforms Zero-shot-CoT\\nacross all reasoning problems and datasets by a\\nlarge margin, and is comparable to or exceeds Zero-\\nshot-Program-of-Thought (PoT) Prompting (Chen\\net al., 2022)). Furthermore, although PS+ prompt-\\ning does not require manual demonstration exam-\\nples, it has a performance similar to an 8-shot CoT\\nprompting in arithmetic reasoning.\\nOverall, our results suggest that (a) Zero-shot PS\\nprompting is capable of generating a higher-quality\\nreasoning process than Zero-shot-CoT prompting,\\nas the PS prompts provide more detailed instruc-\\ntions guiding the LLMs to perform correct rea-\\nsoning tasks; (b) Zero-shot PS+ prompting outper-\\nforms Few-shot manual-CoT prompting on some\\ndatasets, indicating that in some instances it has\\nthe potential to outperform manual Few-shot CoT\\nprompting, which hopefully will spark further de-\\nvelopment of new CoT prompting approaches to\\nelicit reasoning in LLMs.\\n2 Plan-and-Solve Prompting\\nOverview. We introduce PS prompting, a new',\n",
              "  'question': 'Can you explain the difference between Zero-shot PS prompting and Zero-shot-CoT prompting in generating a reasoning process for LLMs?\\n\\n',\n",
              "  'answer': 'Zero-shot PS prompting and Zero-shot-CoT prompting are both methods used to generate a reasoning process for LLMs. However, Zero-shot PS prompting provides more detailed instructions guiding the LLMs to perform correct reasoning tasks, while Zero-shot-CoT prompting does not. Additionally, Zero-shot PS+ prompting outperforms Few-shot manual-CoT prompting on some datasets, indicating that it has the potential to outperform manual Few-shot CoT prompting in some instances. This suggests that Zero-shot PS prompting is capable of generating a higher-quality reasoning process than Zero-shot-CoT prompting.',\n",
              "  'source_doc': 'Plan-and-Solve Prompting_  Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models.pdf'},\n",
              " {'context': 'tion mechanism, and mainly aim at enhancing the throughput rather than the end-to-end latency.\\nAs SoT trades off throughput for end-to-end latency, SoT can make these throughput-oriented tech-\\nniques help with end-to-end latency . This interesting synergy offers opportunities for achieving\\nbetter trade-offs between latency and throughput in future serving systems.\\nIn contrast to model- and system-level techniques, SoT is a data-level technique in a new â€œcontent\\nco-organization for efficiencyâ€ paradigm . See Â§ 6 for more discussions.\\nEfficient LLM methods through parallel generation. Some prior work also addresses the sequen-\\ntial decoding issues. Speculative decoding (SD) methods (Stern et al., 2018) employ smaller models\\nto generate some consecutive tokens sequentially and apply the target LLMs to verify them paral-\\nlelly. Non-autoregressive generation (NAG) methods (Gu et al., 2018; Xiao et al., 2023) sample and\\nrefine consecutive tokens parallelly, often with the support of a modified and tuned model.\\nRelying on either assisting models or special models and sampling schemes, SD and NAG methods\\nconduct parallel verification or sampling and refinement of consecutive tokens . In contrast, SoT\\nprompts the LLM itself to plan the contents in a way that permits the parallel generation of tokens in\\ndifferent segments , by exploiting the emerging instruction-following and planning ability of LLMs.\\nPrompting methods for LLMs. Recent years have witnessed the emergence of the â€œpre-train,\\nprompt, and predictâ€ paradigm, which has shown promise in enhancing LLMsâ€™ quality in math and\\ncommonsense reasoning (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022; Chen et al., 2022)\\nand planning for multi-modality tasks (Shen et al., 2023; Zhu et al., 2023). Instead of focusing on\\nanswer quality, SoT is a first attempt at exploiting the power of prompting to improve efficiency .\\n6 L IMITATIONS , FUTURE WORK,AND OPEN QUESTIONS',\n",
              "  'question': 'In this context, what is the main goal of SoT (Synergistic Transformation) in LLM?\\n\\n',\n",
              "  'answer': 'The main goal of SoT in LLM is to enhance the throughput rather than the end-to-end latency.',\n",
              "  'source_doc': 'Skeleton-of-Thought_ Large Language Models Can Do Parallel Decoding.pdf'},\n",
              " {'context': 'gap between model predictions with and without API calls remains high.\\nmost likely token, we generate the <API> token\\nif it is one of the kmost likely tokens. Table 9\\nshows performance on the T-REx subset of LAMA\\nand on WebQS for different values of k. As ex-\\npected, increasing kleads to the model doing API\\ncalls for more examples â€“ from 40.3% and 8.5%\\nwithk= 1(i.e., regular greedy decoding) to 98.1%\\nand 100% for k= 10 . While for T-REx, there is\\nalready a clear improvement in performance with\\ngreedy decoding, on WebQS our model only starts\\nto make a substantial number of API calls as we\\nslightly increase k. Interestingly, for k= 1 the\\nmodel is calibrated to some extent: It decides to\\ncall APIs for examples that it would perform partic-\\nularly badly on without making API calls. This can\\nbe seen from the fact that performance on examples\\nwhere it decides notto make an API call (44.3 and\\n19.9) is higher than average performance if no API\\ncalls are made at all (34.9 and 18.9). However, this\\ncalibration is lost for higher values of k.\\nData Quality We qualitatively analyze some\\nAPI calls generated with our approach for different\\nAPIs. Table 10 shows some examples of texts from\\nCCNet augmented with API calls, as well as the\\ncorresponding score L\\x00\\ni\\x00L+\\nithat is used as a ï¬l-\\ntering criterion, and whether the API calls made by\\nthe model are intuitively useful in the given context.\\nAs can be seen, high values of L\\x00\\ni\\x00L+\\nitypically\\ncorrespond to useful API calls, whereas low values\\ncorrespond to API calls that do not provide any in-\\nformation that is useful for predicting future tokens.\\nThere are some exceptions, e.g., an API call forT-REx WebQS\\nk All AC NC % All AC NC %\\n0 34.9 â€“ 34.9 0.0 18.9 â€“ 18.9 0.0\\n1 47.8 53.0 44.3 40.3 19.3 17.1 19.9 8.5\\n3 52.9 58.0 29.0 82.8 26.3 26.5 6.6 99.3\\n10 53.5 54.0 22.5 98.1 26.3 26.4 â€“ 100.0\\nTable 9: Toolformer results on the T-REx subset of\\nLAMA and on WebQS for different values of kused\\nduring decoding. Numbers shown are overall perfor-',\n",
              "  'question': 'Why does the gap between model predictions with and without API calls remain high, even when using the most likely token to generate the <API> token?\\n\\n',\n",
              "  'answer': \"The gap between model predictions with and without API calls remains high because the model's calibration is lost for higher values of k. This means that the model is not making intuitively useful API calls in some cases, even when it is calibrated to do so for examples where it would perform poorly without making API calls. This can be seen from the examples in Table 10, where high values of L<sup>i</sup>L+ correspond to useful API calls, whereas low values correspond to API calls that do not provide any information that is useful for predicting future tokens.\",\n",
              "  'source_doc': 'Toolformer_ Language Models Can Teach Themselves to Use Tools.pdf'},\n",
              " {'context': 'as control groups in our experiments, where text-davinci-\\n003 serves as the teacher and augments the Strate-\\ngyQA, CREAK, and ECQA datasets in the same way as\\nTeacherLM-7.1B.Apart from training results in section 4.4, we found that\\nmanual explanations and text-davinci-003â€™s augmentation\\nboth read smoothly but are inherently less detailed than\\nTeacherLM-7.1Bâ€™s. Nevertheless, text-davinci-003 some-\\ntimes reiterates the content of the question, rendering limited\\naugmented information that could help the student model\\nimprove its abilities.\\n4.4. Results\\nMulti-task training In this experiment section, the model\\nunderwent extensive data training and thoroughly learned\\nthe rationales of data augmentation. As shown in Figure\\n3, models of different sizes were able to bring significant\\nbenefits in general. The full experiment results can be seen\\nin appendix B.\\nSingle-task training In this section, we divided the experi-\\nment into two parts. First, we directly fine-tune BLOOMZ-\\n7.1B for each task. In the StrategyQA and CREAK datasets,\\nTeacherLM-7.1B performs beyond manual annotation and\\nhas a more robust augmentation ability than text-davinci-\\n003 in the CREAK task.\\nSecond, we train BLOOMZ-7.1B on the augmented P3-\\nSense-3K dataset, obtaining P3-Augmented-BLOOMZ-\\n7.1B, and then fine-tune it on single tasks. The experiment\\nshows that this further improved the accuracy of the task\\nand, to some extent, solved the problem of insufficient data.\\nIn StrategyQA and CREAK, TeacherLM-7.1B also showsTeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise\\n0 2 42530\\n+1.84Bloom-1B1MMLU\\n0 2 450556065\\n+3.94CREAK\\n0 2 420304050\\n+3.42ECQA\\n0 2 445505560\\n+3.07StrategyQA\\n0 2 4253035\\n+1.97Bloom-3B\\n0 2 4506070\\n+1.09\\n0 2 4204060\\n+2.14\\n0 2 455606570\\n-0.44\\n0 2 42530354045\\n+3.98Bloom-7B10 2 4506070\\n+5.32\\n0 2 4204060\\n+1.96\\n0 2 4506070\\n+3.95\\n0 2 4253035\\n+2.66OPT-1B3\\n0 2 4506070\\n+1.97\\n0 2 4204060\\n+2.55\\n0 2 45060\\n+3.95\\n0 2 425303540\\n+1.84OPT-2B7\\n0 2 4506070\\n+3.28\\n0 2 4204060',\n",
              "  'question': 'What role does text-davinci-003 play in the experiment described in the context?\\n\\n',\n",
              "  'answer': \"Text-davinci-003 serves as a teacher and augments the Strate-gyQA, CREAK, and ECQA datasets in the same way as TeacherLM-7.1B. However, manual explanations and text-davinci-003's augmentation are inherently less detailed than TeacherLM-7.1B's. Despite this, text-davinci-003 sometimes reiterates the content of the question, rendering limited augmented information that could help the student model improve its abilities.\",\n",
              "  'source_doc': 'TeacherLM_ Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise.pdf'},\n",
              " {'context': 'ifstatements, forloops, exceptions, etc.) to logically connect the modules.\\nWe then develop the DSPy compiler (Sec 4), which optimizes any DSPy program to improve quality\\nor cost. The compiler inputs are the program, a few training inputs with optional labels, and a valida-\\ntion metric. The compiler simulates versions of the program on the inputs and bootstraps example\\ntraces of each module for self-improvement, using them to construct effective few-shot prompts\\nor finetuning small LMs for steps of the pipeline. Optimization in DSPy is highly modular: it is\\nconducted by teleprompters ,2which are general-purpose optimization strategies that determine how\\nthe modules should learn from data. In this way, the compiler automatically maps the declarative\\nmodules to high-quality compositions of prompting, finetuning, reasoning, and augmentation.\\nProgramming models like DSPy could be assessed along many dimensions, but we focus on the role\\nof expert-crafted prompts in shaping system performance. We are seeking to reduce or even remove\\ntheir role through DSPy modules (e.g., versions of popular techniques like Chain of Thought) and\\nteleprompters. We report on two expansive case studies: math word problems (GMS8K; Cobbe et al.\\n2021) and multi-hop question answering (HotPotQA; Yang et al. 2018) with explorations of chain\\nof thought, multi-chain reflection, multi-hop retrieval, retrieval-augmented question answering, and\\nagent loops. Our evaluations use a number of different compiling strategies effectively and show\\nthat straightforward DSPy programs outperform systems using hand-crafted prompts, while also\\nallowing our programs to use much smaller and hence more efficient LMs effectively.\\nOverall, this work proposes the first programming model that translates prompting techniques into\\nparameterized declarative modules and introduces an effective compiler with general optimiza-\\ntion strategies (teleprompters) to optimize arbitrary pipelines of these modules. Our main contri-',\n",
              "  'question': 'Given that DSPy is a programming model that translates prompting techniques into parameterized declarative modules and introduces an effective compiler with general optimization strategies (teleprompters) to optimize arbitrary pipelines of these modules, what is the role of expert-crafted prompts in shaping system performance and how does DSPy aim to reduce or remove their role?\\n\\n',\n",
              "  'answer': 'The role of expert-crafted prompts in shaping system performance is significant in many programming models. However, DSPy aims to reduce or remove their role by introducing parameterized declarative modules and an effective compiler with general optimization strategies (teleprompters). By doing so, DSPy enables the automatic mapping of declarative modules to high-quality compositions of prompting, finetuning, reasoning, and augmentation. This allows for the optimization of arbitrary pipelines of these modules, potentially leading to more efficient and effective systems.',\n",
              "  'source_doc': 'DSPy_  Compiling Declarative Language Model Calls into Self-Improving Pipelines.pdf'},\n",
              " {'context': 'output is supported by the cited evidence). Human annotators find SELF-RAGanswers are often\\nplausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is\\nconsistent with Menick et al. (2022). Human annotators also find ISRELand ISSUPreflection token\\npredictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated\\nexamples and explanations on assessments.\\n6 C ONCLUSION\\nThis work introduces SELF-RAG, a new framework to enhance the quality and factuality of LLMs\\nthrough retrieval on demand and self-reflection. SELF-RAGtrains an LM to learn to retrieve, generate,\\nand critique text passages and its own generation by predicting the next tokens from its original\\nvocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables\\nthe tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on\\nsix tasks using multiple metrics demonstrate that SELF-RAGsignificantly outperforms LLMs with\\nmore parameters or with conventional retrieval-augmented generation approaches.\\n10Preprint.\\nETHICAL CONCERNS\\nThis work aims to improve the factuality of LLM outputs, the lack of which continues to cause nu-\\nmerous real-world problems (e.g., spread of misinformation and provision of incorrect and dangerous\\nadvice). While our method shows significant improvements in terms of performance, factuality, and\\ncitation accuracy, it can still generate outputs that are not fully supported by the citations. We hope\\nthat explicit self-reflection and fine-grained attribution may help users verify factual errors in the\\nmodel outputs.\\nACKNOWLEDGMENTS\\nWe thank Sewon Min, Scott Wen-tau Yih, Sean Welleck, and Kawin Ethayarajh for fruitful discussions\\nin the early stages of this work. We thank Sewon Min, Joongwon (Daniel) Kim, and Sandy Kaplan\\nfor valuable feedback on the paper, and Tianyu Gao and Weijia Shi for their help on evaluations.',\n",
              "  'question': 'What does the study by Menick et al. (2022) suggest about the performance of SELF-RAGanswers and ISRELand ISSUPreflection token predictions?\\n\\n',\n",
              "  'answer': 'The study by Menick et al. (2022) suggests that SELF-RAGanswers are often plausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is consistent with the findings of human annotators. Additionally, the study suggests that ISRELand ISSUPreflection token predictions are mostly aligned with the assessments of human annotators.',\n",
              "  'source_doc': 'Self-Rag_ Learning to Retrieve, Generate, and Critique Through Self-Reflection.pdf'},\n",
              " {'context': 'et al., 1990), in which documents are represented as low dimensional dense vectors.\\nNeural network based information retrieval. Following the successful application of deep learning\\nmethods to natural language processing, neural networks techniques were introduced for information retrieval.\\nHuang et al. (2013) proposed a deep bag-of-words model, in which representations of queries and documents are\\ncomputed independently. A relevance score is then obtained by taking the dot product between representations,\\nand the model is trained end-to-end on click data from a search engine. This method was later reï¬ned by\\nreplacing the bag-of-words model by convolutional neural networks (Shen et al., 2014) or recurrent neural\\nnetwork (Palangi et al., 2016). A limitation of bi-encoders is that queries and documents are represented\\nby a single vector, preventing the model to capture ï¬ne-grained interactions between terms. Nogueira & Cho\\n2Published in Transactions on Machine Learning Research (08/2022)\\n(2019) introduced a cross-encoder model, based on the BERT model (Devlin et al., 2019), which jointly\\nencodes queries and documents. The application of a strong pre-trained model, as well as the cross-encoder\\narchitecture, lead to important improvement on the MS MARCO benchmark (Bajaj et al., 2016).\\nThe methods described in the previous paragraph were applied to re-rank documents, which were retrieved\\nwith a traditional IR system such as BM25. Gillick et al. (2018) ï¬rst studied whether continuous retrievers,\\nbased on bi-encoder neural models, could be viable alternative to re-ranking. In the context of question\\nanswering, Karpukhin et al. (2020) introduced a dense passage retriever (DPR) based on the bi-encoder\\narchitecture. This model is initialized with a BERT network, and trained discriminatively using pairs of\\nqueries and relevant documents, with hard negatives from BM25. Xiong et al. (2020) further extended this',\n",
              "  'question': 'Can bi-encoder neural models be used for information retrieval?\\n',\n",
              "  'answer': 'Yes, bi-encoder neural models have been used for information retrieval in various ways, such as in the deep bag-of-words model and the cross-encoder model. These models have been shown to improve the performance of traditional IR systems such as BM25.',\n",
              "  'source_doc': 'Unsupervised Dense Information Retrieval with Contrastive Learning.pdf'},\n",
              " {'context': 'over MBPP-aug (23.4% and 25.2% tasks executed correctly, respectively). However, combining\\nboth CodeNet and the single-line dataset seems lead to the highest performance; tracing produces\\nthe correct ï¬nal output for 26.6% of the tasks, and nearly a quarter of the tasks (24.6%) are traced\\nperfectly for all three examples. These results seem promising: the neural network can often exactly\\ntrace programs. In particular, greedily decoding from the best model produces the exact correct trace\\nfor almost 42% of all traces.\\n6 R ELATED WORK\\nThe tasks in this paper can be viewed as exploring one criticism of large language models, namely,\\nto what extent do they simply rely on surface-level statistical correlations on text, without learn-\\ning semantics or world knowledge (Bender & Koller, 2020)? In response, Li et al. (2021) provide\\nevidence that pre-trained language models do indeed construct approximate representations of the\\nsemantics of the situations they describe in text. In the context of programs, Austin et al. (2021)\\napproach this question by exploring the learning to execute task on MBPP, which we consider in\\nSection 5.2. The idea behind this task was to explore whether neural models for synthesis that gen-\\nerate code could also execute it. While that work ï¬nds existing models perform poorly at predicting\\nexecution, we show that adding a scratchpad allows these models to perform better.\\nWork in learning to execute has considered whether off-the-shelf recurrent neural networks\\n(Zaremba & Sutskever, 2014) or more specialized architectures (Dehghani et al., 2018; Bieber et al.,\\n2020; Wang et al., 2020) have an inductive bias that is sufï¬ciently well suited for executing and\\nreasoning about arbitrary code. The related problem of neural algorithm induction has attracted\\nconsiderable interest (Graves et al., 2014; Kurach et al., 2016; Kaiser & Sutskever, 2016; Graves\\net al., 2016; Reed & de Freitas, 2016; Veli Ë‡ckovi Â´c et al., 2020a;b). This work proposes new neural',\n",
              "  'question': 'How do pre-trained language models construct approximate representations of the semantics of the situations they describe in text?\\n\\n',\n",
              "  'answer': 'Pre-trained language models construct approximate representations of the semantics of the situations they describe in text by learning to associate words and phrases with their meanings and relationships within a given context. This allows them to understand the intended meaning of text, even if it is not explicitly stated or obvious from the surface-level statistical correlations. In the context of programs, this can involve learning to understand the logic and structure of code, as well as the relationships between different parts of a program and how they interact with each other.',\n",
              "  'source_doc': 'Show Your Work_  Scratchpads for Intermediate Computation with Language Models.pdf'},\n",
              " {'context': 'Next, we measure performance through direct comparisons between system outputs. We simplify\\nthe rating scheme to a three-class labeling problem that accounts for ties. We prompt GPT-4 to\\npick the best response or declare a tie and provide an explanation. We conduct these head-to-head\\ncomparisons on all permutations of pairs of systems on both the Vicuna and OA benchmarks.\\nHuman Evaluation While recent work indicates generative models can be effectively employed\\nfor system evaluations [ 19], the reliability GPT-4 ratings to assess chatbot performance is, to our\\nknowledge, yet to be proven to correlate with human judgments. Therefore, we run two parallel\\nhuman evaluations on the Vicuna benchmark matching both automated evaluation protocols described\\nabove. We use Amazon Mechanical Turk (AMT) and get two human annotators for comparisons to\\nChatGPT and three annotators for pairwise comparisons.\\nElo Rating With both human and automated pairwise comparisons, we create a tournament-style\\ncompetition where models compete against each other. The tournament is made up of matches where\\npairs of models compete to produce the best response for a given prompt. This is similar to how Bai\\net al. [4]and Chiang et al. [10] compare models, but we also employ GPT-4 ratings in addition to\\nhuman ratings. We randomly sample from the set of labeled comparisons to compute Elo [ 16,17].\\nElo rating, which is widely used in chess and other games, is a measure of the expected win-rate\\nrelative to an opponentâ€™s win rate, for example, an Elo of 1100 vs 1000 means the Elo 1100 player\\nhas an expected win-rate of approximately 65% against the Elo 1000 opponent; a 1000 vs 1000 or\\n1100 vs 1100 match results in an expected win-rate of 50%. The Elo rating changes after each match\\nproportionally to the expected outcome, that is, an unexpected upset leads to a large change in Elo\\nrating while an expected outcome leads to a small change. Over time, Elo ratings approximately',\n",
              "  'question': 'Can the Elo rating measure effectively assess the performance of chatbots in a tournament-style competition?\\n\\n',\n",
              "  'answer': \"Yes, the Elo rating can effectively assess the performance of chatbots in a tournament-style competition as it is a measure of the expected win-rate relative to an opponent's win rate. The Elo rating changes after each match proportionally to the expected outcome, meaning an unexpected upset leads to a large change in Elo rating while an expected outcome leads to a small change. Therefore, the Elo rating can provide a reliable measure of the performance of chatbots in a tournament-style competition.\",\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': 'and V . Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\\narXiv:1907.11692 , 2019.\\n[39] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y . Tay, D. Zhou, Q. V . Le, B. Zoph, J. Wei,\\net al. The flan collection: Designing data and methods for effective instruction tuning. arXiv\\npreprint arXiv:2301.13688 , 2023.\\n[40] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi. Metaicl: Learning to learn in context.\\narXiv preprint arXiv:2110.15943 , 2021.\\n[41] A. Nematzadeh, K. Burns, E. Grant, A. Gopnik, and T. Griffiths. Evaluating theory of mind in\\nquestion answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 2392â€“2400, 2018.\\n[42] OpenAI. Gpt-4 technical report. arXiv , 2023.\\n[43] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\\nAdvances in Neural Information Processing Systems , 35:27730â€“27744, 2022.\\n[44] G. Park, B. Park, S. J. Kwon, B. Kim, Y . Lee, and D. Lee. nuqmm: Quantized matmul for\\nefficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557 ,\\n2022.\\n[45] B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. arXiv preprint\\narXiv:2304.03277 , 2023.\\n[46] A. Poliak, J. Naradowsky, A. Haldar, R. Rudinger, and B. Van Durme. Hypothesis only baselines\\nin natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and\\nComputational Semantics , pages 180â€“191, 2018.\\n[47] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao,\\nS. Agrawal, and J. Dean. Efficiently scaling transformer inference. arXiv preprint\\narXiv:2211.05102 , 2022.\\n[48] G. Qin and J. Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv\\npreprint arXiv:2104.06599 , 2021.',\n",
              "  'question': 'In \"Roberta: A robustly optimized bert pretraining approach\" (arXiv:1907.11692), what is the objective of the pretraining process and how does it differ from other BERT models?\\n\\n',\n",
              "  'answer': 'The objective of the pretraining process in \"Roberta\" is to learn a language model that can perform well on a wide range of natural language processing tasks. The pretraining process involves training the model on a large corpus of text using a masked language modeling objective, which encourages the model to learn to fill in missing words in a sentence. This differs from other BERT models that are typically fine-tuned on specific tasks after pretraining. \"Roberta\" is designed to be more robust and flexible than other BERT models, allowing it to perform well on a variety of tasks without the need for extensive fine-tuning.',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': 'the phrase â€œIt followed in the wake of the 1845 U.S. annexation of Texas. . . â€ and CoVe generated a\\nverification question When did Texas secede from Mexico? which was answered with 1836 then an\\ninconsistency should be detected by this step.\\n3.4 F INAL VERIFIED RESPONSE\\nFinally, the improved response that takes verification into account is generated. This is executed by a\\nfinal few-shot prompt where the context takes into account all of the previous reasoning steps, the\\nbaseline response and verification question answer pairs, so that the corrections can take place. If the\\nFactor+Revise approach is used from subsection 3.3 then the output of the cross-check inconsistency\\ndetection is provided as well.\\n4 E XPERIMENTS\\nWe use various experimental benchmarks to measure the efficacy of CoVe in reducing hallucination,\\ncomparing against a number of baselines.\\n4.1 T ASKS\\nThe benchmarks we use range from list-based questions where the required answer is a set of entities,\\nto where the answer is a longform generation of multiple freeform sentences.\\n4.1.1 W IKIDATA\\nWe start by testing CoVe on a set of automatically generated questions using the Wikidata API1. We\\ncreate list questions of the form: â€œ Who are some [Profession]s who were born in [City]? â€.\\nFor example, â€œ Who are some politicians who were born in Boston? â€. The answer to these\\nquestions is a set of entities, where the gold list is obtained from the Wikidata knowledge base. This\\nresults in a dataset of 56 test questions, each typically containing âˆ¼600 known gold entities, but\\ntypically an LLM will produce a much shorter list. We then use the precision metric (micro-averaged)\\nto measure performance, in addition to reporting the averaged number of positive and negative entities\\nproduced.\\n4.1.2 W IKI-CATEGORY LIST\\nWe then proceed to a harder set-generation task. We use the QUEST (Malaviya et al., 2023) dataset\\nthat was created using Wikipedia Category lists. We convert these category names to questions by',\n",
              "  'question': 'In what year did Texas secede from Mexico?\\n',\n",
              "  'answer': '1836',\n",
              "  'source_doc': 'Chain-of-Verification Reduces Hallucination in Large Language Models.pdf'},\n",
              " {'context': 'shot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only\\nonce at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation\\non top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named\\nentities. Yet, the improved task performance of such approaches often comes at the expense of\\nruntime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of\\nattributions (Liu et al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to\\nlearn to use retrieval on-demand for diverse instruction-following queries and introduce controlled\\ngeneration guided by reflections tokens to further improve generation quality and attributions.\\nConcurrent RAG work. A few concurrent works2on RAG propose new training or prompting\\nstrategies to improve widely-adopted RAG approaches. Lin et al. (2023) fine-tune both the retriever\\nand LM on instruction-tuning datasets in two steps. While we also train our model on diverse\\ninstruction-following datasets, SELF-RAGenables retrieval on demand and selection of the best\\npossible model output via fine-grained self-reflection, making it widely applicable and more robust\\nand controllable. Yoran et al. (2023) use a natural language inference model and Xu et al. (2023) use\\na summarization model to filter out or compress retrieved passages before using them to prompt the\\nLM to generate the output. SELF-RAGprocesses passages in parallel and filters out irrelevant ones\\nthrough self-reflection, without relying on external models at inference. Moreover, our self-reflection\\nmechanism also evaluates other aspects of the model output quality including factuality. LATS (Zhou\\net al., 2023) prompt off-the-shelf LMs to search for relevant information for question answering tasks\\nand to generate with tree search, guided by LM-generated value scores. While their value function',\n",
              "  'question': 'In \"shot fine-tuning on task datasets\" (Izacard et al., 2022b), what are the main improvements made by Jiang et al. (2023) and Schick et al. (2023) over prior work in terms of task performance, runtime efficiency, robustness to irrelevant context, and attributions?\\n\\n',\n",
              "  'answer': 'Jiang et al. (2023) propose to adaptively retrieve passages for generation on top of a proprietary LLM, while Schick et al. (2023) train an LM to generate API calls for named entities. These approaches improve task performance compared to prior work, but come at the expense of runtime efficiency, robustness to irrelevant context, and lack of attributions.',\n",
              "  'source_doc': 'Self-Rag_ Learning to Retrieve, Generate, and Critique Through Self-Reflection.pdf'},\n",
              " {'context': 'machine generated text. This makes language mod-\\neling both a promising and an urgent new applica-\\ntion area for knowledge grounding, and motivates\\npromoting RALM approaches. Prior research has\\nalready investigated RALM, of course, but it is\\nnot yet widely deployed. One likely reason is that\\nexisting approaches rely upon\\ue000ne-tuning the LM,\\nwhich is typically dif\\ue000cult and costly, and is even\\nimpossible for LMs accessible only via an API.\\nThis paper presented the framework ofIn-\\nContext RALM, enabling frozen, off-the-shelf LMs\\nto bene\\ue000t from retrieval. We demonstrated that\\nsubstantial performance gains can be achieved by\\nusing general purpose retrievers, and showed that\\nadditional gains can be achieved by tailoring the\\ndocument selection to the LM setting. A recent\\nwork by Muhlgay et al. (2023 ) demonstrates that\\nIn-Context RALM is indeed able to improve the\\nfactuality of large LMs.\\nSeveral directions for further improvement re-\\nmain for future work. First, this paper considers\\nonly the case of prepending a single external docu-\\nment to the context; adding more documents could\\ndrive further gains (for example, using the frame-\\nwork of Ratner et al. 2022 ). Second, we retrieved\\ndocuments every\\ue000xed interval of stokens, but see\\npotential for large latency and cost gains by retriev-\\ning more sparsely, such as only when a specialized\\nmodel predicts that retrieval is needed.\\nWe release the code used in this work, for thecommunity to use and improve over. We hope it\\nwill drive further research of RALM, which will\\nenable its wider adoption.\\nAcknowledgements\\nWe would like to thank the reviewers and the Ac-\\ntion Editor for their valuable feedback.',\n",
              "  'question': 'What is the framework of In-Context RALM and how does it improve the factuality of large language models?\\n\\n',\n",
              "  'answer': 'The framework of In-Context RALM enables frozen, off-the-shelf language models to benefit from retrieval. It demonstrates that substantial performance gains can be achieved by using general purpose retrievers and shows that additional gains can be achieved by tailoring the document selection to the LM setting. The framework improves the factuality of large language models by allowing them to access relevant information from external documents.',\n",
              "  'source_doc': 'In-Context Retrieval-Augmented Language Models.pdf'},\n",
              " {'context': 'form of â€œ Q:{question }A:{answer }â€.\\nMuffin-3W We extract 56 supervised datasets from Muffin\\n(Wei et al., 2021), and each dataset includes ten prompts.\\nSimilarly, We select at most 30,000 samples of less thanTeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise\\n0.5 2 10 40 160 64020406080Average Human Expert\\nAverage Human Rateroursoursoursoursours\\nParameters (Billion)Accuracy (%)\\nTeacherLM-560M TeacherLM-1.1B TeacherLM-3B TeacherLM-7.1B\\nTeacherLM-176B Flan-PaLM-8B Flan-PaLM-62B Flan-PaLM-540B\\nGopher-1.4B Gopher-7.1B Gopher-280B GPT-3-13B\\nGPT-3-175B Atlas-11B GPT-Neox-20B Chinchilla-70B\\nGAL-120B GLM-130B BLOOM-176B\\nFigure 2: Average MMLU scores (%) for 57 tasks with model and human accuracy comparisons. TeacherLMs are in the\\n0-shot setting, and the rest are in the 5-shot setting.\\n1,200 tokens for each task. There are 1,155,767 samples in\\ntotal. All tasks are changed to the form of â€œ Q:{question }\\nA:{answer }â€.\\nTeacherData-2M Furthermore, we collect 2 million pieces\\nof multi-domain data not included in any public datasets for\\ntraining. We utilize manual annotation and the STaR (Zelik-\\nman et al., 2022) strategy to construct a five-element fixed\\nparadigm for each sample, including the question, answer,\\nfundamentals, chain of thought, and common mistakes. We\\nselect samples of less than 2048 tokens.\\n3.2. Training Procedure\\nTraining the teacher models aims to obtain checkpoints\\nexcelling at generating comprehensive explanations with\\nsolid generalization ability. In this section, we introduce our\\nbase models and the details of the multi-stage training.\\n3.2.1. M ODELS\\nIn this work, our base models are the BLOOM (Scao et al.,\\n2022) series ranging from 560 million to 176 billion pa-\\nrameters, which are pre-trained on the ROOTS (Lauren c Â¸on\\net al., 2022) corpus in 46 natural languages and 13 program-\\nming languages. BLOOM models are large decoder-only\\nlanguage models pre-trained for 350 billion tokens.\\n3.2.2. M ULTI -STAGE TRAINING',\n",
              "  'question': 'Given that the context describes the training procedure for teacher models in the Muffin-3W project, what is the objective of training these models?\\n\\n',\n",
              "  'answer': 'The objective of training the teacher models in the Muffin-3W project is to obtain checkpoints that excel at generating comprehensive explanations with solid generalization ability.',\n",
              "  'source_doc': 'TeacherLM_ Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise.pdf'},\n",
              " {'context': 'write the overall answer parallelly instead of sequentially. Fig. 1 illustrates how SoT produces the\\nfinal answer to a user question q.\\n(1) Skeleton stage. SoT first assembles a skeleton request ,Ts(question =q), using the skeleton\\nprompt template Ts(Prompt 1, and Prompt 3 in App. B.1) with the question qas the parameter. The\\nskeleton prompt template is written to guide the LLM to output a concise skeleton of the answer.\\nThen, we extract the Bpoints from the skeleton response Rsof the LLM.\\n(2) Point-expanding stage. Based on the skeleton, we let the LLM expand on each point in parallel.\\nSpecifically, for the point with index band skeleton Rs\\nb, SoT uses Tpe(question =q,skeleton =\\nRs,point index =b,point skeleton =Rs\\nb)as the point-expanding request for the LLM, where\\nTpeis the point-expanding prompt template (Prompt 2). Finally, after completing all points, we\\nconcatenate the point-expanding responses {Rpe\\nb}b=1,Â·Â·Â·,Bto get the final answer .\\nParallel point expanding. We conduct parallel point-expanding so that SoT is able to achieve a\\nspeed-up than normal decoding.\\n(1) For proprietary models with only API access , we can issue multiple parallel API calls to get an\\nend-to-end latency gain at the cost of an increased number of API requests and tokens.\\n(2) For open-source models that we can run locally , we let them process the point-expanding re-\\nquests as a batch (paddings are added to the left of the point-expanding requests). We explain below\\nwhy this could achieve speed-ups. A typical LLM generative process consists of two phases: (a)\\ntheprefilling phase in which the prompt is parsed to generate the key-value cache for further use,\\nand (b) the decoding phase in which tokens are generated one by one in a sequential manner. The\\ndecoding phase accounts for the majority of the end-to-end latency, especially when generating a\\nlong response. Note that the decoding phase is bottlenecked by weight loading instead of activation',\n",
              "  'question': 'Given that SoT uses parallel point-expanding to achieve a speed-up, does the use of proprietary models with API access or open-source models that can be run locally affect the speed-up achieved?\\n\\n',\n",
              "  'answer': 'The use of proprietary models with API access can achieve a speed-up by issuing multiple parallel API calls, but at the cost of an increased number of API requests and tokens. On the other hand, open-source models that can be run locally can achieve speed-ups by processing point-expanding requests as a batch, which allows for the use of padding to add to the left of the point-expanding requests. This can improve the speed of the typical LLM generative process, which consists of two phases: prefilling and decoding. The decoding phase is the bottleneck in the process, and the use of padding can help to reduce the end-to-end latency associated with sequential token generation.',\n",
              "  'source_doc': 'Skeleton-of-Thought_ Large Language Models Can Do Parallel Decoding.pdf'},\n",
              " {'context': 'based models on addition, including their ability to generalize out of the training distribution to\\nnumbers with more digits. Models were trained on 1-8 digit addition. The baseline models were\\ntrained without intermediate scratchpad steps.\\nFor all experiments, we used pre-trained dense decoder-only Transformer language models, ranging\\nin size from 2 million to 137 billion parameters. These models were pre-trained on web documents\\nand dialog data, and correspond to the models used in Austin et al. (2021).\\n3 A DDITION\\nAs a ï¬rst task, we consider integer addition. The baseline addition task presents two numbers as the\\ninput, and the target is their sum. For example:2\\nInput: 2 9 + 5 7\\nTarget: 8 6\\nWe implement the scratchpad by including the intermediate steps of the long addition algorithm in\\nthe target, as in Figure 2. We train several models on integer addition problems with inputs that have\\n1-8 digits. We then test performance on in-distribution addition problems (with up to 8 digit inputs),\\nand on out-of-distribution problems with 9 and 10 digit inputs. The models were ï¬ne-tuned on 100k\\nexamples for 5k steps with batch size 32. There are 10k in-distribution test examples, and 1k test\\nexamples for each out-of-distribution task. We examine the performance as a function of model size,\\nranging from 2M to 1B parameters. We compare performance to a baseline which includes the input\\nand target numbers, but no intermediate scratchpad steps.\\nResults Figure 3 compares the performance of the scratchpad algorithm with the baseline. We see\\nthat beyond a critical model size, models are able to solve the addition task using the scratchpad,\\nwhile models trained without a scratchpad fail to do so even at the largest tested scale. On the out-of-\\ndistribution tasks (9-10 digit addition), we ï¬nd that models trained without scratchpad completely\\nfail, while models trained with scratchpad show consistent improvement as a function of model size.\\n4 P OLYNOMIAL EVALUATION',\n",
              "  'question': 'What is the difference between integer addition and polynomial evaluation in the context of addition models?\\n\\n',\n",
              "  'answer': 'In the context of addition models, integer addition involves adding two integers as input and outputting their sum, while polynomial evaluation involves evaluating a polynomial function at a given input. The scratchpad algorithm is used in integer addition to store intermediate steps of the long addition algorithm, while it is not used in polynomial evaluation. The models used for integer addition are pre-trained dense decoder-only Transformer language models, while the models used for polynomial evaluation are likely different types of models.',\n",
              "  'source_doc': 'Show Your Work_  Scratchpads for Intermediate Computation with Language Models.pdf'},\n",
              " {'context': 'that code generation models raise further bias and represen-\\ntation issues beyond problematic natural language: Codex\\ncan generate code with structure that reï¬‚ects stereotypes\\nabout gender, race, emotion, class, the structure of names,\\nand other characteristics. Particularly in the context of users\\nwho might over-rely on Codex or use it without ï¬rst think-\\ning through project design, this issue could have signiï¬cant\\nsafety implications, giving further motivation to discourage\\nover-reliance. We discuss bias and representation issues\\nfurther in Appendix F. Filtration or modulation of generated\\noutputs, documentation, and other interventions may help\\nto mitigate these risks.\\n7.4. Economic and labor market impacts\\nCode generation and associated capabilities have several\\npossible economic and labor market impacts. While Codex\\nat its current capability level may somewhat reduce the cost\\nof producing software by increasing programmer produc-\\ntivity, the size of this effect may be limited by the fact that\\nengineers donâ€™t spend their full day writing code (O*NET,\\n2021). Other important tasks include conferring with col-\\nleagues, writing design speciï¬cations, and upgrading ex-\\nisting software stacks.2We also found that Codex imports\\npackages at different rates, which could advantage some\\npackage authors over others, particularly if programmers\\nand engineers come to rely on Codexâ€™s suggestions. Over a\\nlonger time horizon, the effects of this class of technologies\\non software-related labor markets and on the economy more\\ngenerally could be more substantial as capabilities improve.\\nMore study is needed both on the effects of code genera-\\ntion capabilities and on appropriate responses. We discuss\\neconomic and labor market implications in more detail in\\nAppendix H.\\n2Indeed, BLS classiï¬es computer programmers and software\\ndevelopers separately, where developers are more highly paid than\\nprogrammers, have more tasks indirectly related to writing and',\n",
              "  'question': 'In what ways does the use of code generation models raise further bias and representation issues beyond problematic natural language, and what are the potential economic and labor market impacts of these technologies?\\n\\n',\n",
              "  'answer': \"Code generation models can generate code with structure that reflects stereotypes about gender, race, emotion, class, the structure of names, and other characteristics. This issue could have significant safety implications, particularly in the context of users who might over-rely on Codex or use it without first thinking through project design. While Codex may somewhat reduce the cost of producing software by increasing programmer productivity, the size of this effect may be limited by the fact that engineers don't spend their full day writing code. Other important tasks include conferring with colleagues, writing design specifications, and upgrading existing software stacks. Codex imports packages at different rates, which could advantage some package authors over others, particularly if programmers and engineers come to rely on Codex's suggestions. Over a longer time horizon, the effects of these technologies on software-related labor markets and on the economy more generally could be more substantial as capabilities improve. More study is needed both on the effects of code generation capabilities and on appropriate responses.\",\n",
              "  'source_doc': 'Evaluating Large Language Models Trained on Code.pdf'},\n",
              " {'context': 'Results on the cross-lingual retrieval benchmark derived from MKQA are reported in Table 5, with per\\nlanguagedetailsfortheRecall@100andRecall@20reportedinTable13andTable14ofappendix. Interestingly\\nusing only supervised training data in English, our mContriever ï¬ne-tuned on MS MARCO outperforms\\nthe CORA retriever. Also, similarly to the results reported on Mr. TyDi, adding multilingual contrastive\\npre-training before ï¬ne-tuning on MS MARCO improves performance over its counterpart without pre-training.\\nOn MKQA, evaluation is performed by lowercasing both queries and documents, we observed that this\\nimproves performance. This does not impact the CORA retriever which is based on an uncased version of\\nmBERT.\\n6 Ablation studies\\nIn this section, we investigate the inï¬‚uence of diï¬€erent design choices on our method. In these ablations, all\\nthe models are pre-trained on English Wikipedia for 200k gradient steps, with a batch size of 2,048 (on 32\\nGPUs). Each ï¬ne-tuning on MS MARCO takes 20k gradient steps with a batch size of 512(on 8 GPUs),\\nusing AdamW and no hard negatives.\\nMoCo vs. in-batch negatives. First, we compare the two contrastive pre-training methods: MoCo and\\nin-batch negatives. As in in-batch negatives, the number of negative examples is equal to the batch size, we\\ntrain models with a batch size of 4,096 and restrict the queue in MoCo to the same number of elements. This\\nexperiment measures the eï¬€ect of using of momentum encoder for the keys instead of the same network as\\nfor the queries. Using a momentum also prevents from backpropagating the gradient through the keys. We\\nreport results, without ï¬ne-tuning on MS MARCO in Table 6. We observe that the diï¬€erence of performance\\nbetween the two methods is small, especially after ï¬ne-tuning on MS MARCO. We thus propose to use MoCo\\nas our contrastive learning framework, since it scales to a larger number of negative examples without the\\nneed to increase the batch size.',\n",
              "  'question': 'Does the performance of the mContriever model improve when using multilingual contrastive pre-training before fine-tuning on MS MARCO?\\n\\n',\n",
              "  'answer': 'Yes, the results show that adding multilingual contrastive pre-training before fine-tuning on MS MARCO improves the performance of the mContriever model over its counterpart without pre-training.',\n",
              "  'source_doc': 'Unsupervised Dense Information Retrieval with Contrastive Learning.pdf'},\n",
              " {'context': 'is to learn a set of conditional distributions that continue the sequence similarly to those sequences in\\nthe dataset (green arrows), and avoid incorrect next tokens (red arrows). Our method trains against\\ndivergences that more heavily punish out-of-distribution sequences. We additionally introduce a\\n<backspace> action which can backtrack from an erroneous token (dashed purple arrows).\\nissues can prevent autoregressive models trained with maximum-likelihood from generating fluent\\nsequences at evaluation time. First is the divergence measure used to evaluate the difference between\\nthe model and the data distribution. Because the MLE loss does not have any contribution from OOD\\nsequences, the behavior of the model on OOD sequences (such as those generated autoregressively)\\nis not constrained. We address this by minimizing the Ï‡2-divergence between a mixture of the data\\nand autoregressively generated sequences. This divergence is known to perform much better than\\nMLE in imitiation learning [11, 1].\\nSecondly, if a model generates an OOD token, there may be no natural continuation which is similar to\\nsequences from the data distribution, and so the model may be unable to return to the data distribution\\neven if our Ï‡2-divergence encourages this. To address this, we augment the generation process\\nwith a <backspace> action, which deletes the previous token, and allows the model to correct for\\nerroneous generations. By incorporating recent work in non-adversarial imitation learning [ 11],\\nour method, SequenceMatch , is able to train autoregressive models against alternative divergences\\nsuch as the Ï‡2-mixture divergence while augmenting the policy with a <backspace> action. The\\nSequenceMatch loss is a fully supervised loss without adversarial training, and can be applied on top\\nof pretrained models as a finetuning step. To summarize our contributions:\\nâ€¢We formulate the sequence generation problem as an imitation learning (IL) problem, and',\n",
              "  'question': 'What is the method used to train autoregressive models in the context of sequence generation?\\n\\n',\n",
              "  'answer': 'The method used to train autoregressive models in the context of sequence generation is SequenceMatch. SequenceMatch trains against divergences that more heavily punish out-of-distribution sequences and introduces a <backspace> action which can backtrack from an erroneous token. The SequenceMatch loss is a fully supervised loss without adversarial training and can be applied on top of pretrained models as a finetuning step.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'than exact match and simply check whether the\\ncorrect word is within the ï¬rst ï¬ve words predicted\\nby the model. As LAMA is based on statements\\nobtained directly from Wikipedia, we prevent Tool-\\nformer from using the Wikipedia Search API to\\navoid giving it an unfair advantage.\\nResults for all models can be seen in Table 3.\\nAll GPT-J models without tool use achieve similar\\nperformance. Crucially, Toolformer clearly outper-\\nforms these baseline models, improving upon the\\nbest baseline by 11.7, 5.2 and 18.6 points, respec-\\ntively. It also clearly outperforms OPT (66B) and\\nGPT-3 (175B), despite both models being much\\nlarger. This is achieved because the model inde-\\npendently decides to ask the question answering\\ntool for the required information in almost all cases\\n(98.1%); for only very few examples, it uses a dif-\\nferent tool (0.7%) or no tool at all (1.2%).\\n4.2.2 Math Datasets\\nWe test mathematical reasoning abilities on ASDiv\\n(Miao et al., 2020), SV AMP (Patel et al., 2021) and\\nthe MAWPS benchmark (Koncel-Kedziorski et al.,\\n2016). We again account for the fact that we test\\nall models in a zero-shot setup by using a more\\nlenient evaluation criterion: As the required output\\nis always a number, we simply check for the ï¬rstModel SQuAD Google-RE T-REx\\nGPT-J 17.8 4.9 31.9\\nGPT-J + CC 19.2 5.6 33.2\\nToolformer (disabled) 22.1 6.3 34.9\\nToolformer 33.8 11.5 53.5\\nOPT (66B) 21.6 2.9 30.1\\nGPT-3 (175B) 26.8 7.0 39.8\\nTable 3: Results on subsets of LAMA. Toolformer uses\\nthe question answering tool for most examples, clearly\\noutperforming all baselines of the same size and achiev-\\ning results competitive with GPT-3 (175B).\\nModel ASDiv SVAMP MAWPS\\nGPT-J 7.5 5.2 9.9\\nGPT-J + CC 9.6 5.0 9.3\\nToolformer (disabled) 14.8 6.3 15.0\\nToolformer 40.4 29.4 44.0\\nOPT (66B) 6.0 4.9 7.9\\nGPT-3 (175B) 14.0 10.0 19.8\\nTable 4: Results for various benchmarks requiring\\nmathematical reasoning. Toolformer makes use of the\\ncalculator tool for most examples, clearly outperform-',\n",
              "  'question': 'To what extent does Toolformer rely on external tools to perform well on mathematical reasoning tasks in comparison to other models?\\n\\n',\n",
              "  'answer': 'Toolformer relies on external tools, specifically the calculator tool, for most examples (70.6%) in comparison to other models. This is significantly higher than the other models tested, which use external tools for only a small percentage of examples. The results show that Toolformer performs significantly better than other models, achieving results competitive with GPT-3 (175B) despite being much smaller.',\n",
              "  'source_doc': 'Toolformer_ Language Models Can Teach Themselves to Use Tools.pdf'},\n",
              " {'context': 'and September 2012. While distributing it, Brazau sometimes yelled obscenities about\\nIslam \"in a tone of voice that suggested he was very angry and had little interest in\\ndebate,\" Clements said. Brazau had argued that he did not intend to promote hate speech;\\ninstead he wanted to stimulate debate about censorship, \"blasphemy laws\" and Sharia law,\\nClements said. Article Continued Below Clements disagreed.\"\"He presented that he could not create the views or arguments required in terms of hatred\\nwhich I thought would promote even greater debate,\" Clements said of Brazau. Brazauâ€™s\\nlawyers have accused him of lying to get away with distributing offensive flyers to\\ndelegitimize a mosque location. Clements said Brazau wasnâ€™t guilty of participating in\\nthe defacement of mosque equipment. He said because of his alleged victimization in an\\nanti-Muslim message for months, the flyer has become an \"internal crusade against the\\nfundamental rights of our Muslim community.\" He said, although Brazau received laments\\nfor his actions, police did not find any unlawful activity during the June 2012 incident.\\n\"They could not seem to distinguish that the flyer has done anything illegal,\" Clements\\nsaid. \"We did find up-close images of the flyer at a nearby parking lot on June 12,\\nwhich is a stunning part of what happened.\" The flyer said he had been planning to post\\nan editorial about the case on his website in January. He wrote a memo outlining how\\nhis comments to his prior remarks had affected his views on Islam. He told a friend at\\none point that \"I was trying to add as much drama as possible\" to Muslim accusations he\\nhad received about his recent conduct, Clements said, but Brazauâ€™s actions attracted too\\nmuch attention. Article Continued Below He denied defacing the apartment they shared\\nwith his brother. He was sentenced to 15 monthsâ€™ probation and ordered to pay $10,000\\nin restitution to his victims. Mesher Lalaini, the lawyer who represented Brazau on',\n",
              "  'question': 'Given the context, what was the purpose of the flyer distributed by Brazau in June 2012?\\n\\n',\n",
              "  'answer': 'The purpose of the flyer distributed by Brazau in June 2012 was to stimulate debate about censorship, \"blasphemy laws\" and Sharia law.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'âˆ¼0.15across the CoD candidates. From Table 1,\\nwe can see that this density aligns with human-written\\nsummaries ( 0.151), yet is noticeable higher than sum-\\nmaries produced with a vanilla GPT-4 prompt ( 0.122).\\nAutomatic Metrics. As an evaluator, GPT-4\\nhas been shown to adequately correlate to human\\njudgments (Fu et al., 2023; Liu et al., 2023a), even\\npotentially outperforming crowd-sourced workers\\non some annotation tasks (Gilardi et al., 2023). As\\na complement to our human evaluation (below), we\\nprompt GPT-4 to rate CoD summaries (1-5) along\\n5 dimensions: Informative ,Quality ,Coherence ,At-\\ntributable , andOverall . The definitions of Informa-\\ntive,Quality , and Attributable come from Aharoni\\net al. (2023), while Coherence comes from Fabbri\\net al. (2021)3.Overall aims to capture the qualities\\njointly. Please see Appendix A for the prompts used\\n3Quality andCoherence are article-independent metrics.to solicit scores for each dimension. Table 3 suggests\\nthat densification is correlated with informativeness,\\nyet there is a limit, with the score peaking at Step 4\\n(4.74). Article-free dimensions: Quality andCoher-\\nence, decline sooner (after 2 and 1 steps, respectively).\\nAll summaries are deemed Attributable to the source\\narticle. The Overall scores skew toward denser and\\nmore informative summaries, with Step 4 having the\\nhighest score. On average across dimensions, the first\\nand last CoD steps are least favored, while the mid-\\ndle three are close ( 4.78,4.77, and 4.76, respectively).\\nIn Appendix A, we report highest summary-\\nlevel correlations of the Overall metric to human\\njudgments (0.31 Pearson correlation), yet note low cor-\\nrelations overallâ€“a phenomenon observed by Deutsch\\net al. (2022) when summaries are of similar quality.\\nQualitative Analysis. There exists a clear trade-off\\nbetween coherence / readability of summaries and in-\\nformativeness. To illustrate, in Figure 4, we present\\ntwoCoD steps: one for which the summary is im-',\n",
              "  'question': 'To what extent does the performance of GPT-4 in rating CoD summaries across different dimensions align with human judgments?\\n\\n',\n",
              "  'answer': 'The performance of GPT-4 in rating CoD summaries across different dimensions aligns with human judgments to varying degrees. The Overall metric has the highest correlation with human judgments (0.31 Pearson correlation), while the Informative and Coherence dimensions have a moderate correlation (0.27 and 0.26 Pearson correlation, respectively). The Quality and Attributable dimensions have a low correlation with human judgments (0.15 and 0.13 Pearson correlation, respectively). Additionally, the first and last CoD steps are least favored by both GPT-4 and human evaluators, while the mid-dle three are close. These findings suggest that while GPT-4 is able to adequately correlate with human judgments for some dimensions, there is still room for improvement in its ability to capture the full range of qualities that humans consider important in summaries.',\n",
              "  'source_doc': 'From Sparse to Dense_  GPT-4 Summarization with Chain of Density Prompting.pdf'},\n",
              " {'context': 'prior work involves training the LM. We begin by\\ndescribing works that use this approach for tack-\\nling downstream tasks, and then mention works ori-\\nented towards RALM. Lewis et al. (2020) and Izac-\\nard and Grave (2021 )\\ue000ne tuned encoderâ€“decoder\\narchitectures for downstream knowledge-intensive\\ntasks. Izacard et al. (2022b ) explored different\\nways of pretraining such models, while Levine\\net al. (2022c ) pretrained an autoregressive LM on\\nclusters of nearest neighbors in sentence embed-\\nding space. Levine et al. (2022a ) showed competi-\\ntive open domain question-answering performance\\nby prompt-tuning a frozen LM as a reader. Guu\\net al. (2020 ) pretrained REALM, a retrieval aug-\\nmentedbidirectional, maskedLM, later\\ue000ne-tunedfor open-domain question answering. The work\\nclosest to this paperâ€”with a focus on the language\\nmodeling taskâ€”is RETRO ( Borgeaud et al. ,2022),\\nwhich modi\\ue000es an autoregressive LM to attend to\\nrelevant documents via chunked cross-attention,\\nthus introducing new parameters to the model. Our\\nIn-Context RALM differs from prior work in this\\nfamily of models in two key aspects:\\nâ€¢We useoff-the-shelfLMs for document read-\\ningwithout any further training of the LM.\\nâ€¢We focus onhow to choose documents for\\nimproved LM performance.\\n3 Our Framework\\n3.1 In-Context RALM\\nLanguage models de\\ue000ne probability distributions\\nover sequences of tokens. Given such a sequence\\nx1, ..., x n, the standard way to model its probabil-\\nity is via next-token prediction: p(x 1, ..., x n) =\\ue007n\\ni=1p(x i|x<i), where x<i:=x 1, ..., x iâˆ’1is the\\nsequence of tokens preceding xi, also referred to\\nas itspre\\ue000x. This autoregressive model is usu-\\nally implemented via a learned transformer net-\\nwork ( Vaswani et al. ,2017 ) parameterized by the\\nset of parametersÎ¸:\\np(x 1, ..., x n) =n\\ue009\\ni=1pÎ¸(xi|x<i),(1)\\nwhere the conditional probabilities are modeled\\nby employing a causal self-attention mask ( Rad-\\nford et al. ,2018 ). Notably, leading LMs such\\nas GPT-2 ( Radford et al. ,2019 ), GPT-3 ( Brown',\n",
              "  'question': 'What is the difference between the In-Context RALM and prior work in the family of models?\\n\\n',\n",
              "  'answer': 'The In-Context RALM differs from prior work in two key aspects. Firstly, it uses off-the-shelf LMs for document reading without any further training of the LM. Secondly, it focuses on how to choose documents for improved LM performance.',\n",
              "  'source_doc': 'In-Context Retrieval-Augmented Language Models.pdf'},\n",
              " {'context': 'queries and relevant documents, with hard negatives from BM25. Xiong et al. (2020) further extended this\\nwork by mining hard negatives with the model itself during optimization, and trained on the MS MARCO\\ndataset. Once a collection of documents, such as Wikipedia articles, is encoded, retrieval is performed\\nwith a fast k-nearest neighbors library such as FAISS Johnson et al. (2019). To alleviate the limitations of\\nbi-encoders, Humeau et al. (2019) introduces the poly-encoder architecture, where documents are encoded\\nby multiple vectors. Similarly, Khattab et al. (2020) proposes the ColBERT model, which keeps a vector\\nrepresentation for each term of the queries and documents. To make the retrieval tractable, the term-level\\nfunction is approximated to ï¬rst retrieve an initial set of candidates, which are then re-ranked with the true\\nscore. In the context of question answering, knowledge distillation has been used to train retrievers, either\\nusing the attention scores of the reader of the downstream task as synthetic labels (Izacard & Grave, 2020a),\\nor the relevance score from a cross encoder (Yang & Seo, 2020). Luan et al. (2020) compares, theoretically and\\nempirically, the performance of sparse and dense retrievers, including bi-, cross- and poly-encoders. Dense\\nretrievers, such as DPR, can lead to indices weighing nearly 100GB when encoding document collections such\\nas Wikipedia. Izacard et al. (2020) shows how to compress such indices, with limited impact on performance,\\nmaking them more practical to use.\\nSelf-supervised learning for NLP. Following the success of word2vec (Mikolov et al., 2013), many\\nself-supervised techniques have been proposed to learn representation of text. Here, we brieï¬‚y review the ones\\nthat are most related to our approach: sentence level models and contrastive techniques. Jernite et al. (2017)\\nintroduced diï¬€erent objective functions to learn sentence representations, including next sentence prediction',\n",
              "  'question': 'In NLP, what is the difference between sentence level models and contrastive techniques?\\n\\n',\n",
              "  'answer': 'Sentence level models and contrastive techniques are two different approaches to learning representation of text in NLP. Sentence level models focus on learning representations of sentences, while contrastive techniques focus on learning representations of sentences through contrastive learning.',\n",
              "  'source_doc': 'Unsupervised Dense Information Retrieval with Contrastive Learning.pdf'},\n",
              " {'context': 'Table 3: Comparison of Chat models. Mistral 7B â€“\\nInstruct outperforms all 7B models on MT-Bench, and\\nis comparable to 13B â€“ Chat models.To evaluate the generalization capabilities of\\nMistral 7B, we fine-tuned it on instruction datasets\\npublicly available on the Hugging Face repository.\\nNo proprietary data or training tricks were utilized:\\nMistral 7B â€“ Instruct model is a simple and\\npreliminary demonstration that the base model can\\neasily be fine-tuned to achieve good performance.\\nIn Table 3, we observe that the resulting model,\\nMistral 7B â€“ Instruct, exhibits superior perfor-\\nmance compared to all 7B models on MT-Bench,\\nand is comparable to 13B â€“ Chat models. An\\nindependent human evaluation was conducted on\\nhttps://llmboxing.com/leaderboard .\\nIn this evaluation, participants were provided with a set of questions along with anonymous responses\\nfrom two models and were asked to select their preferred response, as illustrated in Figure 6. As of\\nOctober 6, 2023, the outputs generated by Mistral 7B were preferred 5020 times, compared to 4143\\ntimes for Llama 2 13B.\\n4Figure 5: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension for\\nMistral 7B and Llama 2 (7B/13B/70B) . Mistral 7B largely outperforms Llama 2 13B on all evaluations, except\\non knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which limits the\\namount of knowledge it can compress).\\n5 Adding guardrails for front-facing applications\\nThe ability to enforce guardrails when it comes to AI generation is important for front-facing appli-\\ncations. In this section, we highlight how to leverage system prompting to optionally enforce output\\nconstraints on top of our models. Additionally, we showcase the ability of Mistral 7B to perform\\nfine-grained content moderation, which can be useful to enforce quality content in applications.\\n5.1 System prompt to enforce guardrails',\n",
              "  'question': 'How does the performance of the Mistral 7B â€“ Instruct model compare to other 7B models on MT-Bench and 13B â€“ Chat models?\\n\\n',\n",
              "  'answer': 'The Mistral 7B â€“ Instruct model outperforms all 7B models on MT-Bench and is comparable to 13B â€“ Chat models. An independent human evaluation conducted on <https://llmboxing.com/leaderboard> showed that the outputs generated by Mistral 7B were preferred 5020 times, compared to 4143 times for Llama 2 13B.',\n",
              "  'source_doc': 'Mistral7B.pdf'},\n",
              " {'context': 'This accords with our intuition in Â§ 4.2. (3) The prompting and trained routers could even surpass\\nhuman router (e.g., on roleplay questions; see more examples on WizardLM in App. I.2).\\nWe discuss the consistency across three routers in App. C.3. The primary takeaways include: (1)\\non Vicuna-80, there is a notable consistency among all three routers, and (2) on WizardLM, greater\\ndiscrepancies emerge, with the trained router showing higher alignment with human annotations.\\n5 R ELATED WORK\\nThis section positions SoT in related work to reveal how SoT (1) is connected to, (2) is different\\nfrom, and (3) can harness the power of other methods. See App. D for the expanded discussion.\\nEfficient LLM methods at model and system levels. At the model level, prior work proposes ef-\\nficient architectures, including dynamic mixture-of-experts (Lepikhin et al., 2021), low-complexity\\n8Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding\\nattention (Kitaev et al., 2020), and multi-query attention (Shazeer, 2019). However, they usually\\nrequire a significant re-training cost. In contrast, compression methods require a smaller amount\\nof fine-tuning cost by reducing the complexity of pre-trained LLMs, such as quantization (Frantar\\net al., 2022) and weight or activation sparsification (Mishra et al., 2021; Zaheer et al., 2020).\\nAt the system level, prior work (1) optimizes the computational graph (Dao et al., 2022), (2) op-\\ntimizes the assignment and scheduling of computational graph on devices (Sheng et al., 2023), or\\n(3) designs batching or caching mechanisms for serving multiple users (Fang et al., 2021). These\\ntechniques address the large memory access and footprint posed by the vast model scale and atten-\\ntion mechanism, and mainly aim at enhancing the throughput rather than the end-to-end latency.\\nAs SoT trades off throughput for end-to-end latency, SoT can make these throughput-oriented tech-',\n",
              "  'question': 'Can we use SoT (1) to optimize the computational graph at the system level?\\n\\n',\n",
              "  'answer': 'Yes, SoT (1) can be used to optimize the computational graph at the system level as it is mentioned in the context that prior work (1) optimizes the computational graph.',\n",
              "  'source_doc': 'Skeleton-of-Thought_ Large Language Models Can Do Parallel Decoding.pdf'},\n",
              " {'context': 'algorithms depending on the tree structure. We explore two relatively simple search algorithms and\\nleave more advanced ones (e.g. A* [9], MCTS [2]) for future work:\\n(a)Breadth-ï¬rst search (BFS) (Algorithm 1) maintains a set of the bmost promising states\\nper step. This is used for Game of 24 and Creative Writing where the tree depth is limit\\n(T\\x143), and initial thought steps can be evaluated and pruned to a small set ( b\\x145).\\n(b)Depth-ï¬rst search (DFS) (Algorithm 2) explores the most promising state ï¬rst, until the\\nï¬nal output is reached ( t > T ), or the state evaluator deems it impossible to solve the\\nproblem from the current s(V(p\\x12;fsg)(s)\\x14vthfor a value threshold vth). In the latter\\ncase, the subtree from sispruned to trade exploration for exploitation. In both cases, DFS\\nbacktracks to the parent state of sto continue exploration.\\nConceptually, ToT has several beneï¬ts as a method for general problem-solving with LMs: (1) Gener-\\nality. IO, CoT, CoT-SC, and self-reï¬nement can be seen as special cases of ToT (i.e. trees of limited\\ndepth and breadth; Figure 1). (2) Modularity. The base LM, as well as the thought decomposition,\\ngeneration, evaluation, and search procedures can all be varied independently. (3) Adaptability .\\nDifferent problem properties, LM capabilities, and resource constraints can be accommodated. (4)\\nConvenience. No extra training is needed, just a pre-trained LM is sufï¬cient. The next section will\\nshow how these conceptual beneï¬ts translate to strong empirical performance in different problems.\\n4 Experiments\\nWe propose three tasks that are hard even when sampling from the state-of-the-art language model,\\nGPT-4 [ 20], using standard IO prompting or chain-of-thought (CoT) prompting. We show how\\n4Game of 24 Creative Writing 5x5 Crosswords\\nInput 4 numbers (4 9 10 13) 4 random sentences 10 clues (h1. presented;..)\\nOutput An equation to reach 24\\n(13-9)*(10-4)=24A passage of 4 paragraphs\\nending in the 4 sentences5x5 letters: SHOWN;\\nWIRRA; A V AIL; ...',\n",
              "  'question': 'What is the difference between Breadth-first search (BFS) and Depth-first search (DFS) in terms of their exploration strategy and pruning mechanism?\\n\\n',\n",
              "  'answer': 'Breadth-first search (BFS) and Depth-first search (DFS) are two different search algorithms with different exploration strategies and pruning mechanisms. BFS maintains a set of the most promising states per step and explores all the neighboring states at the current depth before moving on to the next level. This allows BFS to explore all possible paths at a given depth before moving on to the next level, making it useful for problems with a limited tree depth and initial thought steps that can be evaluated and pruned to a small set. On the other hand, DFS explores the most promising state first and backtracks to the parent state if the subtree from the current state cannot be solved or if the state evaluator deems it impossible to solve the problem from the current state. DFS is useful for problems with a large tree depth where exploring all possible paths is not feasible. Both algorithms have their own benefits and drawbacks, and the choice of algorithm depends on the problem properties, LM capabilities, and resource constraints.',\n",
              "  'source_doc': 'Tree of Thoughts_  Deliberate Problem Solving with Large Language Models.pdf'},\n",
              " {'context': '2022) series in a multi-task setting. Compared to non-\\naugmented versions, the experimental results indicate\\nthat TeacherLMâ€™s data augmentation gains clear bene-\\nfits.\\nâ€¢Fight big with small TeacherLM-7.1B model achieved\\na zero-shot score of 52.3 on MMLU, surpassing most\\nmodels with over 100B parameters.\\nâ€¢Cost friendly TeacherLM-7.1B has only 7.1 billion\\nparameters; compared to models such as text-davinci-\\n003 , it has efficient inference speed and lower running\\nconfiguration requirements. Therefore, with the sig-\\nnificant cost reduction, we can augment NLP datasets\\nof millions of levels, further opening the door to the\\nreasoning world.\\nâ€¢Open source We will release the TeacherLM series of\\nmodels and augmented datasets as open-source.\\n2. Related Work\\nThis paper explores the intersection of various NLP re-\\nsearch fields, including multi-task learning, instruction tun-\\ning, multi-step reasoning, and data augmentation. In this\\nsection, we will introduce several key related works.\\nReasoning via finetuning In this study, we construct a\\nlarge-scale Reasoning dataset for training TeacherLM. Pre-\\nvious works have utilized manually annotated multi-stepreasoning to improve model performance (Ling et al., 2017;\\nCamburu et al., 2018; Rajani et al., 2019; Talmor et al., 2020;\\nCobbe et al., 2021; Nye et al., 2021; Zelikman et al., 2022;\\nChung et al., 2022). Compared to these works, TeacherLM-\\n7.1B has certain advantages when compared to models of\\nthe same scale.\\nUsing large models as zero-Shot data augmentation gen-\\nerators Combining chain of thought and in-context learning\\nhas unlocked more robust reasoning capabilities for LLMs\\n(Wei et al., 2022b; Suzgun et al., 2022; Lampinen et al.,\\n2022), guiding models to move from learning to rote infor-\\nmation to now learning to think critically. Furthermore, by\\nadding the sentence â€œLetâ€™s think step by stepâ€ before LLMs\\ngenerate answers, the model can generate a step-by-step\\nthought process and significantly improve accuracy in solv-',\n",
              "  'question': 'Can the TeacherLM-7.1B model achieve a zero-shot score of 52.3 on MMLU with only 7.1 billion parameters, compared to models such as text-davinci-003 with over 100B parameters?\\n\\n',\n",
              "  'answer': 'Yes, the TeacherLM-7.1B model achieved a zero-shot score of 52.3 on MMLU, surpassing most models with over 100B parameters, despite having only 7.1 billion parameters. This is due to its efficient inference speed and lower running configuration requirements, which make it cost-friendly and allow for the augmentation of NLP datasets of millions of levels.',\n",
              "  'source_doc': 'TeacherLM_ Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise.pdf'},\n",
              " {'context': 'a novel â€œdata-levelâ€ pathway by letting the LLM organize its output content. This novel perspective\\nis becoming feasible and is expected to grow in relevance, owing to the evolving capabilities of\\nstate-of-the-art LLMs. We hope this work can stimulate more research in the realm of data-centric\\noptimization (Zha et al., 2023; HazyResearch, 2023) for efficiency.\\n2Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding\\nPrompt 1. Skeleton Prompt Template Ts\\n[User:] Youâ€™re an organizer responsible for only giving the skeleton (not the full content) for answering the question.\\nProvide the skeleton in a list of points (numbered 1., 2., 3., etc.) to answer the question. Instead of writing a full sentence,\\neach skeleton point should be very short with only 3 âˆ¼5 words. Generally, the skeleton should have 3 âˆ¼10 points. Now,\\nplease provide the skeleton for the following question.\\n{question }\\nSkeleton:\\n[Assistant:] 1.\\nPrompt 2. Point-Expanding Prompt Template Tpe\\n[User:] Youâ€™re responsible for continuing the writing of one and only one point in the overall answer to the following\\nquestion.\\n{question }\\nThe skeleton of the answer is\\n{skeleton }\\nContinue and only continue the writing of point {point index }. Write it **very shortly** in 1 âˆ¼2 sentence and\\ndo not continue with other points!\\n[Assistant:] {point index }.{point skeleton }\\nThe rest of the paper is organized as follows. We first introduce SoT in Â§ 2 and show its results in\\nÂ§ 3. Then, we expand on the SoT-R extension in Â§ 4. Â§ 5 positions SoT in the research ecosystem\\n(expanded in App. D). Finally, we analyze the limitations and share outlooks of SoT in Â§ 6.\\n2 S KELETON -OF-THOUGHT (SOT)\\n2.1 M ETHOD\\nOverview. Based on the intuition that humans usually think about and answer a question in an\\norganized way, the core idea of this work is to guide the LLM itself to give a skeleton first and then\\nwrite the overall answer parallelly instead of sequentially. Fig. 1 illustrates how SoT produces the',\n",
              "  'question': 'Given that LLMs are increasingly capable of organizing their output content, what is the intuition behind the \"skeleton-first\" approach to answering questions?\\n\\n',\n",
              "  'answer': 'The intuition behind the \"skeleton-first\" approach is that humans typically think about and answer questions in an organized manner. By guiding the LLM to give a skeleton first and then write the overall answer parallelly, the LLM can mimic this human-like thinking process and potentially improve the efficiency of data-centric optimization.',\n",
              "  'source_doc': 'Skeleton-of-Thought_ Large Language Models Can Do Parallel Decoding.pdf'},\n",
              " {'context': \"calls a retriever model on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple\\nretrieved passages, evaluating their relevance and then generating corresponding task outputs (Step\\n2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms\\nof factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which\\n1Our code and trained models are available at https://selfrag.github.io/ .\\n1arXiv:2310.11511v1  [cs.CL]  17 Oct 2023Preprint.\\nStep 1: Retrieve K documentsCalifornia was named after a ï¬ctional island in a Spanish book. Prompt How did US states get their names? \\nUS states got their names from a variety of sources. Eleven states are named after an individual person (e.g, California was named after Christopher Columbus). Some states including Texas and Utah, are named after Native American tribe.\\nRetrieval-Augmented Generation (RAG)Ours: Self-reï¬‚ective Retrieval-Augmented Generation (Self-RAG) \\nPopular names by states. In Texas, Emma is a popular baby name. Of the ï¬fty states, eleven are named after an individual person. \\nPrompt How did US states get their names? + Step 2: Prompt LM with K docs and generateRetriever\\nLM\\nPrompt How did US states get their names? US states got their names from a variety of sources. RetrieveStep 1: Retrieve on demand  \\nPrompt +  \\n11 of 50 state namesRelevant\\nStep 2: Generate segment in parallel \\ncome from persons.SupportedIrrelevantTexas is namedafter a Native American tribe. Step 3: Critique outputs and select best segmentorigins in a 16th-century novel Las Sergas de EsplandiÃ¡n. California's name has itsRelevantPartially\\nUS states got their names from a variety of sources. 11 of 50 states names are come from persons.    26 states are named after Native Americans, including Utah. \\nPrompt: Write an essay of your best summer vacation\\nPrompt: Write an essay of your best summer vacation\",\n",
              "  'question': 'Can you explain the process of Self-RAG and how it differs from conventional RAG?\\n\\n',\n",
              "  'answer': 'Self-RAG is a model that retrieves K documents on demand, processes them in parallel, evaluates their relevance, generates corresponding task outputs, and critiques its own output to choose the best one. In contrast, conventional RAG retrieves and processes multiple documents in parallel but does not have a self-critique step.',\n",
              "  'source_doc': 'Self-Rag_ Learning to Retrieve, Generate, and Critique Through Self-Reflection.pdf'},\n",
              " {'context': 'on demonstrations using supervised learning (SFT), and ï¬nally by training on comparison data using\\nPPO. Adding updates on the pretraining mix during PPO does not lead to large changes in labeler\\npreference. To illustrate the magnitude of our gains: when compared directly, 175B InstructGPT\\noutputs are preferred to GPT-3 outputs 85 \\x063% of the time, and preferred 71 \\x064% of the time to\\nfew-shot GPT-3.\\nWe also found that our results do not change signiï¬cantly when evaluated on prompts submitted to\\nGPT-3 models on the API (see Figure 3), though our PPO-ptx models perform slightly worse at larger\\nmodel sizes.\\nIn Figure 4 we show that labelers also rate InstructGPT outputs favorably along several more concrete\\naxes. Speciï¬cally, compared to GPT-3, InstructGPT outputs are more appropriate in the context of a\\ncustomer assistant, more often follow explicit constraints deï¬ned in the instruction (e.g. â€œWrite your\\nanswer in 2 paragraphs or less.â€), are less likely to fail to follow the correct instruction entirely, and\\nmake up facts (â€˜hallucinateâ€™) less often in closed-domain tasks. These results suggest that InstructGPT\\nmodels are more reliable and easier to control than GPT-3. Weâ€™ve found that our other metadata\\n11GPT GPT\\n(prompted)SFT PPO PPO-ptx00.250.500.75PrevalenceAttempts correct instruction\\nGPT GPT\\n(prompted)SFT PPO PPO-ptx00.10.20.30.40.5Follows explicit constraints\\nGPT GPT\\n(prompted)SFT PPO PPO-ptx00.20.4Hallucinations\\nGPT GPT\\n(prompted)SFT PPO PPO-ptx00.250.500.75Uses language appropriate\\nfor customer assistantFigure 4: Metadata results on the API distribution. Note that, due to dataset sizes, these results are\\ncollapsed across model sizes. See Appendix E.2 for analysis that includes model size. Compared\\nto GPT-3, the PPO models are more appropriate in the context of a customer assistant, are better at\\nfollowing explicit constraints in the instruction and attempting the correct instruction, and less likely',\n",
              "  'question': 'In comparison to GPT-3, what are the improvements in reliability and ease of control achieved by InstructGPT models through the use of supervised learning (SFT) and PPO training?\\n\\n',\n",
              "  'answer': 'InstructGPT models show significant improvements in reliability and ease of control compared to GPT-3. Specifically, InstructGPT models are more appropriate in the context of a customer assistant, better at following explicit constraints in the instruction, more likely to attempt the correct instruction, and less likely to hallucinate in closed-domain tasks. These improvements are achieved through the use of supervised learning (SFT) and PPO training.',\n",
              "  'source_doc': 'Training language models to follow instructions with human feedback.pdf'},\n",
              " {'context': 'candidate program Pson eachxi. This process produces much larger tracing and execution datasets\\nwith 17k new programs, which we refer to as MBPP-aug .\\nConceptually, we have augmented the dataset using a combination of tools already available to us,\\nnamely a) the neural model, and b) program execution via a Python interpreter. We ï¬ne-tune direct\\nexecution and scratchpad models on this new augmented dataset MBPP-aug, using the same process\\nas above.\\nThe â€œMBPP-augâ€ columns in Table 3 show the results of this experiment. While the direct execution\\napproach suffers a decrease in accuracy when trained on this additional data, the performance of the\\nscratchpad model is greatly improved; the model trained on the augmented data solves more than\\nthree times the number of tasks as the model trained on only the original MBPP programs. We also\\nnote that if we measure the raw correctness across samples, the model already achieves 26.8% exact\\ntrace match, which is surprisingly high.\\n5.3 S CRATCHPAD TRAINING MAKES GOOD USE OF LARGE DATASETS\\nIn this section, we examine whether collecting additional tracing data from human-written pro-\\ngrams further improves tracing performance. This will allow us to understand whether the tracing\\nprocedure here is likely to scale well when slightly out-of-distribution tracing data is added to the\\nï¬ne-tuning set. We experiment using two datasets:\\n7Table 3: Comparison of models ï¬ne-tuned on different data sets and evaluated on MBPP programs.\\nWe report â€œper-taskâ€ execution and tracing accuracies, which require all examples to be correctly\\nexecuted/traced. We additionally report â€œper-exampleâ€ accuracies, which correspond to the total\\nfraction of test examples which are executed/traced correctly across the dataset. We ï¬nd that training\\nscratchpad models on an dataset augmented with samples from the model signiï¬cantly improves\\nperformance for the scratchpad model, while it harms the direct execution model. Combining tracing',\n",
              "  'question': 'What is the effect of scratchpad training on the performance of the direct execution model when large datasets are used?\\n\\n',\n",
              "  'answer': 'The scratchpad model trained on the augmented dataset MBPP-aug solves more than three times the number of tasks as the model trained on only the original MBPP programs, achieving 26.8% exact trace match. However, when scratchpad training is combined with an out-of-distribution dataset, it improves the performance of the scratchpad model, while it harms the direct execution model.',\n",
              "  'source_doc': 'Show Your Work_  Scratchpads for Intermediate Computation with Language Models.pdf'},\n",
              " {'context': 'manufactured by [QA(\"Who manufactures Coca-Cola?\")]  \\nthe Coca-Cola Company. \\nInput:  x \\nOutput: Figure 3: An exemplary prompt P(x)used to generate\\nAPI calls for the question answering tool.\\nMitself on this dataset. Each of these steps is\\ndescribed in more detail below.\\nSampling API Calls For each API, we write a\\npromptP(x)that encourages the LM to anno-\\ntate an example x=x1;:::;x nwith API calls.\\nAn example of such a prompt for a question an-\\nswering tool is shown in Figure 3; all prompts\\nused are shown in Appendix A.2. Let pM(zn+1j\\nz1;:::;z n)be the probability that Massigns to\\ntokenzn+1as a continuation for the sequence\\nz1;:::;z n. We ï¬rst sample up to kcandidate posi-\\ntions for doing API calls by computing, for each\\ni2f1;:::;ng, the probability\\npi=pM(<API>jP(x);x1:i\\x001)\\nthatMassigns to starting an API call at position\\ni. Given a sampling threshold \\x1cs, we keep all po-\\nsitionsI=fijpi>\\x1csg; if there are more than k\\nsuch positions, we only keep the top k.\\nFor each position i2I, we then obtain up to m\\nAPI callsc1\\ni;:::;cm\\niby sampling from Mgiven the\\nsequence [P(x);x1;:::;x i\\x001;<API> ]as a preï¬x\\nand</API> as an end-of-sequence token.2\\n2We discard all examples where Mdoes not generate the\\n</API> token.Executing API Calls As a next step, we execute\\nall API calls generated by Mto obtain the corre-\\nsponding results. How this is done depends entirely\\non the API itself â€“ for example, it can involve call-\\ning another neural network, executing a Python\\nscript or using a retrieval system to perform search\\nover a large corpus. The response for each API call\\ncineeds to be a single text sequence ri.\\nFiltering API Calls Letibe the position of the\\nAPI callciin the sequence x=x1;:::;x n, and let\\nribe the response from the API. Further, given a\\nsequence (wiji2N)ofweights , let\\nLi(z) =\\x00nX\\nj=iwj\\x00i\\x01logpM(xjjz;x1:j\\x001)\\nbe the weighted cross entropy loss for Mover the\\ntokensxi;:::;x nif the model is preï¬xed with z.\\nWe compare two different instantiations of this loss:\\nL+\\ni=Li(e(ci;ri))\\nL\\x00',\n",
              "  'question': 'What is the process of generating API calls for a question answering tool using a language model?\\n\\n',\n",
              "  'answer': 'The process of generating API calls for a question answering tool using a language model involves several steps. First, a prompt is written that encourages the language model to annotate an example with API calls. Then, a sampling threshold is set, and k candidate positions for doing API calls are selected based on their probability of generating the API call token. Next, API calls are executed to obtain the corresponding results, which may involve calling another neural network, executing a Python script, or using a retrieval system. Finally, the weighted cross entropy loss is calculated for the language model over the tokens in the sequence if the model is prefixed with the API call token.',\n",
              "  'source_doc': 'Toolformer_ Language Models Can Teach Themselves to Use Tools.pdf'},\n",
              " {'context': 'LM samples 5 different plans, then votes 5 times to decide which plan is best. The majority choice is\\nused to consequently write the output passage with the same sample-vote procedure.\\nIO CoT ToT IO\\n+refineToT\\n+refine468\\n(a) GPT-4 coherency scores\\nCoT > ToT Similar ToT > CoT010203040\\n213841(b) Human coherency comparison\\nFigure 5: Creative Writing results.Method Success Rate (%)\\nLetter Word Game\\nIO 38.7 14 0\\nCoT 40.6 15.6 1\\nToT (ours) 78 60 20\\n+best state 82.4 67.5 35\\n-prune 65.4 41.5 5\\n-backtrack 54.6 20 5\\nTable 3: Mini Crosswords results.\\nit improves IO coherency score from 6.19 to 7.67, and ToT coherency score from 7.56 to 7.91. We\\nbelieve it could be thought of as a third approach to thought generation in the ToT framework, where\\nnew thoughts can arise from reï¬ning old thoughts instead of i.i.d. or sequentially generated.\\n4.3 Mini Crosswords\\nIn Game of 24 and Creative Writing, ToT is relatively shallow â€” at most 3 thought steps are needed\\nto reach the ï¬nal output. Here we explore 5\\x025mini crosswords as a harder search problem involving\\nnatural language. Again, the goal is not just to solve the task, as more general crosswords can be\\nreadily solved with specialized NLP pipelines [ 31] that leverages large-scale retrieval instead of LM.\\nRather, we aim to explore the limit of LM as a general problem solver that explores its own thoughts\\nand guides its own exploration with deliberate reasoning as heuristics.\\nTask Setup. We scrape data from GooBix, which contains 156 games of 5\\x025mini crosswords. As\\nwe observe adjacent games contain similar clues, we use 20 games with indices 1;6;\\x01\\x01\\x01;91;96for\\ntesting, and games 136;141;146;151;156for prompting. For each task, the input describes the 5\\nhorizontal clues and 5 vertical clues, and the output should be a board of 5\\x025 = 25 letters to solve\\nthe crosswords. For evaluation, we consider three levels of success: the portion of correct letters (25\\nper game), words (10 per game), and games.',\n",
              "  'question': 'How does the proposed refinement approach improve the coherency scores in the ToT framework for mini crosswords?\\n\\n',\n",
              "  'answer': 'The proposed refinement approach improves the coherency scores in the ToT framework for mini crosswords by allowing new thoughts to arise from refining old thoughts instead of i.i.d. or sequentially generated. This approach is demonstrated through the improvement in IO coherency score from 6.19 to 7.67, and ToT coherency score from 7.56 to 7.91.',\n",
              "  'source_doc': 'Tree of Thoughts_  Deliberate Problem Solving with Large Language Models.pdf'},\n",
              " {'context': 'et al., 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact\\nwith, ReAct outperforms vanilla action generation models while being competitive with chain-of-\\nthought reasoning ( CoT) (Wei et al., 2022). The best approach overall is a combination of ReAct\\nandCoT that allows for the use of both internal knowledge and externally obtained information\\nduring reasoning. On ALFWorld and WebShop, two or even one-shot ReAct prompting is able\\nto outperform imitation or reinforcement learning methods trained with 103\\x18105task instances,\\nwith an absolute improvement of 34% and 10% in success rates respectively. We also demonstrate\\nthe importance of sparse, versatile reasoning in decision making by showing consistent advantages\\nover controlled baselines with actions only. Besides general applicability and performance boost,\\nthe combination of reasoning and acting also contributes to model interpretability, trustworthiness,\\nand diagnosability across all domains, as humans can readily distinguish information from modelâ€™s\\ninternal knowledge versus external environments, as well as inspect reasoning traces to understand\\nthe decision basis of model actions.\\nTo summarize, our key contributions are the following: (1) we introduce ReAct , a novel prompt-\\nbased paradigm to synergize reasoning and acting in language models for general task solving; (2) we\\nperform extensive experiments across diverse benchmarks to showcase the advantage of ReAct in a\\nfew-shot learning setup over prior approaches that perform either reasoning or action generation in\\nisolation; (3) we present systematic ablations and analysis to understand the importance of acting in\\nreasoning tasks, and reasoning in interactive tasks; (4) we analyze the limitations of ReAct under the\\nprompting setup (i.e. limited support of reasoning and acting behaviors), and perform initial ï¬netuning\\nexperiments showing the potential of ReAct to improve with additional training data. Scaling up',\n",
              "  'question': 'What is the advantage of using ReAct over vanilla action generation models in language models for general task solving?\\n\\n',\n",
              "  'answer': 'ReAct outperforms vanilla action generation models while being competitive with chain-of-thought reasoning (CoT) (Wei et al., 2022). Additionally, combining ReAct with CoT allows for the use of both internal knowledge and externally obtained information during reasoning. On ALFWorld and WebShop, two or even one-shot ReAct prompting is able to outperform imitation or reinforcement learning methods trained with 103 task instances, with an absolute improvement of 34% and 10% in success rates respectively. Furthermore, the combination of reasoning and acting contributes to model interpretability, trustworthiness, and diagnosability across all domains.',\n",
              "  'source_doc': 'ReAct_ Synergizing Reasoning and Acting in Language Models.pdf'},\n",
              " {'context': 'size the updates to the weights also have a low â€œintrinsic rankâ€ during adaptation. For a pre-trained\\nweight matrix W02Rd\\x02k, we constrain its update by representing the latter with a low-rank de-\\ncomposition W0+ \\x01W=W0+BA, whereB2Rd\\x02r;A2Rr\\x02k, and the rank r\\x1cmin(d;k).\\nDuring training, W0is frozen and does not receive gradient updates, while AandBcontain trainable\\nparameters. Note both W0and\\x01W=BAare multiplied with the same input, and their respective\\noutput vectors are summed coordinate-wise. For h=W0x, our modiï¬ed forward pass yields:\\nh=W0x+ \\x01Wx=W0x+BAx (3)\\nWe illustrate our reparametrization in Figure 1. We use a random Gaussian initialization for Aand\\nzero forB, so\\x01W=BAis zero at the beginning of training. We then scale \\x01Wx by\\x0b\\nr, where\\x0b\\nis a constant in r. When optimizing with Adam, tuning \\x0bis roughly the same as tuning the learning\\nrate if we scale the initialization appropriately. As a result, we simply set \\x0bto the ï¬rstrwe try\\nand do not tune it. This scaling helps to reduce the need to retune hyperparameters when we vary\\nr(Yang & Hu, 2021).\\nA Generalization of Full Fine-tuning. A more general form of ï¬ne-tuning allows the training of\\na subset of the pre-trained parameters. LoRA takes a step further and does not require the accumu-\\nlated gradient update to weight matrices to have full-rank during adaptation. This means that when\\napplying LoRA to all weight matrices and training all biases2, we roughly recover the expressive-\\nness of full ï¬ne-tuning by setting the LoRA rank rto the rank of the pre-trained weight matrices. In\\nother words, as we increase the number of trainable parameters3, training LoRA roughly converges\\nto training the original model, while adapter-based methods converges to an MLP and preï¬x-based\\nmethods to a model that cannot take long input sequences.\\nNo Additional Inference Latency. When deployed in production, we can explicitly compute and\\nstoreW=W0+BA and perform inference as usual. Note that both W0andBA are inRd\\x02k.',\n",
              "  'question': 'What is the difference between LoRA and full fine-tuning in terms of the optimization process and the expressiveness of the resulting models?\\n\\n',\n",
              "  'answer': 'LoRA and full fine-tuning differ in their optimization process and the expressiveness of the resulting models. Full fine-tuning requires the accumulated gradient update to weight matrices to have full-rank during adaptation, while LoRA does not. LoRA does not require the accumulated gradient update to weight matrices to have full-rank during adaptation, and it allows the training of a subset of the pre-trained parameters. As a result, LoRA roughly recovers the expressiveness of full fine-tuning by setting the LoRA rank r to the rank of the pre-trained weight matrices. In other words, as we increase the number of trainable parameters, training LoRA roughly converges to training the original model, while adapter-based methods converge to an MLP and prefect-based methods to a model that cannot take long input sequences.',\n",
              "  'source_doc': 'LoRA_ Low-Rank Adaptation of Large Language Models.pdf'},\n",
              " {'context': 'into a new era within the industry. Toyota will be taking a significant step forward\\nmoving forward with the program, along with all of the veteran EV experts who will be\\non board. Toyota will invest in another budget program with Nissan, and will also\\nspend close to 2 percent more in this effort. Late last week, Toyota said the automaker\\nis seeking federal money to launch and develop a Phase I funding program to support\\nthe Toyota SSP. While many people are skeptical of any measure<BKSPC> possible future\\npartnerships, Toyota was confident that the group will be determined to get the program\\noff the ground by 2020, and that a program would be here before too long. Toyotaâ€™s\\ncurrent projections indicate it will acquire Mitsubishi A-brand engines to support its\\nNo. 2 seat car, which will supply approximately 220,000 electric vehicles in 2020. The\\ncompany is expected to have expanded the name in 2017, by launching a new hydrogen fuel\\ncell vehicle between 2021 and 2023. Ralph Hisefner as CEO New initiatives on Toyotaâ€™s\\nLevel 3 Vision Vision Future Systems Competition have been revealed today, but they are\\nbeing designed to launch new development capabilities in this rapidly approaching market.\\nBy doing so, Toyota will be taking a step closer to the promise of wide-mileage electric\\nvehicles, a goal already put to the â€™Pim Super Super 500 Hybridâ€™ and the Toyota Vision\\nFuture technologies for the Toyota platform. Hyundai Tribute Pack Member (subsequently\\nname changed): BMW i3 BMW i3 Gran Coupe SSI Kortis Accordingly, Toyota is assembling\\nthe brand S6, and with that said, even the really big crossover may be made by this new\\nentrant. As stated in a Japanese blog: The KC-BY is forecast to surpass the 2012 â€™Konami\\nHDâ€™ as the global hybrid vehicle market continues to grow. Toyota aims for the 2020\\nmodel to be able to extend global road segment recognition and hybrid vehicle capabilities\\nto the top tier of the vehicle. The new vehicle that utilizes self-driving technology',\n",
              "  'question': 'What measures is Toyota taking to support the development of electric vehicles and autonomous driving technology?\\n\\n',\n",
              "  'answer': 'Toyota is taking several measures to support the development of electric vehicles and autonomous driving technology. First, the company is investing in a Phase I funding program to support the Toyota SSP, with the aim of launching it by 2020. Toyota is also investing in another budget program with Nissan and plans to acquire Mitsubishi A-brand engines to support its No. 2 seat car, which will supply approximately 220,000 electric vehicles in 2020. Additionally, Toyota is assembling the brand S6, which may be capable of producing a really big crossover using self-driving technology. The company aims for the 2020 model to be able to extend global road segment recognition and hybrid vehicle capabilities to the top tier of the vehicle.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'resentative results and omit data that does not bring relevant\\ninsights (e.g., CoT-SC).\\n7.1 Evaluation Methodology\\nWe use 100 input samples for each task and comparison\\nbaseline. We set temperature to be 1.0 and we use 4k con-\\ntext unless stated otherwise. For each experiment, we fix the\\nnumbers of thoughts in respective schemes to achieve simi-\\nlar costs in each experiment.\\nParameters We experiment extensively with the branching\\nfactor kand the number of levels Lto ensure that we com-\\npare GoT to cost-effective and advantageous configurations.\\nWe plot two variants of ToT: one with higher kand lower\\ndepth (ToT), the other with lower kbut higher L(ToT2).\\nWe usually aim to achieve a sweetspot in the tradeoff be-\\ntween sparser generation rounds (lower k) vs. more rounds\\n(larger L). Usually more responses per round is more expen-\\nsive (e.g., 80 vs. 60 total responses for Figure 7 but $6 vs. $3\\ncosts). We also try different problem sizes P(e.g., in sorting,\\nPstates how many numbers are to be sorted).\\nUsed LLMs Due to budget restrictions, we focus on GPT-\\n3.5, using GPT-4. We also experimented with Llama-2, but\\nit was usually worse than GPT-3.5 and also much slower to\\nrun, making it infeasible to obtain enough samples.IOCoT ToTToT2 GoT0246810121416#incorrectly sorted elements; the lower the better\\n32 elements\\n0.00.20.40.60.81.01.21.41.6\\nIOCoT ToTToT2 GoT0481216202428323640444852566064\\n64 elements\\n0.00.30.60.91.21.51.82.12.42.73.03.33.63.94.24.54.8\\nIOCoT ToTToT2 GoT081624324048566472808896104112120128\\n128 elements\\n012345678910111213141516\\nTotal Cost ($); the lower the betterL=2\\nk=20\\nL=3\\nk=10GoT: Figure 4\\nclipped\\nL=4\\nk=20L=7\\nk=10GoT: Figure 4\\nclipped\\nL=4\\nk=20\\nL=10\\nk=10GoT:\\nFigure 4Figure 5: Number of errors and cost in sorting tasks with ChatGPT-3.5. Landkindicate the structure of ToT (see Sections 3.2\\nand 6).\\n7.2 Analysis of GoTâ€™s Advantages\\nThe results of analysis are in Figure 5 (sorting), 6 (set inter-\\nsection), 7 (keyword counting), and 8 (document merging);',\n",
              "  'question': 'How does the tradeoff between sparser generation rounds (lower k) and more rounds (larger L) affect the cost and accuracy of GoT in various tasks?\\n\\n',\n",
              "  'answer': 'The tradeoff between sparser generation rounds (lower k) and more rounds (larger L) affects the cost and accuracy of GoT in various tasks. Generally, more responses per round is more expensive, but achieving a sweetspot in the tradeoff between sparser generation rounds and more rounds can lead to cost-effective and advantageous configurations. The specific effects on cost and accuracy depend on the task and problem size.',\n",
              "  'source_doc': 'Graph of Thoughts_ Solving Elaborate Problems with Large Language Models.pdf'},\n",
              " {'context': 'Input Tokens: \\n<begin-sentence>    Apples    are      blue        are     green \\nTargets: \\n     Apples          are      blue  <backspace>    green   N/A \\nPosition IDs: \\n        0             1        2        3           2       3 Logical Sequence Trajectory Figure 2: Attention masks, position IDs, inputs and labels for states and actions. Transforming states\\nand actions to single-pass inputs allows parallel computation of logits for all (state, action) pairs.\\nProof. The full proof is given in detail in the appendix. As a sketch, the first equality holds from\\nthe previous section. The second is obtained by replacing rwithTÎ¸Qand verifying that the two\\noptimization problems are equal. The third line is via a telescoping sum argument first described\\nin [18]. In the fourth line we replace Ïˆ(r)with a simpler regularizer Es,aâˆ¼Ïdata[g(r(s, a))], where\\ng(r) =râˆ’Ï•(r)ifrâˆˆâ„¦, and infinity otherwise. In the fifth line we expand the telescoping sum in a\\ndifferent way, allowing us to incorporate samples from any policy. In the final line we parameterize\\nthe policy from the Q-values, setting logpQ(a|s) =Q(s, a)âˆ’logP\\naâ€²âˆˆAexpQ(s, aâ€²). We then\\nshow that the optimization problem over (Q, pQ)has the same optimum as the optimization over\\nQ, Î¸, allowing us to eliminate Î¸from the optimization entirely.\\nBecause the Q-value directly gives the logits of the optimal policy via logpQ(a|s) =Q(s, a)âˆ’\\nlogP\\naâ€²âˆˆAexpQ(s, aâ€²), we relabel Qasâ„“Î¸to make the connection to logits more clear. This results\\nin the fully supervised objective over the logits of a policy â„“Î¸\\nJ(â„“Î¸) =1\\nÎ±Es,a,sâ€²âˆ¼Ïdata[Ï•(Î±â„“Î¸(a|s)âˆ’Î±Î³V(sâ€²)]âˆ’1\\n2Es,sâ€²âˆ¼Ïdata[V(s)âˆ’Î³V(sâ€²)] (6)\\nâˆ’1\\n2Es,sâ€²âˆ¼ÏÎ¸[V(s)âˆ’Î³V(sâ€²)],\\nwhere (s, a, sâ€²)âˆ¼Ïcorresponds to sampling s, afromÏandsâ€²fromP(Â·|s, a). The value function is\\nV(s) = logP\\naâ€²âˆˆAexpâ„“Î¸(aâ€²|s).\\nMinimizing this objective is the same as solving minÎ¸dÏˆ(ÏÎ¸, Ïdata)âˆ’Î±H[ÏÎ¸], where dÏˆ(P, Q) =\\nsuprâˆˆâ„¦Exâˆ¼P[Ï•(r(x))]âˆ’Exâˆ¼Q[r(x)]. By choosing â„¦andÏ•, we can recover f-divergences by',\n",
              "  'question': 'Given the proof of the objective function for supervised learning of a policy in reinforcement learning, what is the fully supervised objective over the logits of a policy?\\n\\n',\n",
              "  'answer': 'The fully supervised objective over the logits of a policy is given by equation (6):\\n\\nJ(â„“Î¸) = 1 / Î±Es,a,sâ€² â‰ˆ Ïdata[Ï•(Î±â„“Î¸(a|s) - Î±Î³V(sâ€²))] - 1\\n2Es,sâ€² â‰ˆ Ïdata[V(s) - Î³V(sâ€²)]',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'tecture for implementing GoT ( contribution #2 ), coming\\nwith two design highlights. First, we enable a fine-grained\\ncontrol over individual thoughts . This enables us to fully\\ncontrol the ongoing conversation with the LLM, and apply\\nadvanced thought transformations, such as combining most\\npromising thoughts from the ongoing reasoning into a new\\none. Second, we ensure that our architecture can be seam-\\nlessly extended with novel thought transformations, patterns\\nof reasoning (i.e., graphs of thoughts), and LLM models.\\nThis enables rapid prototyping of novel prompting ideas us-\\ning GoT, while experimenting with different models such as\\nGPT-3.5, GPT-4, or Llama-2 [63].\\nWe illustrate several use cases for GoT (sorting, keyword\\ncounting for summaries, set operations, document merging)\\nand we detail how to implement them using the graph-based\\nparadigm ( contribution #3 ). We evaluate GoT and show its\\nadvantages over the state of the art ( contribution #4 ). Over-\\nall, we observe that GoT is particularly well-suited for tasks\\nthat can be naturally decomposed into smaller subtasks that\\nare solved individually and then merged for a final solution.\\nHere, GoT outperforms other schemes, for example improv-\\ning upon CoT and ToT by, respectively, â‰ˆ70% and â‰ˆ62%,\\nin terms of the quality of sorting, while simultaneously re-\\nducing costs by >31% over ToT.\\nWe qualitatively compare GoT to other prompting\\nschemes in Table 1. GoT is the only one to enable arbitrary\\ngraph-based thought transformations within a prompt, such\\nas aggregation, embracing all previously proposed schemes.\\nScheme Sc? Mc? Tr? Ag?\\nChain-of-Thought (CoT) [70] /reve /reve /reve\\nSelf-Consistency with CoT [66] /reve /reve\\nThought decomposition [74] /reve\\nTree-of-Thought (ToT) [43] /reve\\nTree of Thoughts (ToT) [76] /reve\\nGraph of Thoughts (GoT) \\nTable 1: Comparison of prompting schemes, with re-\\nspect to the supported transformations of thoughts. â€œSc?â€ :\\nsingle chain of thoughts? â€œMc?â€ : multiple chains of',\n",
              "  'question': 'In GoT, how does the graph-based paradigm enable the implementation of fine-grained control over individual thoughts and seamless extension with novel thought transformations, patterns of reasoning, and LLM models?\\n\\n',\n",
              "  'answer': 'The graph-based paradigm in GoT enables fine-grained control over individual thoughts by representing thoughts as nodes in a graph and the relationships between them as edges. This allows for advanced thought transformations, such as combining most promising thoughts from the ongoing reasoning into a new one. Additionally, the graph-based paradigm enables seamless extension with novel thought transformations, patterns of reasoning, and LLM models by allowing for the addition of new nodes and edges to the graph. This enables rapid prototyping of novel prompting ideas using GoT, while experimenting with different models such as GPT-3.5, GPT-4, or Llama-2.',\n",
              "  'source_doc': 'Graph of Thoughts_ Solving Elaborate Problems with Large Language Models.pdf'},\n",
              " {'context': 'problem-space and guide the problem-solver towards a solution. This perspective highlights two key\\nshortcomings of existing approaches that use LMs to solve general problems: 1) Locally, they do not\\nexplore different continuations within a thought process â€“ the branches of the tree. 2) Globally, they\\ndo not incorporate any type of planning, lookahead, or backtracking to help evaluate these different\\noptions â€“ the kind of heuristic-guided search that seems characteristic of human problem-solving.\\nTo address these shortcomings, we introduce Tree of Thoughts (ToT) , a paradigm that allows LMs to\\nexplore multiple reasoning paths over thoughts (Figure 1(c)). ToT frames any problem as a search\\nover a tree, where each node is a states= [x;z1\\x01\\x01\\x01i]representing a partial solution with the input and\\nthe sequence of thoughts so far. A speciï¬c instantiation of ToT involves answering four questions:\\n1. How to decompose the intermediate process into thought steps; 2. How to generate potential\\nthoughts from each state; 3. How to heuristically evaluate states; 4. What search algorithm to use.\\n1. Thought decomposition. While CoT samples thoughts coherently without explicit decomposition,\\nToT leverages problem properties to design and decompose intermediate thought steps. As Table 1\\nshows, depending on different problems, a thought could be a couple of words (Crosswords), a line of\\nequation (Game of 24), or a whole paragraph of writing plan (Creative Writing). In general, a thought\\nshould be â€œsmallâ€ enough so that LMs can generate promising and diverse samples (e.g. generating\\na whole book is usually too â€œbigâ€ to be coherent), yet â€œbigâ€ enough so that LMs can evaluate its\\nprospect toward problem solving (e.g. generating one token is usually too â€œsmallâ€ to evaluate).\\n2. Thought generator G(p\\x12;s;k).Given a tree state s= [x;z1\\x01\\x01\\x01i], we consider two strategies to\\ngeneratekcandidates for the next thought step:\\n(a)Sample i.i.d. thoughts from a CoT prompt (Creative Writing, Figure 4): z(j)\\x18',\n",
              "  'question': 'What is the purpose of Tree of Thoughts (ToT) in problem-solving?\\n\\n',\n",
              "  'answer': \"The purpose of Tree of Thoughts (ToT) in problem-solving is to allow LMs to explore multiple reasoning paths over thoughts, addressing the shortcomings of existing approaches that use LMs to solve general problems. ToT frames any problem as a search over a tree, where each node represents a partial solution with the input and the sequence of thoughts so far. ToT involves answering four questions: thought decomposition, thought generator, heuristic evaluation, and search algorithm. Thought decomposition involves decomposing intermediate thought steps based on problem properties, ensuring that LMs can generate promising and diverse samples while also evaluating their prospects toward problem solving. Thought generator involves generating candidates for the next thought step by sampling i.i.d. thoughts from a CoT prompt or by using other strategies. Heuristic evaluation involves evaluating states based on the problem's requirements. Finally, the search algorithm determines the most promising path to take based on the heuristic evaluation.\",\n",
              "  'source_doc': 'Tree of Thoughts_  Deliberate Problem Solving with Large Language Models.pdf'},\n",
              " {'context': 'hypothesized that it would be beneï¬cial to ï¬ne-tune from\\nthe GPT-3 (Brown et al., 2020) model family, which already\\ncontains strong natural language representations. Surpris-\\ningly, we did not observe improvements when starting from\\na pre-trained language model, possibly because the ï¬ne-\\ntuning dataset is so large. Nevertheless, models ï¬ne-tuned\\nfrom GPT converge more quickly, so we apply this strategy\\nfor all subsequent experiments.\\nWe train Codex using the same learning rate as the corre-\\nsponding GPT model, with a 175 step linear warmup and\\ncosine learning rate decay. We train for a total of 100 billion\\ntokens, using the Adam optimizer with \\x0c1= 0:9,\\x0c2= 0:95,\\n\\x0f= 10\\x008, and a weight decay coefï¬cient of 0:1.\\nIn order to maximally leverage text representations from\\nGPT, we base our code lexer on the GPT-3 text tokenizer.\\nSince the distribution of words in GitHub code differs from\\nthat of natural text, this tokenizer is not very effective for\\nrepresenting code. The largest source of inefï¬ciency arises\\nfrom encoding whitespace, so we add an additional set of\\ntokens for representing whitespace runs of different lengths.\\nThis allows us to represent code using approximately 30%\\nfewer tokens.\\nTo compute pass@ k, we assemble each HumanEval prob-\\nlem into a prompt consisting of a header, a signature, and\\na docstring, which is illustrated in Figure 2. We sample\\ntokens from Codex until we encounter one of the following\\nstop sequences: â€˜nnclass â€™, â€˜nndef â€™, â€˜nn#â€™, â€˜nnifâ€™, or\\nâ€˜nnprint â€™, since the model will continue generating addi-\\ntional functions or statements otherwise. We use nucleus\\nsampling (Holtzman et al., 2020) with top p= 0:95for all\\nsampling evaluation in this work.\\n3.3. Results\\nIn Figure 4, we plot test loss on a held-out validation set\\nagainst Codex model size. We ï¬nd that just as languageEvaluating Large Language Models Trained on Code\\nFigure 4. Model cross-entropy test loss measured on a held-out\\nsplit of our Python GitHub code corpus. The smooth power law',\n",
              "  'question': 'Given a pre-trained language model and a large fine-tuning dataset, why would fine-tuning from a GPT model family not result in improvements compared to starting from a pre-trained language model?\\n\\n',\n",
              "  'answer': 'It is possible that the large fine-tuning dataset may not allow for effective fine-tuning when starting from a pre-trained language model, as the model may not be able to effectively capture the relevant information in the dataset. Additionally, the distribution of words in GitHub code may differ from that of natural text, making it difficult for the language model to effectively represent code. Fine-tuning from a GPT model family, on the other hand, may allow for more effective utilization of text representations from the GPT model.',\n",
              "  'source_doc': 'Evaluating Large Language Models Trained on Code.pdf'},\n",
              " {'context': '2\\n3CoT = dspy.ChainOfThought(\"question -> answer\") # GSM8K Program â€˜CoTâ€˜\\n1class ThoughtReflection(dspy.Module):\\n2def __init__(self, num_attempts):\\n3 self.predict = dspy.ChainOfThought(\"question -> answer\", n=num_attempts)\\n4 self.compare = dspy.MultiChainComparison(â€™question -> answerâ€™, M=num_attempts)\\n5\\n6def forward(self, question):\\n7 completions = self.predict(question=question).completions\\n8 return self.compare(question=question, completions=completions)\\n9\\n10reflection = ThoughtReflection(num_attempts=5) # GSM8K Program â€˜reflectionâ€˜\\nInreflection , five reasoning chains are sampled from the LM (alongside their answers) and they\\nare compared in parallel by a built-in MultiChainComparison module, which generalizes Yoran\\net al. (2023). This generates a new answer taking into account the patterns from the five attempts.\\nCritically, the modules used are all generic, none is specific math problems or particular LM.\\nCompiling As we discussed in Section 4, DSPy programs can be compiled into new, optimized\\nprograms. In our experiments, we evaluate the programs zero-shot (no compiling) as well as a\\nnumber of strategies for compiling. Our simplest compiler is LabeledFewShot :\\n1fewshot = dspy.LabeledFewShot(k=8).compile(program, trainset=trainset)\\nHere,program can be any DSPy module. This simply samples k=8random demonstrations from the\\ntrainset for the fields common to the training examples and the signature(s), in this case, question\\nandanswer , but not the reasoning for instance. We report the average of 3â€“5 runs (depending on the\\nsetting) when applying such random sampling.\\n8Preprint\\nNext, we also consider bootstrapping few-shot examples with random search:\\n1tp = BootstrapFewShotWithRandomSearch(metric=gsm8k_accuracy)\\n2bootstrap = tp.compile(program, trainset=trainset, valset=devset)\\nThis will generate demonstration chains for examples in the training set and optimize the selection',\n",
              "  'question': 'Can DSPy programs be compiled into optimized programs for zero-shot evaluation or other strategies for compiling?\\n\\n',\n",
              "  'answer': 'Yes, DSPy programs can be compiled into optimized programs using LabeledFewShot or BootstrapFewShotWithRandomSearch strategies for zero-shot evaluation or other optimization methods. The LabeledFewShot strategy samples k=8 random demonstrations from the trainset for the fields common to the training examples and the signature(s), such as question and answer, but not the reasoning for instance. The BootstrapFewShotWithRandomSearch strategy generates demonstration chains for examples in the training set and optimizes the selection using random search.',\n",
              "  'source_doc': 'DSPy_  Compiling Declarative Language Model Calls into Self-Improving Pipelines.pdf'},\n",
              " {'context': 'RealToxicityPrompts. A total of 1,729 prompts were labeled for three different 175B models, both\\nwith and without \"respectful\" instructions. The automatic evaluations shown here are calculated\\nover the same set of prompts as the human evaluations, and thus differ slightly from the full set of\\nevaluations recorded in Table 14 in Appendix D.\\nstandard evaluation procedure for this dataset, and we also send these samples to labelers to obtain\\nratings on absolute toxicity, toxicity relative to the prompt, continuity, and overall output preference.\\nWe sample prompts from this dataset uniformly according to prompt toxicity to better assess how our\\nmodels perform with high input toxicity (see Figure 39 in Appendix E); this differs from the standard\\nprompt sampling for this dataset, and thus our absolute toxicity numbers are inï¬‚ated.\\nOur results are in Figure 7. We ï¬nd that, when instructed to produce a safe and respectful output\\n(â€œrespectful promptâ€), InstructGPT models generate less toxic outputs than those from GPT-3\\naccording to the Perspective API. This advantage disappears when the respectful prompt is removed\\n(â€œno promptâ€). Interestingly, when explicitly prompted to produce a toxic output, InstructGPT outputs\\nare much more toxic than those from GPT-3 (see Figure 39).\\nThese results are conï¬rmed in our human evaluations: InstructGPT is less toxic than GPT-3 in the\\nâ€œrespectful promptâ€ setting, but performs similarly in the â€œno promptâ€ setting. We provide extended\\nresults in Appendix E. To summarize: all of our models are rated as less toxic than expected given\\nthe prompt (they get a negative score on a scale from -1 to 1, where 0 is â€˜about as toxic as expectedâ€™).\\nOur SFT baseline is the least toxic out of all of our models, but also has the lowest continuity and\\nis the least preferred in our rankings, which could indicate that the model generates very short or\\ndegenerate responses.',\n",
              "  'question': 'To what extent do the models generate less toxic outputs when instructed to produce a safe and respectful output?\\n\\n',\n",
              "  'answer': 'InstructGPT models generate less toxic outputs than those from GPT-3 when instructed to produce a safe and respectful output (\"respectful prompt\"). This advantage disappears when the respectful prompt is removed (\"no prompt\").',\n",
              "  'source_doc': 'Training language models to follow instructions with human feedback.pdf'},\n",
              " {'context': 'SequenceMatch: Imitation Learning for\\nAutoregressive Sequence Modelling with Backtracking\\nChris Cundy1Stefano Ermon1\\n1Department of Computer Science, Stanford University\\n{cundy, ermon }@cs.stanford.edu\\nAbstract\\nIn many domains, autoregressive models can attain high likelihood on the task\\nof predicting the next observation. However, this maximum-likelihood (MLE)\\nobjective does not necessarily match a downstream use-case of autoregressively\\ngenerating high-quality sequences. The MLE objective weights sequences propor-\\ntionally to their frequency under the data distribution, with no guidance for the\\nmodelâ€™s behaviour out of distribution (OOD): leading to compounding error during\\nautoregressive generation. In order to address this compounding error problem, we\\nformulate sequence generation as an imitation learning (IL) problem. This allows\\nus to minimize a variety of divergences between the distribution of sequences\\ngenerated by an autoregressive model and sequences from a dataset, including\\ndivergences with weight on OOD generated sequences. The IL framework also\\nallows us to incorporate backtracking by introducing a backspace action into\\nthe generation process. This further mitigates the compounding error problem by\\nallowing the model to revert a sampled token if it takes the sequence OOD. Our\\nresulting method, SequenceMatch, can be implemented without adversarial training\\nor architectural changes. We identify the SequenceMatch- Ï‡2divergence as a more\\nsuitable training objective for autoregressive models which are used for generation.\\nWe show that empirically, SequenceMatch training leads to improvements over\\nMLE on text generation with language models.\\n1 Introduction\\nAutoregressive models such as the GPT series of causally masked transformers [ 7,25] are able to\\nperform a variety of downstream tasks such as question-answering, translation, and summarization,\\nafter simply training on a large corpus of text with the objective of predicting the next token given',\n",
              "  'question': 'How does the SequenceMatch method address the compounding error problem in autoregressive sequence modeling with backtracking?\\n\\n',\n",
              "  'answer': 'The SequenceMatch method addresses the compounding error problem in autoregressive sequence modeling with backtracking by formulating sequence generation as an imitation learning (IL) problem. This allows the model to minimize a variety of divergences between the distribution of sequences generated by an autoregressive model and sequences from a dataset, including divergences with weight on out-of-distribution (OOD) generated sequences. The IL framework also allows the model to incorporate backtracking by introducing a backspace action into the generation process. This further mitigates the compounding error problem by allowing the model to revert a sampled token if it takes the sequence OOD. The resulting method, SequenceMatch, can be implemented without adversarial training or architectural changes, and the SequenceMatch-Ï‡2divergence is identified as a more suitable training objective for autoregressive models used for generation.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'SST-2 to range from near chance (54.3%) to near state of\\nthe art (93.4%) (Zhao et al., 2021). In this ï¬nal subsec-\\ntion, we evaluate robustness to chains of thought written\\nby different annotators. In addition to the results above,\\nwhich used chains of thought written by an Annotator\\nA, two other co-authors of this paper (Annotators B and\\nC) independently wrote chains of thought for the same\\nfew-shot exemplars (shown in Appendix H). Annotator A\\nalso wrote another chain of thought that was more concise\\nthan the original, following the style of solutions given in\\nCobbe et al. (2021).1\\nFigure 6 shows these results for LaMDA 137B on GSM8K\\nand MAWPS (ablation results for other datasets are given\\nin Appendix Table 6 / Table 7). Although there is variance\\namong different chain of thought annotations, as would be\\nexpected when using exemplar-based prompting (Le Scao\\nand Rush, 2021; Reynolds and McDonell, 2021; Zhao\\net al., 2021), all sets of chain of thought prompts outper-\\nform the standard baseline by a large margin. This result\\nimplies that successful use of chain of thought does not\\ndepend on a particular linguistic style.\\nTo conï¬rm that successful chain-of-thought prompting\\nworks for other sets of exemplars, we also run experiments\\nwith three sets of eight exemplars randomly sampled from the GSM8K training set, an independent\\n1For instance, whereas original chain of thought uses several short sentences ( â€œâ€™There were originally 9\\ncomputers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is\\n29. â€), the concise chain of thought would read â€œ5 * 4 = 20 new computers were added. So there are 9 + 20 = 29\\nnew computers in the server room nowâ€ .\\n6source (examples in this dataset already included reasoning steps like a chain of thought).2Fig-\\nure 6 shows that these prompts performed comparably with our manually written exemplars, also\\nsubstantially outperforming standard prompting.',\n",
              "  'question': 'To what extent does the performance of LaMDA 137B on GSM8K and MAWPS vary when using different chain of thought annotations?\\n\\n',\n",
              "  'answer': 'The performance of LaMDA 137B on GSM8K and MAWPS varies when using different chain of thought annotations, with all sets of chain of thought prompts outperforming the standard baseline by a large margin. Additionally, experiments with three sets of eight exemplars randomly sampled from the GSM8K training set also showed that these prompts performed comparably with manually written exemplars and substantially outperformed standard prompting.',\n",
              "  'source_doc': 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.pdf'},\n",
              " {'context': 'ture, or algorithmic execution. When working on a novel\\nidea, a human would not only follow a chain of thoughts\\n(as in CoT) or try different separate ones (as in ToT), but\\nwould actually form a more complex network of thoughts.\\nFor example, one could explore a certain chain of reason-\\ning, backtrack and start a new one, then realize that a cer-\\ntain idea from the previous chain could be combined with\\nthe currently explored one, and merge them both into a new\\nsolution, taking advantage of their strengths and eliminat-\\ning their weaknesses. Similarly, brains form complex net-\\nworks, with graph-like patterns such as recurrence [28]. Ex-\\necuting algorithms also expose networked patterns, often\\nrepresented by Directed Acyclic Graphs. The correspond-\\ninggraph-enabled transformations bring a promise of more\\npowerful prompting when applied to LLM thoughts, but they\\nare not naturally expressible with CoT or ToT.\\nWe observe that these (and many other) thought trans-\\nformations can be naturally enabled when modeling a rea-\\nsoning process of an LLM as a graph . For this, we pro-\\npose Graph of Thoughts (GoT), an approach that en-\\nhances LLMsâ€™ capabilities through networked reasoning\\n(contribution #1 ). In GoT, an LLM thought is modeled\\nas a vertex, while an edge is a dependency between such\\nthoughts. Using GoT, one can aggregate arbitrary thoughts\\nby constructing vertices that have more than one incom-\\ning edge. Overall, the graph abstraction harnessed by GoT\\nseamlessly generalizes CoT and ToT to more complex\\nthought patterns, without resorting to any model updates .\\nYet, putting GoT to practice requires solving several de-\\nsign challenges. For example, what is the best graph struc-\\nture for different tasks? How to best aggregate thoughts to\\nmaximize accuracy and minimize cost? To answer these andarXiv:2308.09687v2  [cs.CL]  21 Aug 2023many other questions, we carefully design a modular archi-\\ntecture for implementing GoT ( contribution #2 ), coming',\n",
              "  'question': \"In what ways does the Graph of Thoughts (GoT) approach enhance LLMs' capabilities through networked reasoning?\\n\\n\",\n",
              "  'answer': \"The Graph of Thoughts (GoT) approach enhances LLMs' capabilities through networked reasoning by modeling an LLM thought as a vertex and an edge as a dependency between such thoughts. This allows for the aggregation of arbitrary thoughts by constructing vertices that have more than one incoming edge. The graph abstraction harnessed by GoT seamlessly generalizes CoT and ToT to more complex thought patterns without resorting to any model updates. However, implementing GoT requires solving several design challenges, such as determining the best graph structure for different tasks and minimizing cost while maximizing accuracy.\",\n",
              "  'source_doc': 'Graph of Thoughts_ Solving Elaborate Problems with Large Language Models.pdf'},\n",
              " {'context': 'language model rather than training a model from scratch. In the domain of program synthesis,\\nNye et al. (2021) leverage language models to predict the ï¬nal outputs of Python programs via\\nï¬rst line-to-line predicting the intermediate computational results, and show that their step-by-step\\nprediction method performs better than directly predicting the ï¬nal outputs.\\nNaturally, this paper also relates closely to the large body of recent work on prompting. Since the\\npopularization of few-shot prompting as given by Brown et al. (2020), several general approaches\\nhave improved the prompting ability of models, such as automatically learning prompts (Lester et al.,\\n2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang\\net al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\\ninstructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\\noutputs of language models with a chain of thought.\\n8 Conclusions\\nWe have explored chain-of-thought prompting as a simple and broadly applicable method for enhanc-\\ning reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense\\nreasoning, we ï¬nd that chain-of-thought reasoning is an emergent property of model scale that allows\\nsufï¬ciently large language models to perform reasoning tasks that otherwise have ï¬‚at scaling curves.\\nBroadening the range of reasoning tasks that language models can perform will hopefully inspire\\nfurther work on language-based approaches to reasoning.\\n9Acknowledgements\\nWe thank Jacob Devlin, Claire Cui, Andrew Dai, and Ellie Pavlick for providing feedback on the\\npaper. We thank Jacob Austin, Yuhuai Wu, Henryk Michalewski, Aitor Lewkowycz, Charles Sutton,\\nand Aakanksha Chowdhery for helpful discussions. We thank Sid Maxwell for notifying us about a\\nmistake in the manual error analysis in the original manuscript.',\n",
              "  'question': 'What is the difference between first line-to-line predicting and directly predicting the final outputs in program synthesis using language models?\\n\\n',\n",
              "  'answer': 'First line-to-line predicting involves predicting the intermediate computational results of a program, while directly predicting the final outputs involves predicting the output of the entire program at once. Nye et al. (2021) found that their step-by-step prediction method, which involves first line-to-line predicting, performs better than directly predicting the final outputs in program synthesis using language models.',\n",
              "  'source_doc': 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.pdf'},\n",
              " {'context': 'adding â€œLetâ€™s think step by stepâ€ to the zero-shot prompt. We find that the instruction fine-tuned\\nmodels tend to generate extraneous content when queried. This can especially be a problem for the\\nlist-based tasks. To deal with this we add an extra line to our prompt: â€œ List only the answers\\nseparated by a comma â€. We also add another layer of post-processing to extract the answers by\\nusing an off-the-shelf NER model to further avoid this issue as this helped. However, we still expect\\nfew-shot to improve over this, especially for tasks like Multi-Span-QA where the answers are not all\\nnamed entities, and the few-shot examples effectively show the domain of the task.\\nFor the longform generation of biographies we also compare to several existing model results reported\\nin Min et al. (2023), in particular InstructGPT (Ouyang et al., 2022), ChatGPT2and PerplexityAI3.\\n4.3 R ESULTS\\nWe are interested in empirically answering the following research questions:\\nRQ1 Can C OVEeffectively reduce the rate of hallucinatory content produced by the LLM?\\nRQ2 Can COVEbe used to fix or remove incorrect generations without decreasing the amount of\\ncorrect content?\\n2https://openai.com/blog/chatgpt\\n3www.perplexity.ai\\n6LLM Method F1 ( â†‘) Prec. Rec.\\nLlama 2 70B Chat Zero-shot 0.20 0.13 0.40\\nLlama 2 70B Chat CoT 0.17 0.11 0.37\\nLlama 65B Few-shot 0.39 0.40 0.38\\nLlama 65B CoVe (joint) 0.46 0.50 0.42\\nLlama 65B CoVe (factored) 0.48 0.50 0.46\\nTable 2: Closed book MultiSpanQA test performance, comparing CoVe with various baselines.\\nLLM Method F ACTSCORE . (â†‘) Avg. # facts\\nInstructGPTâˆ—Zero-shot 41.1 26.3\\nChatGPTâˆ—Zero-shot 58.7 34.7\\nPerplexityAIâˆ—Retrieval-based 61.6 40.8\\nLlama 2 70B Chat Zero-shot 41.3 64.9\\nLlama 2 70B Chat CoT 41.1 49.0\\nLlama 65B Few-shot 55.9 16.6\\nLlama 65B CoVe (joint) 60.8 12.8\\nLlama 65B CoVe (factored) 63.7 11.7\\nLlama 65B CoVe (factor+revise) 71.4 12.3\\nTable 3: Longform generation of biographies with metrics defined from Min et al. (2023). Models',\n",
              "  'question': 'Does C OVE effectively reduce the rate of hallucinatory content produced by LLMs?\\n\\n',\n",
              "  'answer': 'Yes, C OVE effectively reduces the rate of hallucinatory content produced by LLMs. When compared to the zero-shot and few-shot baselines, the Llama 65B CoVe (joint) and Llama 65B CoVe (factored) models produced significantly less hallucinatory content. Additionally, the Llama 65B CoVe (factor+revise) model produced even less hallucinatory content while maintaining a high accuracy score.',\n",
              "  'source_doc': 'Chain-of-Verification Reduces Hallucination in Large Language Models.pdf'},\n",
              " {'context': 'BM25 Â§ 5â€“ 29.6 16.1 10.9 11.3 9.6\\nBM25 Zero-shot Â§ 6.1 28.6 15.5 10.1 10.6 8.8\\nBM25 Predictive Â§ 6.2 26.8 â€“ â€“ â€“ â€“\\nGPT-2 Mâ€“ â€“ 26.3 15.7 9.3 8.8 9.6\\nBM25 Â§ 5â€“ 21.5 12.4 8.6 8.1 7.4\\nBM25 Zero-shot Â§ 6.1 20.8 12.0 8.0 7.7 6.9\\nBM25 Predictive Â§ 6.2 19.7 â€“ â€“ â€“ â€“\\nGPT-2 Lâ€“ â€“ 22.0 13.6 8.4 8.5 8.7\\nBM25 Â§ 5â€“ 18.1 10.9 7.8 7.8 6.8\\nBM25 Zero-shot Â§ 6.1 17.6 10.6 7.3 7.4 6.4\\nBM25 Predictive Â§ 6.2 16.6 â€“ â€“ â€“ â€“\\nGPT-2 XLâ€“ â€“ 20.0 12.4 7.8 8.0 8.0\\nBM25 Â§ 5â€“ 16.6 10.1 7.2 7.4 6.4\\nBM25 Zero-shot Â§ 6.1 16.1 9.8 6.8 7.1 6.0\\nBM25 Predictive Â§ 6.2 15.4 â€“ â€“ â€“ â€“\\nTable 1: Perplexity on the test set of WikiText-103, RealNews and three datasets from the Pile. For\\neach LM, we report: (a) its performance without retrieval, (b) its performance when fed the top-scored\\npassage by BM25 (Â§ 5), and (c) its performance when applied on the top-scored passage of each of our two\\nsuggested rerankers (Â§ 6). All models share the same vocabulary, thus token-level perplexity (token ppl)\\nnumbers are comparable. For WikiText we follow prior work and report word-level perplexity (word ppl).\\nModel RetrievalWikiText-103\\nword ppl\\nLLaMA-7B- 9.9\\nBM25, Â§ 5 8.8\\nLLaMA-13B- 8.5\\nBM25, Â§ 5 7.6\\nLLaMA-33B- 6.3\\nBM25, Â§ 5 6.1\\nTable 2: The performance of models from the\\nLLaMA family, measured by word-level perplexity\\non the test set of WikiText-103.\\nContext RALM: applying a sparse BM25 retriever\\nthat receives â„“= 32 query tokens and is applied\\nas frequently as possible. Practically, we retrieve\\nevery s= 4 tokens ( â„“andsare de\\ue000ned in Â§ 3).\\nTable 1shows for the GPT-2 models that across\\nall the examined corpora, employing In-Context\\nRALM with an off-the-shelf retriever improved\\nLM perplexity to a suf\\ue000cient extent that it matched\\nthat of a 2â€“3Ã—larger model. Figure 4and Tables 2\\nand5show that this trend holds across model sizes\\nup to 66B parameters, for both WikiText-103 andRealNews.\\n5.1 BM25 Outperforms Off-the-Shelf Neural\\nRetrievers in Language Modeling\\nWe experimented with different off-the-shelf gen-',\n",
              "  'question': 'What is the difference between BM25 and GPT-2 models in terms of their performance on the test set of WikiText-103, RealNews, and three datasets from the Pile?\\n\\n',\n",
              "  'answer': 'BM25 models outperformed GPT-2 models in terms of their performance on the test set of WikiText-103, RealNews, and three datasets from the Pile. Specifically, BM25 models achieved lower perplexity than GPT-2 models when fed the top-scored passage by BM25, and when applied on the top-scored passage of each of the two suggested rerankers. The trend holds across model sizes up to 66B parameters.',\n",
              "  'source_doc': 'In-Context Retrieval-Augmented Language Models.pdf'},\n",
              " {'context': 'task of producing docstrings from code bodies, and that the\\nperformance proï¬les of these models were similar. Finally,\\nwe expanded on the broader impacts of code generating\\nmodels, and discussed model limitations, ï¬nding signiï¬cant\\nroom for improvement.\\nAcknowledgements\\nWe thank Sandhini Agarwal, Casey Chu, Jeffrey Ding, Pe-\\nter Eckersley, Gillian Hadï¬eld, Rich Harang, Jacob Jack-\\nson, Yunxin Jiao, Jade Leung, Andrew Lohn, Ryan Lowe,\\nThomas McGuire, Margaret Mitchell, Florentine Eloundou\\nNekoul, Cullen Oâ€™Keefe, Long Ouyang, Pranav Shyam,\\nIrene Solaiman, Aravind Srinivas, Helen Toner, Ashish\\nVaswani, and Jeffrey Wu for helpful discussions and feed-\\nback on drafts of this work. We are also grateful to the Accel-\\neration and Supercomputing teams at OpenAI for their work\\non software and hardware infrastructure that this project\\nused. Finally, we thank GitHub for partnering to build\\nGitHub Copilot and Microsoft Azure for supporting model\\ntraining with infrastructure management.\\nRef',\n",
              "  'question': 'How did the performance profiles of code generating models compare to each other?\\n\\n',\n",
              "  'answer': 'The performance profiles of the code generating models were similar.',\n",
              "  'source_doc': 'Evaluating Large Language Models Trained on Code.pdf'},\n",
              " {'context': 'the first method that would enable the finetuning of such models. We estimate that with an iPhone 12\\nPlus, QLORAcan finetune 3 million tokens per night while the phone is charging. While finetuned\\n7B models do not reach the quality of ChatGPT, we believe that the quality is good enough to enable\\nnovel applications that have not been possible before due to privacy or LLM quality issues. QLORA\\ncan help enable privacy-preserving usage of LLMs, where users can own and manage their own data\\nand models, while simultaneously making LLMs easier to deploy.\\nHowever, finetuning is a dual-use technology that can be abused to cause harm. Widespread use of\\nLLMs has known dangers [ 8,6], but we believe that equalizing access to a technology that is quickly\\nbecoming ubiquitous will allow for better more independent analysis than keeping the power of LLMs\\nin the hands of large corporations that do not release models or source code for auditing.\\nAll in all, we believe that QLORAwill have a broadly positive impact making the finetuning of high\\nquality LLMs much more widely and easily accessible.\\nAcknowledgements\\nWe thank Aditya Kusupati, Ofir Press, Ashish Sharma, Margaret Li, Raphael Olivier, Zihao Ye, and\\nEvangelia Spiliopoulou for their valuable feedback. Our research was facilitated by the advanced\\ncomputational, storage, and networking infrastructure of the Hyak supercomputer system at the\\nUniversity of Washington. We thank the Hyak team for ensuring a smooth operation. We thank\\nthe beta testers of the bitsandbytes library, in particular Alex Birch and Alyssa Vance. We thank\\nYounes Belkada for help with the integration of our software into the Hugging Face transformers\\nstack.\\n16References\\n[1]S. An, Y . Li, Z. Lin, Q. Liu, B. Chen, Q. Fu, W. Chen, N. Zheng, and J.-G. Lou. Input-tuning:\\nAdapting unfamiliar inputs to frozen pretrained models. arXiv preprint arXiv:2203.03131 ,\\n2022.\\n[2]A. Askell, Y . Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann,',\n",
              "  'question': \"Given QLORA's ability to finetune 3 million tokens per night on an iPhone 12 Plus while charging, what is the estimated quality of finetuned 7B models compared to ChatGPT?\\n\\n\",\n",
              "  'answer': 'While finetuned 7B models do not reach the quality of ChatGPT, they are believed to be good enough to enable novel applications that have not been possible before due to privacy or LLM quality issues.',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': '6ÂµÅ—Æ“Å¤Ã²Ë¤ÃŠË¤Ã§ÄµÄÃ²Å—Ã²Ä®ÆœË¤Å”ÃŠÅÅÃŠÄˆÃ²Ë¤ÄµÆ™Ë¤ÊË¤ÅÄÄµÅ—ÆœË¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄÅÊ›Ë¤\\x9aÄÃ²Ë¤Ã²Ä®Ã­Ë¤ÅÃ²Ä®Å¤Ã²Ä®Ã§Ã²Ë¤ÄµÆ™Ë¤Ã²ÃŠÃ§ÄË¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄË¤Ä­Å©ÅÆœË¤Ã¦Ã²ÊË¤É¾Ê›Ë¤GÆœË¤Æ“ÅÄ®Ë™Æ›Ë¤Ã­Ä‘Æ™Æ™Æ“Ã§Å©Ä¦ÆœË¤Æ˜ÄµË¤Ã­ÄµË¤ÃŠË¤ÄÃŠÄ®Ã­ÅÅ¤ÃŠÄ®Ã­Ë¤Ä‘Æ™Ë¤Æ†ÄµÅ©Ë¤Ä Å©ÅÆœË¤ÅÅ¤ÃŠÄ®Ã­Ë¤ÄµÄ®Ë¤Æ†ÄµÅ©Å—Ë¤ÄÃŠÄ®Ã­ÅÊ›Ë¤É¿Ê›Ë¤GÆœË¤Ã§ÃŠÅ©ÄˆÄÆœË¤ÄÄ‘Ä­Ë¤ÄµÆ™Æ™Ë¤ÄˆÅ©ÃŠÅ—Ã­Ë¤Æ›ÄÃŠÆœË¤ÅÅ”ÃŠÃ§Ã²Ë¤ÅÄ­Ã²Ä¦Ä¦Ã²Ã­Ë¤ÄµÆ™Ë¤ÅÃ²ÃŠÅ—Ã²Ã­Ë¤ÅÅ¤Ã²ÃŠÄ£Ê›Ë¤Ê€Ê›Ë¤ÂµÄÃ²Ä®Ë¤ÅÄÃ²Ë¤Ã­Æ“Ã­Ä®Ë’Æ›Ë¤Ä¦Ä‘Ä£Ã²Ë¤ÃŠË¤ÄˆÅ©Æ†Ë¤Æ€ÄÄµË¤Æ€ÃŠÅË¤Æ›Å—Æ†Ä‘Ä®ÄˆË¤Æ˜ÄµË¤Å”Æ“Ã§Ä£Ë¤ÄÃ²Å—Ë¤Å©Å”ÊœË¤ÅÄÃ²Ë¤ÅÅ¤ÃŠÅ—Å¤Ã²Ã­Ë¤Å©ÅÄ‘Ä®ÄˆË¤ÅÆ“ÄˆÄ®Ë¤Ä¦ÃŠÄ®ÄˆÅ©ÃŠÄˆÃ²Ê›Ë¤ÊÊ›Ë¤)ÃŠÃ§ÄË¤Å”Ã²Å—ÅÄµÄ®Ë¤Æ€ÄÄµË¤Ä£Ä®ÄµÆ€ÅË¤Æ†ÄµÅ©Ë¤ÄÃŠÅË¤ÃŠË¤Ã­Ä‘Æ™Ä‡Ã²Å—Ã²Ä®ÆœË¤Å”Ã²Å—Ã§Ã²Å”ÆœÆ“ÄµÄ®Ë¤ÄµÆ™Ë¤Æ€ÄÄµË¤Æ†ÄµÅ©Ë¤ÃŠÅ—Ã²Ê›Ë¤Ë¤É¾Ê›Ë¤GÄ®ÆœÅ—ÄµÃ­Å©Ã§Ã²Ë¤ÃŠÄ®Ã­Ë¤Ã²Æ…Å”Ä¦ÃŠÄ‘Ä®Ë¤Æ›ÄÃ²Ë¤Æ˜Ã²Ã§ÄÄ®Æ“Å–Å©Ã²Ë¤ÄµÆ™Ë¤Ã­ÄµÄ‘Ä®ÄˆË¤ÃŠË¤ÄÃŠÄ®Ã­ÅÅ¤ÃŠÄ®Ã­Ë¤É¿Ê›Ë¤\\x92Æ€Æ“Å¤Ã§ÄË¤Æ˜ÄµË¤ÃŠË¤ÅÅ¤ÄµÅ—Æ†Ë¤ÃŠÃ¦ÄµÅ©ÆœË¤ÃŠÄ®Ë¤ÃŠÅÆœÅ—ÄµÄ®ÃŠÅ©ÆœË™ÅË¤ÆšÄ‘Å—ÅÆœË¤Æ›Ä‘Ä­Ã²Ë¤Ä‘Ä®Ë¤ÅÅ”ÃŠÃ§Ã²Ë¤Ê€Ê›Ë¤#Ã²ÅÃ§Å—Ä‘Ã¦Ã²Ë¤ÃŠË¤ÅÆ“ÆœÅ©ÃŠÆœÆ“ÄµÄ®Ë¤Æ€ÄÃ²Å—Ã²Ë¤ÃŠË¤Æ€ÄµÄ­ÃŠÄ®Ë¤Å©ÅÃ²ÅË¤ÅÆ“ÄˆÄ®Ë¤Ä¦ÃŠÄ®ÄˆÅ©ÃŠÄˆÃ²Ë¤Æ˜ÄµË¤ÃŠÅ¿ÄµÆ“Ã­Ë¤Å©Ä®Æ€ÃŠÄ®Å¤Ã²Ã­Ë¤ÃŠÆœÅ¤Ã²Ä®ÆœÆ“ÄµÄ®Ë¤ÊÊ›Ë¤\\x9aÄÃ²Ë¤ÆšÄ‘Ä®ÃŠÄ¦Ë¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄË¤Ã²Æ…Å”Ä¦ÃŠÄ‘Ä®ÅË¤ÄÄµÆ€Ë¤Ã²Å¿Ã²Å—Æ†ÄµÄ®Ã²Ë¤ÄÃŠÅË¤Ã­Ä‘Æ™Ä‡Ã²Å—Ã²Ä®ÆœË¤Å”Ã²Å—Ã§Ã²Å”ÆœÆ“ÄµÄ®ÅË¤ÄµÆ™Ë¤ÄµÆœÄÃ²Å—ÅÉ¾Ê›Ë¤GÄ®ÆœÅ—ÄµÃ­Å©Ã§ÆœÆ“ÄµÄ®Ë¤Æ˜ÄµË¤ÃŠÄ®Ë¤Å©Ä®Å©ÅÅ©ÃŠÄ¦Ë¤ÅÃ²Ä¦Æ™ËŠÄÃ²Ä¦Å”Ë¤Ã¦ÄµÄµÄ£ÊœË¤Ä­Ã²Ä®ÆœÆ“ÄµÄ®Ä‘Ä®ÄˆË¤ÃŠË¤ÄÃŠÄ®Ã­ÅÅ¤ÃŠÄ®Ã­Ë¤ÃŠÅË¤ÃŠË¤Ä­Ã²Å¤ÃŠÅ”ÄÄµÅ—Ë¤Æ—ÄµÅ—Ë¤Ã²Ä­Ã¦Å—ÃŠÃ§Ä‘Ä®ÄˆË¤Ã§ÄÃŠÄ¦Ä¦Ã²Ä®ÄˆÃ²ÅÊ›Ë¤É¿Ê›Ë¤#Æ“ÅÃ§Å©ÅÅË¤Æ›ÄÃ²Ë¤Å©Ä®Ã²Æ…Å”Ã²Ã§Å¤Ã²Ã­Ë¤Æ›ÄÄ‘Ä®ÄˆÅË¤Ä¦Ã²ÃŠÅ—Ä®Ã²Ã­Ë¤ÆšÅ—ÄµÄ­Ë¤ÃŠÅÆœÅ—ÄµÄ®ÃŠÅ©Å¤ÅÊœË¤Ä‘Ä®Ã§Ä¦Å©Ã­Ä‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤ÅÄ­Ã²Ä¦Ä¦Ë¤ÄµÆ™Ë¤ÅÅ”ÃŠÃ§Ã²Ê›Ë¤Ê€Ê›Ë¤#Ã²ÅÃ§Å—Ä‘Ã¦Ã²Ë¤ÃŠË¤Æ€ÄµÄ­ÃŠÄ®Ë™ÅË¤Ã§Ä¦Ã²Å¿Ã²Å—Ë¤Æ˜ÃŠÃ§ÆœÆ“Ã§Ë¤Æ—ÄµÅ—Ë¤ÃŠÅ¿ÄµÆ“Ã­Ä‘Ä®ÄˆË¤Å©Ä®Æ€ÃŠÄ®Å¤Ã²Ã­Ë¤ÃŠÆœÅ¤Ã²Ä®ÆœÆ“ÄµÄ®Ë¤ÃŠÆœË¤ÃŠË¤Ã¦ÃŠÅ—Ê›Ë¤ÊÊ›Ë¤\\x1dÄµÄ®Å¤Ã²Ä­Å”Ä¦ÃŠÅ¤Ã²Ë¤ÄÄµÆ€Ë¤Ã­Ä‘Æ™Ä‡Ã²Å—Ã²Ä®ÆœË¤Å”Ã²Å—Ã§Ã²Å”ÆœÆ“ÄµÄ®ÅË¤ÄµÆ™Ë¤ÄµÄ®Ã²ÅÃ²Ä¦Æ™Ë¤Ã§ÃŠÄ®Ë¤ÅÄÃŠÅ”Ã²Ë¤ÄµÄ®Ã²Ë™ÅË¤Æ“Ã­Ã²Ä®ÆœÆ“Å¤Æ†Ê›Ê±ÃŠÊ²Ë¤GÄ®Å”Å©ÆœÊ±Ã¦Ê²Ë¤\\x89Ä¦ÃŠÄ®ÅÊ±Ê€Ë¤Ä­ÄµÅ—Ã²Ë¤ÄµÄ­Æ“ÆœÅ¤Ã²Ã­Ê²Ê±Ã§Ê²Ë¤Â´ÄµÅ¤Ã²Å\\x01Ä®ÃŠÄ¦Æ†ÆÄ‘Ä®ÄˆË¤Ã²ÃŠÃ§ÄË¤Ã§ÄÄµÆ“Ã§Ã²Ë¤Ä‘Ä®Ë¤Ã­Ã²Å¤ÃŠÆ“Ä¦ÊË¤Ë¤\\x1dÄÄµÆ“Ã§Ã²Ë¤É¾ÊœË¤Æ€ÄÆ“Ä¦Ã²Ë¤Ä‘Ä®Ã§ÄµÅ—Å”ÄµÅ—ÃŠÆœÄ‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤Å—Ã²Å–Å©Ä‘Å—Ã²Ã­Ë¤Ã²Ä®Ã­Ë¤ÅÃ²Ä®Å¤Ã²Ä®Ã§Ã²ÅÊœË¤ÅÃ²Ã²Ä­ÅË¤Æ˜ÄµË¤Ä¦ÃŠÃ§Ä£Ë¤ÃŠË¤Ã§Ä¦Ã²ÃŠÅ—Ë¤Ã§ÄµÄ®Ä®Ã²Ã§ÆœÆ“ÄµÄ®Ë¤Ã¦Ã²Å¤Æ€Ã²Ã²Ä®Ë¤Æ›ÄÃ²Ë¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄÅÊŸË¤\\x1dÄÄµÆ“Ã§Ã²Ë¤É¿Ë¤ÄµÆ™Ä‡Ã²Å—ÅË¤ÃŠÄ®Ë¤Ä‘Ä®Å¤Ã²Å—Ã²ÅÆœÄ‘Ä®ÄˆË¤Å”Ã²Å—ÅÅ”Ã²Ã§ÆœÆ“Å¿Ã²Ë¤Ã¦Æ†Ë¤Å©ÅÄ‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤Å—Ã²Å–Å©Ä‘Å—Ã²Ã­Ë¤Ã²Ä®Ã­Ë¤ÅÃ²Ä®Å¤Ã²Ä®Ã§Ã²ÅË¤Æ˜ÄµË¤Å”Å—Ã²ÅÃ²Ä®ÆœË¤ÃŠË¤ÅÃ²Ä¦Æ™ËŠÄÃ²Ä¦Å”Ë¤Ã¦ÄµÄµÄ£Ë™ÅË¤Ã§ÄµÄ®Å¤Ã²Ä®ÆœÊ›Ë¤GÆœË¤Ã§ÄµÄ®Ä®Ã²Ã§Å¤ÅË¤Æ›ÄÃ²Ë¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄÅË¤Æ€Æ“ÆœÄË¤Æ›ÄÃ²Ë¤Æ›ÄÃ²Ä­Ã²Ë¤ÄµÆ™Ë¤ÅÃ²Ä¦Æ™ËŠÄ‘Ä­Å”Å—ÄµÅ¿Ã²Ä­Ã²Ä®ÆœË¤ÃŠÄ®Ã­Ë¤Ã²Ä­Ã¦Å—ÃŠÃ§Ä‘Ä®ÄˆË¤Ã§ÄÃŠÄ¦Ä¦Ã²Ä®ÄˆÃ²ÅÊœË¤Ä­ÃŠÄ£Ä‘Ä®ÄˆË¤Æ—ÄµÅ—Ë¤ÃŠË¤Ã§ÄµÄÃ²Å—Ã²Ä®ÆœË¤Å”ÃŠÅÅÃŠÄˆÃ²Ê›Ë¤Ë¤Ê³Ê›Ê›Ê›Ê´Ë¤\\x9aÄÃ²Ë¤Ã¦Ã²ÅÆœË¤Ã§ÄÄµÆ“Ã§Ã²Ë¤Æ“ÅË¤É¿Ê›Ê±É½Ê«Ê‚Ë¤Å¿ÄµÅ¤Ã²ÅÊ²\\x01Ä®ÃŠÄ¦Æ†ÆÄ‘Ä®ÄˆË¤Ã²ÃŠÃ§ÄË¤Ã§ÄÄµÆ“Ã§Ã²Ë¤Ä‘Ä®Ë¤Ã­Ã²Å¤ÃŠÆ“Ä¦ÊË¤Ë¤\\x1dÄÄµÆ“Ã§Ã²Ë¤É¾ÊœË¤Æ€ÄÆ“Ä¦Ã²Ë¤Ä‘Ä®Ã§ÄµÅ—Å”ÄµÅ—ÃŠÆœÄ‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤Å—Ã²Å–Å©Ä‘Å—Ã²Ã­Ë¤Ã²Ä®Ã­Ë¤ÅÃ²Ä®Å¤Ã²Ä®Ã§Ã²ÅÊœË¤ÅÃ²Ã²Ä­ÅË¤Æ˜ÄµË¤Ä¦ÃŠÃ§Ä£Ë¤ÃŠË¤Ã§Ä¦Ã²ÃŠÅ—Ë¤Ã§ÄµÄ®Ä®Ã²Ã§ÆœÆ“ÄµÄ®Ë¤Ã¦Ã²Å¤Æ€Ã²Ã²Ä®Ë¤Æ›ÄÃ²Ë¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄÅÊŸË¤\\x1dÄÄµÆ“Ã§Ã²Ë¤É¿Ë¤ÄµÆ™Ä‡Ã²Å—ÅË¤ÃŠÄ®Ë¤Ä‘Ä®Å¤Ã²Å—Ã²ÅÆœÄ‘Ä®ÄˆË¤Å”Ã²Å—ÅÅ”Ã²Ã§ÆœÆ“Å¿Ã²Ë¤Ã¦Æ†Ë¤Å©ÅÄ‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤Å—Ã²Å–Å©Ä‘Å—Ã²Ã­Ë¤Ã²Ä®Ã­Ë¤ÅÃ²Ä®Å¤Ã²Ä®Ã§Ã²ÅË¤Æ˜ÄµË¤Å”Å—Ã²ÅÃ²Ä®ÆœË¤ÃŠË¤ÅÃ²Ä¦Æ™ËŠÄÃ²Ä¦Å”Ë¤Ã¦ÄµÄµÄ£Ë™ÅË¤Ã§ÄµÄ®Å¤Ã²Ä®ÆœÊ›Ë¤GÆœË¤Ã§ÄµÄ®Ä®Ã²Ã§Å¤ÅË¤Æ›ÄÃ²Ë¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄÅË¤Æ€Æ“ÆœÄË¤Æ›ÄÃ²Ë¤Æ›ÄÃ²Ä­Ã²Ë¤ÄµÆ™Ë¤ÅÃ²Ä¦Æ™ËŠÄ‘Ä­Å”Å—ÄµÅ¿Ã²Ä­Ã²Ä®ÆœË¤ÃŠÄ®Ã­Ë¤Ã²Ä­Ã¦Å—ÃŠÃ§Ä‘Ä®ÄˆË¤Ã§ÄÃŠÄ¦Ä¦Ã²Ä®ÄˆÃ²ÅÊœË¤Ä­ÃŠÄ£Ä‘Ä®ÄˆË¤Æ—ÄµÅ—Ë¤ÃŠË¤Ã§ÄµÄÃ²Å—Ã²Ä®ÆœË¤Å”ÃŠÅÅÃŠÄˆÃ²Ê›Ë¤Ë¤Ê³Ê›Ê›Ê›Ê´Ë¤\\x9aÄÃ²Ë¤Ã¦Ã²ÅÆœË¤Ã§ÄÄµÆ“Ã§Ã²Ë¤Æ“ÅË¤É¿Ê›\\x01Ä®ÃŠÄ¦Æ†ÆÄ‘Ä®ÄˆË¤Ã²ÃŠÃ§ÄË¤Ã§ÄÄµÆ“Ã§Ã²Ë¤Ä‘Ä®Ë¤Ã­Ã²Å¤ÃŠÆ“Ä¦ÊË¤Ë¤\\x1dÄÄµÆ“Ã§Ã²Ë¤É¾ÊœË¤Æ€ÄÆ“Ä¦Ã²Ë¤Ä‘Ä®Ã§ÄµÅ—Å”ÄµÅ—ÃŠÆœÄ‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤Å—Ã²Å–Å©Ä‘Å—Ã²Ã­Ë¤Ã²Ä®Ã­Ë¤Å',\n",
              "  'question': 'What is the definition of \"deep factual or conceptual question\"?\\n\\n',\n",
              "  'answer': 'A deep factual or conceptual question is a question that requires a thorough understanding of a subject or concept, often involving complex concepts, analysis, and critical thinking. It is not a straightforward question that can be answered with a simple \"yes\" or \"no\" or by recalling basic information. A deep question requires a deeper level of understanding and often involves exploring multiple perspectives and ideas.',\n",
              "  'source_doc': 'Tree of Thoughts_  Deliberate Problem Solving with Large Language Models.pdf'},\n",
              " {'context': 'thermore, we find that current chatbot benchmarks are not trustworthy to accurately\\nevaluate the performance levels of chatbots. A lemon-picked analysis demonstrates\\nwhere Guanaco fails compared to ChatGPT. We release all of our models and code,\\nincluding CUDA kernels for 4-bit training.2\\n1 Introduction\\nFinetuning large language models (LLMs) is a highly effective way to improve their performance,\\n[40,62,43,61,59,37] and to add desirable or remove undesirable behaviors [ 43,2,4]. However,\\nfinetuning very large models is prohibitively expensive; regular 16-bit finetuning of a LLaMA 65B\\nparameter model [ 57] requires more than 780 GB of GPU memory. While recent quantization\\nmethods can reduce the memory footprint of LLMs [ 14,13,18,66], such techniques only work for\\ninference and break down during training [65].\\nWe demonstrate for the first time that it is possible to finetune a quantized 4-bit model without any\\nperformance degradation. Our method, QLORA, uses a novel high-precision technique to quantize\\na pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights [ 28]\\nâˆ—Equal contribution.\\n2https://github.com/artidoro/qlora andhttps://github.com/TimDettmers/bitsandbytes\\nPreprint. Under review.arXiv:2305.14314v1  [cs.LG]  23 May 2023Table 1: Elo ratings for a competition between\\nmodels, averaged for 10,000 random initial order-\\nings. The winner of a match is determined by\\nGPT-4 which declares which response is better for\\na given prompt of the the Vicuna benchmark. 95%\\nconfidence intervals are shown ( Â±). After GPT-\\n4, Guanaco 33B and 65B win the most matches,\\nwhile Guanaco 13B scores better than Bard.\\nModel Size Elo\\nGPT-4 - 1348 Â±1\\nGuanaco 65B 41 GB 1022 Â±1\\nGuanaco 33B 21 GB 992 Â±1\\nVicuna 13B 26 GB 974 Â±1\\nChatGPT - 966 Â±1\\nGuanaco 13B 10 GB 916 Â±1\\nBard - 902 Â±1\\nGuanaco 7B 6 GB 879 Â±1that are tuned by backpropagating gradients through\\nthe quantized weights.\\nQLORAreduces the average memory requirements',\n",
              "  'question': 'How does QLORA reduce the memory requirements of LLMs during training without any performance degradation?\\n\\n',\n",
              "  'answer': 'QLORA uses a novel high-precision technique to quantize a pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights. This method reduces the average memory requirements of LLMs during training without any performance degradation.',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': 'people who will use and be affected by our deployed models. As a simple example, our labelers are\\nprimarily English-speaking and our data consists almost entirely of English instructions.\\nThere are also many ways in which we could improve our data collection set-up. For instance, most\\ncomparisons are only labeled by 1 contractor for cost reasons. Having examples labeled multiple\\ntimes could help identify areas where our contractors disagree, and thus where a single model is\\nunlikely to align to all of them. In cases of disagreement, aligning to the average labeler preference\\nmay not be desirable. For example, when generating text that disproportionately affects a minority\\ngroup, we may want the preferences of labelers belonging to that group to be weighted more heavily.\\nModels. Our models are neither fully aligned nor fully safe; they still generate toxic or biased\\noutputs, make up facts, and generate sexual and violent content without explicit prompting. They can\\nalso fail to generate reasonable outputs on some inputs; we show some examples of this in Figure 9.\\nPerhaps the greatest limitation of our models is that, in most cases, they follow the userâ€™s instruction,\\neven if that could lead to harm in the real world. For example, when given a prompt instructing the\\nmodels to be maximally biased, InstructGPT generates more toxic outputs than equivalently-sized\\nGPT-3 models. We discuss potential mitigations in the following sections.\\n5.4 Open questions\\nThis work is a ï¬rst step towards using alignment techniques to ï¬ne-tune language models to follow a\\nwide range of instructions. There are many open questions to explore to further align language model\\nbehavior with what people actually want them to do.\\nMany methods could be tried to further decrease the modelsâ€™ propensity to generate toxic, biased,\\nor otherwise harmful outputs. For example, one could use an adversarial set-up where labelers ï¬nd',\n",
              "  'question': 'In what ways do our models fail to generate reasonable outputs on some inputs, and what could be done to mitigate these limitations?\\n\\n',\n",
              "  'answer': 'Our models fail to generate reasonable outputs on some inputs due to their propensity to generate toxic, biased, or harmful outputs. To mitigate these limitations, methods such as using an adversarial set-up where labelers find and correct biased outputs could be tried. Additionally, techniques such as fine-tuning the models on specific domains or tasks could help them generate more accurate and appropriate outputs. It is important to continue exploring open questions and developing new methods to align language model behavior with what people actually want them to do.',\n",
              "  'source_doc': 'Training language models to follow instructions with human feedback.pdf'},\n",
              " {'context': 'is that the modelâ€™s behavior out-of-distribution from the data is essentially undetermined. In other\\nwords, there is no optimization pressure on the model to learn how to â€˜recoverâ€™ from mistakes.\\nIn non-autoregressive generative modelling, a wide variety of different divergences are commonly\\nused, such as the Wasserstein distance [ 2] and Fisher divergence [ 30]. Particularly interesting is the\\nÏ‡2divergence DÏ‡2(PÎ¸, Pdata) =Exâˆ¼Pdata\\x02\\n(PÎ¸(x)/Pdata(x)âˆ’1)2\\x03\\n. Indeed we see in figure 1 that\\ntheÏ‡2-divergence in this case is equal to the squared probability of staying in the data distribution\\nof sequences. We can further penalize out-of-distribution behavior by considering the divergence\\nbetween mixtures DÏ‡2(PÎ¸,(Pdata+PÎ¸)/2), as we do in our practical algorithm. However, it is\\ngenerally difficult in practice to compute any divergence involving the density of the data, which\\nmust be substituted for with an approximation from a discriminator.\\nIn the field of reinforcement learning, several methods have been discovered which can minimize\\ndivergences such as Wasserstein and JS between the distribution of trajectories from an expert and\\na learned policy. The approaches are non-adversarial, even though the expert density is not known\\n[11,32,4,1]. A key feature of these methods is that they operate on occupancy measures instead of\\njoint distributions, a concept which we introduce in the next section 3.\\n3 Method\\n3.1 Sequence Modelling as a Markov Decision Process\\nWe consider a sequence model represented as a Markov decision process (MDP), defined by a tuple\\n(S,A,P, r, Î³).S,Arepresent state and action spaces, P(sâ€²|s, a)represents the dynamics, r(s, a)\\nrepresents the reward function, and Î³âˆˆ(0,1)represents the discount factor. In our case, the state\\n1Some care is required here, as each averaging the loss of each example over its length leads to an inconsistent\\nestimator.\\n3spaceSis the set of all sequences (of all lengths) with elements in a finite set X(the vocabulary plus',\n",
              "  'question': 'How does the use of Ï‡2-divergence in non-autoregressive generative modeling help to penalize out-of-distribution behavior?\\n\\n',\n",
              "  'answer': 'The Ï‡2-divergence in non-autoregressive generative modeling is used to penalize out-of-distribution behavior by measuring the squared probability of staying in the data distribution of sequences. By optimizing for a lower Ï‡2-divergence, the model is encouraged to generate sequences that are more likely to stay within the data distribution, reducing the likelihood of out-of-distribution behavior. Additionally, the Ï‡2-divergence can be further penalized by considering the divergence between mixtures, which helps to further reduce out-of-distribution behavior.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'V\\x03WKLQN\\x03VWHS\\x03E\\\\\\x03VWHS\\x11\\x03$SSOH\\x035HPRWH\\x03ZDV\\x03RULJLQDOO\\\\\\x03GHVLJQHG\\x03WR\\x03LQWHUDFW\\x03ZLWK\\x03$SSOH\\x0379\\x11\\x03$SSOH\\x0379\\x03FDQ\\x03EH\\x03FRQWUROOHG\\x03E\\\\\\x03L3KRQH\\x0f\\x03L3DG\\x0f\\x03DQG\\x03L3RG\\x037RXFK\\x11\\x036R\\x03WKH\\x03DQVZHU\\x03LV\\x03L3KRQH\\x0f\\x03L3DG\\x0f\\x03DQG\\x03L3RG\\x037RXFK\\x11$QVZHU\\x1d\\x03L3KRQH\\x0f\\x03L3DG\\x0f\\x03L3RG\\x037RXFK\\x0b\\x14F\\x0c\\x03$FW\\x102QO\\\\\\x03$FW\\x03\\x14\\x1d\\x036HDUFK>$SSOH\\x035HPRWH@\\x032EV\\x03\\x14\\x1d\\x037KH\\x03$SSOH\\x035HPRWH\\x03LV\\x03D\\x03UHPRWH\\x03FRQWURO\\x03Âª$FW\\x03\\x15\\x1d\\x036HDUFK>)URQW\\x035RZ@\\x032EV\\x03\\x15\\x1d\\x03&RXOG\\x03QRW\\x03ILQG\\x03>)URQW\\x035RZ@\\x11\\x036LPLODU\\x1d\\x03Âª$FW\\x03\\x16\\x1d\\x036HDUFK>)URQW\\x035RZ\\x03\\x0bVRIWZDUH\\x0c@\\x032EV\\x03\\x16\\x1d\\x03)URQW\\x035RZ\\x03LV\\x03D\\x03GLVFRQWLQXHG\\x03PHGLD\\x03FHQWHU\\x03VRIWZDUH\\x03Âª$FW\\x03\\x17\\x1d\\x03)LQLVK>\\\\HV@\\x0b\\x14G\\x0c\\x035H$FW\\x03\\x0b5HDVRQ\\x03\\x0e\\x03$FW\\x0c7KRXJKW\\x03\\x14\\x1d\\x03,\\x03QHHG\\x03WR\\x03VHDUFK\\x03$SSOH\\x035HPRWH\\x03DQG\\x03ILQG\\x03WKH\\x03SURJUDP\\x03LW\\x03ZDV\\x03RULJLQDOO\\\\\\x03GHVLJQHG\\x03WR\\x03LQWHUDFW\\x03ZLWK\\x11$FW\\x03\\x14\\x1d\\x036HDUFK>$SSOH\\x035HPRWH@2EV\\x03\\x14\\x1d\\x037KH\\x03$SSOH\\x035HPRWH\\x03LV\\x03D\\x03UHPRWH\\x03FRQWURO\\x03LQWURGXFHG\\x03LQ\\x032FWREHU\\x03\\x15\\x13\\x13\\x18\\x03E\\\\\\x03$SSOH\\x03Âª\\x03RULJLQDOO\\\\\\x03GHVLJQHG\\x03WR\\x03FRQWURO\\x03WKH\\x03)URQW\\x035RZ\\x03PHGLD\\x03FHQWHU\\x03SURJUDP\\x03Âª7KRXJKW\\x03\\x15\\x1d\\x03$SSOH\\x035HPRWH\\x03ZDV\\x03RULJLQDOO\\\\\\x03GHVLJQHG\\x03WR\\x03FRQWURO\\x03WKH\\x03)URQW\\x035RZ\\x03PHGLD\\x03FHQWHU\\x03SURJUDP\\x11\\x03,\\x03QHHG\\x03WR\\x03VHDUFK\\x03)URQW\\x035RZ\\x03QH[W\\x03DQG\\x03ILQG\\x03ZKDW\\x03RWKHU\\x03GHYLFH\\x03FDQ\\x03FRQWURO\\x03LW\\x11$FW\\x03\\x15\\x1d\\x036HDUFK>)URQW\\x035RZ@2EV\\x03\\x15\\x1d\\x03&RXOG\\x03QRW\\x03ILQG\\x03>)URQW\\x035RZ@\\x11\\x036LPLODU\\x1d\\x03>\\n)URQW\\x035RZ\\x036HDW\\x03WR\\x03(DUWK\\n\\x0f\\x03\\n)URQW\\x035RZ\\x030RWRUVSRUWV\\n\\x0f\\n)URQW\\x035RZ\\x03\\x0bVRIWZDUH\\x0c\\n\\x0f\\x03Âª@7KRXJKW\\x03\\x16\\x1d\\x03)URQW\\x035RZ\\x03LV\\x03QRW\\x03IRXQG\\x11\\x03,\\x03QHHG\\x03WR\\x03VHDUFK\\x03)URQW\\x035RZ\\x03\\x0bVRIWZDUH\\x0c\\x03\\x11$FW\\x03\\x16\\x1d\\x036HDUFK>)URQW\\x035RZ\\x03\\x0bVRIWZDUH\\x0c@2EV\\x03\\x16\\x1d\\x03)URQW\\x035RZ\\x03LV\\x03D\\x03GLVFRQWLQXHG\\x03PHGLD\\x03FHQWHU\\x03VRIWZDUH\\x03Âª7KRXJKW\\x03\\x17\\x1d\\x03)URQW\\x035RZ\\x03\\x0bVRIWZDUH\\x0c\\x03LV\\x03FRQWUROOHG\\x03E\\\\\\x03DQ\\x03$SSOH\\x035HPRWH\\x03RU\\x03WKH\\x03NH\\\\ERDUG\\x03IXQFWLRQ\\x03NH\\\\V\\x11\\x036R\\x03WKH\\x03DQVZHU\\x03LV\\x03NH\\\\ERDUG\\x03IXQFWLRQ\\x03NH\\\\V\\x11$FW\\x03\\x17\\x1d\\x03)LQLVK>NH\\\\ERDUG\\x03IXQFWLRQ\\x03NH\\\\V@Ò¼4XHVWLRQ\\x1d\\x03$VLGH\\x03IURP\\x03WKH\\x03$SSOH\\x035HPRWH\\x0f\\x03ZKDW\\x03RWKHU\\x03GHYLFH\\x03FDQ\\x03FRQWURO\\x03WKH\\x03SURJUDP\\x03$SSOH\\x035HPRWH\\x03ZDV\\x03RULJLQDOO\\\\\\x03GHVLJQHG\\x03WR\\x03LQWHUDFW\\x03ZLWK\"',\n",
              "  'question': 'What is the value of $SSOH in the given context?\\n',\n",
              "  'answer': 'The value of $SSOH in the given context is \"79\".',\n",
              "  'source_doc': 'ReAct_ Synergizing Reasoning and Acting in Language Models.pdf'},\n",
              " {'context': 'chosen randomly from zero to half the context length. The completion is always set to continue up to\\nthe context length or <EOS> token.\\nFor the evaluation, we set the prompt length to be half the context length. We then generate sequences\\nof length 256, for both the context length 512 and 1024. For the generation, we set the temperature\\nto 1 and the top-p sampling to 1. However, for the increased length of the 1024 context length, we\\nset the top-p parameter to 0.95. We use the default settings to evaluate the mauve score with 512\\nsamples.\\n18Small Medium Large\\nGPT2 Model Size100\\n7Ã—101\\n8Ã—101\\n9Ã—101\\nDiversity Measure\\nDiversity Score\\nSequenceMatch (Full Model)\\nBehavioral Cloning (Full Model)\\nMLE (Full Model)\\nSequenceMatch (LM Head)\\nBehavioral Cloning (LM Head)\\nMLE (LM Head)Figure 4: Diversity score [ 17] when fine-tuning GPT2 of various sizes on the openwebtext dataset\\nwith context length 512. Higher is better. Full and LM Head refer to which parts of the pretrained\\nmodel are trained. We see that SequenceMatch outperforms behavioral cloning and MLE for a fixed\\nmodel size. However, the fully trained models obtain lower scores than the variant only training the\\nhead.\\nFetch, Process Batch Grad. Step Sample Raw Grad. steps per Sample Total\\nSM-(Small, Full) 0.3 Â±0.05 2.2 Â±0.02 10 Â±2 16 3.1 Â±0.5\\nMLE-(Small, Full) 0.25 Â±0.05 1.2 Â±0.1 N/A N/A 1.45 Â±0.15\\nSM-(Large, LM Head) 0.3 Â±0.05 4.9 Â±0.1 33 Â±5 12 7.95 Â±0.7\\nMLE-(Large, LM Head) 0.25 Â±0.05 2.2 Â±0.1 N/A N/A 2.45 Â±0.15\\nTable 2: Execution time of various parts of the training loop for the different models for the 512\\ncontext length, Ï‡2objective with model rollouts regularization. We show the raw time to sample a\\nbatch of trajectories, as well as the time for sampling once amortized due to the fact that we do not\\nsample every training step.\\nF Additional Plots\\nIn figure 4 we show the results for the diversity metric from [ 17]. We see that for a fixed model',\n",
              "  'question': 'Given the provided context, what is the correlation between the diversity score and the model size when fine-tuning GPT2 on the openwebtext dataset with a context length of 512?\\n\\n',\n",
              "  'answer': 'The diversity score tends to be higher for models with a smaller size when fine-tuning GPT2 on the openwebtext dataset with a context length of 512. This is evidenced by the fact that SequenceMatch outperforms behavioral cloning and MLE for a fixed model size, and the fully trained models obtain lower scores than the variant only training the head.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'that North Korean leader Kim Jong Un was behind the misdeeds of his country. In response,\\nTrumpâ€™s new administration has shown itself to be actively talking about North Korean\\ndiplomacy, telling the American community last month that \"\"North Korea is a wonderful,\\ngreat nation. It has turned several dangerous institutions into havens of radicalism and\\nterrorism, and I am confident that North Korea is also prepared to launch a strike and\\ndestroy any attempts to stop and shut down its programme. We must put forward options and\\nseek a comprehensive, comprehensive, and comprehensive response.\"\" North Korea had clearly\\nmisreported the strategic consequences of its ballistic missile launch Sunday. A missile\\ndoes not threaten the US or the South; it appears to have fallen from the sky. In fact,\\nNorth Korea has already launched about 95,000 missiles in less than a month.<|endoftext|>Table 4: Model Prompts and Completions for the 1024 MLE trained model. In this setting we add a random\\ntoken at the end of the prompt and observe the behavior of the generated sequences. Continued on next page\\nPrompt Completion\\nWHAT?!??! I know. Thatâ€™s what youâ€™re saying right now. \"WHAT?! DISNEY HAS A DONUT\\nSUNDAE AND I DIDNâ€™T KNOW ABOUT IT?!\" How do I know youâ€™re saying that? Because thatâ€™s\\nexactly what I was saying when Tina (<- amazing DFB photographer whoâ€™s frigging awesome)\\nspotted this at the Plaza Ice Cream Parlor in the Magic Kingdom this week! But itâ€™s\\nOK. Itâ€™s brand new | not even on the menu yet | so we didnâ€™t miss out on too much of\\nthe Donut Sundae lifespan. And weâ€™re hoping that lifespan is a nice, long one! The\\nMain Street Plaza Ice Cream Parlor can be found at the intersection of Main Street USA\\nand Tomorrowland, just before you get to Cinderella Castle. And the sundae joins a few\\nother must-have treats on the Ice Cream Parlorâ€™s menu, including the house-made ice cream\\nsandwich (preferably ordered with a drizzled sauce!), the \"kidsâ€™ cone\" (itâ€™s totally OK to',\n",
              "  'question': \"How did the misdeeds of North Korea's leader Kim Jong Un lead to the current situation with North Korea?\\n\\n\",\n",
              "  'answer': \"The misdeeds of North Korea's leader Kim Jong Un have led to the current situation with North Korea due to the dangerous institutions and radicalism and terrorism that the country has become. This has resulted in the country being prepared to launch a strike and destroy any attempts to stop and shut down its program. The misreporting of the strategic consequences of its ballistic missile launch Sunday also contributed to the current situation.\",\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'j\\x02j= 2\\x02^LLoRA\\x02dmodel\\x02r, where ^LLoRA is the number of weight matrices we apply LoRA to.\\n6Model & Method # Trainable E2E NLG Challenge\\nParameters BLEU NIST MET ROUGE-L CIDEr\\nGPT-2 M (FT)* 354.92M 68.2 8.62 46.2 71.0 2.47\\nGPT-2 M (AdapterL)* 0.37M 66.3 8.41 45.0 69.8 2.40\\nGPT-2 M (AdapterL)* 11.09M 68.9 8.71 46.1 71.3 2.47\\nGPT-2 M (AdapterH) 11.09M 67.3\\x06.68.50\\x06.07 46.0\\x06.2 70.7\\x06.2 2.44\\x06.01\\nGPT-2 M (FTTop2)* 25.19M 68.1 8.59 46.0 70.8 2.41\\nGPT-2 M (PreLayer)* 0.35M 69.7 8.81 46.1 71.4 2.49\\nGPT-2 M (LoRA) 0.35M 70.4\\x06.18.85\\x06.02 46.8\\x06.2 71.8\\x06.1 2.53\\x06.02\\nGPT-2 L (FT)* 774.03M 68.5 8.78 46.0 69.9 2.45\\nGPT-2 L (AdapterL) 0.88M 69.1\\x06.18.68\\x06.03 46.3\\x06.0 71.4\\x06.2 2.49\\x06.0\\nGPT-2 L (AdapterL) 23.00M 68.9\\x06.38.70\\x06.04 46.1\\x06.1 71.3\\x06.2 2.45\\x06.02\\nGPT-2 L (PreLayer)* 0.77M 70.3 8.85 46.2 71.7 2.47\\nGPT-2 L (LoRA) 0.77M 70.4\\x06.18.89\\x06.02 46.8\\x06.2 72.0\\x06.2 2.47\\x06.02\\nTable 3: GPT-2 medium (M) and large (L) with different adaptation methods on the E2E NLG\\nChallenge. For all metrics, higher is better. LoRA outperforms several baselines with comparable\\nor fewer trainable parameters. Conï¬dence intervals are shown for experiments we ran. * indicates\\nnumbers published in prior works.\\n5.2 R OBERT A BASE /LARGE\\nRoBERTa (Liu et al., 2019) optimized the pre-training recipe originally proposed in BERT (Devlin\\net al., 2019a) and boosted the latterâ€™s task performance without introducing many more trainable\\nparameters. While RoBERTa has been overtaken by much larger models on NLP leaderboards\\nsuch as the GLUE benchmark (Wang et al., 2019) in recent years, it remains a competitive and\\npopular pre-trained model for its size among practitioners. We take the pre-trained RoBERTa base\\n(125M) and RoBERTa large (355M) from the HuggingFace Transformers library (Wolf et al., 2020)\\nand evaluate the performance of different efï¬cient adaptation approaches on tasks from the GLUE\\nbenchmark. We also replicate Houlsby et al. (2019) and Pfeiffer et al. (2021) according to their',\n",
              "  'question': 'Given that LoRA outperforms several baselines with comparable or fewer trainable parameters on the E2E NLG Challenge, what is the mechanism used by LoRA to achieve this performance?\\n\\n',\n",
              "  'answer': 'LoRA (Layer-wise Adaptive Representation) is a technique that applies a linear projection to the weight matrices of a pre-trained model, allowing the model to adapt to a specific task without the need for extensive re-training. This adaptation is done layer-wise, hence the name \"Layer-wise\". By adjusting the weight matrices, LoRA is able to modify the model\\'s representation of the input and output, enabling it to better capture the features relevant to the task at hand. This allows LoRA to achieve high performance on the E2E NLG Challenge with fewer trainable parameters compared to other methods.',\n",
              "  'source_doc': 'LoRA_ Low-Rank Adaptation of Large Language Models.pdf'},\n",
              " {'context': 'that can perform simple numeric calculations; we\\nonly support the four basic arithmetic operations.\\nResults are always rounded to two decimal places.\\nWikipedia Search Our third tool is a search en-\\ngine that, given a search term, returns short textsnippets from Wikipedia. Compared to our ques-\\ntion answering tool, this search enables a model\\nto get more comprehensive information on a sub-\\nject, but requires it to extract the relevant parts by\\nitself. As our search engine, we use a BM25 re-\\ntriever (Robertson et al., 1995; Baeza-Yates et al.,\\n1999) that indexes the Wikipedia dump from KILT\\n(Petroni et al., 2021).\\nMachine Translation System Our fourth tool is\\na machine translation system based on a LM that\\ncan translate a phrase from any language into En-\\nglish. More concretely, we use the 600M parameter\\nNLLB (Costa-jussÃ  et al., 2022) as our multilingual\\nmachine translation model that works for 200 lan-\\nguages (including low-resource ones). The source\\nlanguage is automatically detected using the fast-\\nTextclassiï¬er (Joulin et al., 2016), while the target\\nlanguage is always set to English.\\nCalendar Our ï¬nal tool is a calendar API that,\\nwhen queried, returns the current date without tak-\\ning any input. This provides temporal context for\\npredictions that require some awareness of time.\\n4 Experiments\\nWe investigate whether our approach enables a\\nmodel to use tools without any further supervision\\nand to decide for itself when and how to call which\\nof the available tools. To test this, we select a vari-\\nety of downstream tasks where we assume at least\\none of the considered tools to be useful, and evalu-\\nate performance in zero-shot settings (Section 4.2).\\nBeyond that, we also ensure that our approach does\\nnot hurt the modelâ€™s core language modeling abili-\\nties; we verify this by looking at perplexity on two\\nlanguage modeling datasets (Section 4.3). Finally,\\nwe investigate how the ability to learn using tools\\nis affected by model size (Section 4.4).\\n4.1 Experimental Setup',\n",
              "  'question': 'Can you explain how the use of tools can improve the performance of a language model on various downstream tasks?\\n\\n',\n",
              "  'answer': \"The use of tools can improve the performance of a language model on various downstream tasks by providing more comprehensive information and enabling the model to perform specific calculations or translations. For example, the use of a Wikipedia search engine can provide more information on a subject, while a machine translation system can translate a phrase from any language into English. These tools can enhance the model's abilities to understand and generate language, leading to better performance on various downstream tasks.\",\n",
              "  'source_doc': 'Toolformer_ Language Models Can Teach Themselves to Use Tools.pdf'},\n",
              " {'context': 'Brazau had argued that he did not intend to promote hate speech; instead he wanted to\\nstimulate debate about censorship, \"blasphemy laws\" and Sharia law, Clements said. Article\\nContinued Below Clements disagreed particulawith Brazauâ€™s argument that people should not be allowed to speak in a respectful manner\\nabout public information and even the workings of mosques under the guise of religion. He\\nsaid that parties to these principles must have their act out with civility training | in\\nother words, someone is responsible to their location. Clements also agreed with Brazauâ€™s\\ncounsel that these are disagreements within the boundaries of the courtroom. \"Based on the\\ndetails in his core prosecution, the Attorney General has established that a search warrant\\nwas issued to his home located in the Brampton area of Greater Toronto prior to this arrest\\nwhich led to a search warrant being executed,\" Clements wrote in his ruling. Clements\\nadded that prosecutors obtained emergency caution letters sent to him last week noting\\nthe violence in the community. Quebec Court Justice C. David Jones wrote for Clements\\nthat \"treat of hatred on members of a religion is a systematic part of a larger system of\\nreligious policing and a serious mistake.\" Jones said after Brazauâ€™s conviction that his\\nconviction would only worsen if a further trial was to be opened. A judge in the Woodin\\nRegional Court recently recommended Brazau be held without bail to avoid a conviction or to\\navoid detention pending a potential appeal. Read more about:<|endoftext|>\\n<BOS> Ã—Some Seattle businesses closed for â€™A Day Without Immigrantsâ€™, but others decided\\nagainst it SEATTLE | While there is no official list of local businesses participating\\nin this movement, \"A Day Without Immigrants\", we did find some businesses that had closed\\ntheir doors and posted signs up saying they were participating. In fact, one business\\nowner says heâ€™s doing things a little bit differently. Instead of shutting down, he says',\n",
              "  'question': 'How is the concept of censorship related to the context provided?\\n\\n',\n",
              "  'answer': 'The context discusses the issue of censorship in relation to religious beliefs and practices. The Brazilian activist, Brazau, did not intend to promote hate speech but wanted to stimulate debate about censorship, \"blasphemy laws,\" and Sharia law. The Canadian judge, Clements, disagreed with Brazau\\'s argument that people should not be allowed to speak in a respectful manner about public information and even the workings of mosques under the guise of religion. Instead, Clements believed that parties to these principles must have their acts out with civility training. This suggests that there is a need for balanced and respectful dialogue around these issues, rather than censorship or suppression of certain beliefs or practices.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'ï¬ndings across several risk areas below.\\nWhile some of our ï¬ndings about the potential societal\\nimpacts of code generation systems were informed by work\\ntowards responsible deployment of the production-oriented\\nCodex models (which descended from the research-oriented\\nCodex models described in this paper), this section is not\\nintended to provide a full account of any particular productâ€™s\\nsafety features. Unless otherwise speciï¬ed, we anchor our\\nanalysis in the speciï¬c properties of the models described\\nin this paper. We share this analysis in the belief that some\\nof it generalizes to the broader class of code generation\\nsystems, and to encourage a norm of performing detailed\\nimpact analysis as part of major machine learning research\\nprojects.\\nNote that by focusing largely on risks in this section, we do\\nnot mean to imply that we expect the impact of this class of\\ntechnologies to be net-negative; rather, risks merit particular\\nattention here because they may be subtle or require deliber-\\nate effort to address, whereas we expect the beneï¬ts to be\\nmore obvious and â€œautomaticâ€ from the perspective of most\\nusers and affected stakeholders.\\n7.1. Over-reliance\\nOne of the key risks associated with using code generation\\nmodels in practice is over-reliance on generated outputs.\\nDue to the limitations described above as well as alignment\\nissues described below, Codex may suggest solutions that\\nsuperï¬cially appear correct but do not actually perform the\\ntask the user intended. This could particularly affect novice\\nprogrammers, and could have signiï¬cant safety implications\\ndepending on the context. We discuss a related issue in\\nAppendix G, namely that code generation models can sug-\\ngest insecure code. For these reasons, human oversight and\\nvigilance is required for safe use of code generation systems\\nlike Codex.\\nWe note several immediate ways to improve safety in the\\nsubsection on risk mitigation below, though over-reliance',\n",
              "  'question': 'Given the context, what are the potential societal impacts of code generation systems, and how can they be mitigated?\\n\\n',\n",
              "  'answer': 'Code generation systems have the potential to impact society in various ways, including over-reliance on generated outputs, alignment issues, and the suggestion of insecure code. To mitigate these risks, human oversight and vigilance are required. Additionally, immediate ways to improve safety include improving the quality of the models, providing better guidance to users, and enhancing the ability to detect and correct errors. It is important to perform detailed impact analysis as part of major machine learning research projects to ensure the safe and responsible deployment of code generation systems.',\n",
              "  'source_doc': 'Evaluating Large Language Models Trained on Code.pdf'},\n",
              " {'context': 'customers. Weâ€™re committed to winning in every community to be a leader in theTable 8: Last Model Prompt and Completion for the 1024 MLE trained model. In this setting we simply\\ngenerate sequences based on the prompt. Continued from the previous page.\\nPrompt Completion\\nNorth Korean leader Kim Jong Un. AP Images / Business Insider North Korea attempted\\nto fire a missile Sunday, but it blew up within seconds. It happened one day after\\nthe anniversary of the countryâ€™s founding. While North Koreaâ€™s missile program may be\\nthe shadowiest on earth, itâ€™s possible that US cyber warriors were the reason for the\\nfailed launch. A recent New York Times report uncovered a Kham secret operation to\\nderail North Koreaâ€™s nuclear-missile program that has been raging for at least three\\nyears. Essentially, the report attributes North Koreaâ€™s high rate of failure with\\nRussian-designed missiles to the US meddling in the countryâ€™s missile software and\\nnetworks. Although North Koreaâ€™s missile infrastructure lacks the competence of Russiaâ€™s,\\nthe Soviet-era missile on which North Korea based its missile had a 13 rotten% failure\\nrate, and the North Korean version failed a whopping 88% of the time, according to the\\nreport. While the missile failure on Sunday could have just been due to poor workmanship,\\nUS Deputy National Security Adviser K.T. McFarland seemed to leave room for speculation\\nabout espionage, telling Fox News, \"We canâ€™t talk about secret intelligence and things\\nthat might have been doneto defame someone. Iâ€™m just not a spy.\" Even the hacking incident did come with some\\ninitial dissent, in which North Korean leader Kim Jong Un accused Obama of \"cheating\"\\nby overthrowing the countryâ€™s democratically elected government. He also said Obama\\nshould postpone a nuclear test to four years, as well as a lingering diplomatic hostage\\nsituation. \"Iâ€™ve been trying to convince the American people that the nuclear submarine',\n",
              "  'question': \"How do the United States' actions contribute to North Korea's high rate of failure with Russian-designed missiles?\\n\\n\",\n",
              "  'answer': 'The United States\\' actions contribute to North Korea\\'s high rate of failure with Russian-designed missiles by meddling in the country\\'s missile software and networks, according to a recent New York Times report. This has led to a failure rate of 88% for North Korea\\'s version of the Soviet-era missile on which it is based, compared to a 13% failure rate for the original Soviet missile. US Deputy National Security Adviser K.T. McFarland has left room for speculation about espionage, but initial dissent came from North Korean leader Kim Jong Un, who accused Obama of \"cheating\" by overthrowing the country\\'s democratically elected government and called for a postponement of a nuclear test and a lingering diplomatic hostage situation.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'retrieve from. We report the top-k retrieval accuracy, i.e.the number of questions for which at least one of\\nthe top-k passages contain the answer.\\nSecond, we use the BEIR benchmark, introduced by Thakur et al. (2021), which contains 18 retrieval datasets,\\ncorresponding to nine tasks, such as fact checking or citation prediction, and covering diï¬€erent domains, such\\nas Wikipedia or scientiï¬c publications. Most datasets from BEIR do not contain a training set, and the focus\\nof the benchmark is zero-shot retrieval . However, most machine learning based retrievers are still trained\\non supervised data, such as the large scale retrieval dataset MS MARCO (Bajaj et al., 2016). Following\\nstandard practice, we report two metrics on this benchmark: nDCG@10 and Recall@100. The nDCG@10\\nfocuses on the ranking of the top 10 retrieved documents, and is good at evaluating rankings returned to\\nhumans, for example in a search engine. On the other hand, Recall@100 is relevant to evaluate retrievers that\\nare used in machine learning systems, such as question answering. Indeed, such models can process hundreds\\nof documents, and ignore their ranking (Izacard & Grave, 2020b). While nDCG@10 is the main metric of\\nBEIR, we are more interested in the Recall@100 to evaluate bi-encoders, as our goal is to develop retrievers\\nthat can be used in ML systems. Moreover, in many settings, retrieved documents can be re-ranked with a\\nmore powerful model such as a cross-encoder, thus improving the nDCG@10.\\n6Published in Transactions on Machine Learning Research (08/2022)\\nTable 1:Unsupervised recall@k on the test sets of NaturalQuestions and TriviaQA. For Inverse Cloze\\nTask and Masked Salient Spans we report the results of Sachan et al. (2021). The Masked Salient Spans\\nmodel uses annotated named entity recognition data. For BM25 we report the results of Ma et al. (2021)\\nNaturalQuestions TriviaQA\\nR@5 R@20 R@100 R@5 R@20 R@100\\nInverse Cloze Task (Sachan et al., 2021) 32.3 50.9 66.8 40.2 57.5 73.6',\n",
              "  'question': 'How is the evaluation of machine learning-based retrievers different from that of supervised data-based retrievers on the BEIR benchmark?\\n\\n',\n",
              "  'answer': 'The evaluation of machine learning-based retrievers on the BEIR benchmark focuses on Recall@100, while the evaluation of supervised data-based retrievers focuses on nDCG@10. Recall@100 is relevant to evaluate retrievers that are used in machine learning systems, such as question answering, as these models can process hundreds of documents and ignore their ranking. On the other hand, nDCG@10 is the main metric of BEIR and is good at evaluating rankings returned to humans, such as in a search engine. While nDCG@10 is important, the focus on Recall@100 allows for a better evaluation of bi-encoders, which is the goal of the current study.',\n",
              "  'source_doc': 'Unsupervised Dense Information Retrieval with Contrastive Learning.pdf'},\n",
              " {'context': 'Megan Leszczynski, Laurel Orr, Yuhuai Wu, Beidi Chen, and Xun Huang for their constructive feedback and\\nsuggestions on early drafts of the paper. We thank Markus Rabe and Charles Staats for helpful discussion of\\ntheir attention algorithm.\\nWe gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos.\\nCCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ARL under\\nNo. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000141712266 (Unifying Weak\\nSupervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine\\nLearning; N000142012275 (NEPTUNE); NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba,\\nTSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce,\\nTotal, the HAI-GCP & HAI-Azure Cloud Credits for Research program, the Stanford Data Science Initiative\\n(SDSI), Department of Defense (DoD) through the National Defense Science and Engineering Graduate\\nFellowship (NDSEG) Program, and members of the Stanford DAWN project: Facebook, Google, and\\nVMWare. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes\\n10notwithstanding any copyright notation thereon. Any opinions, ï¬ndings, and conclusions or recommendations\\nexpressed in this material are those of the authors and do not necessarily reï¬‚ect the views, policies, or\\nendorsements, either expressed or implied, of NIH, ONR, or the U.S. Government. Atri Rudraâ€™s research is\\nsupported by NSF grant CCF-1763481.\\nR',\n",
              "  'question': 'How does the paper propose a new attention algorithm for machine learning?\\n\\n',\n",
              "  'answer': 'The paper proposes a new attention algorithm for machine learning by combining the advantages of both sparse and dense attention mechanisms. The algorithm first generates dense attention weights for each token in the input sequence, which captures the relative importance of each token to the output. Then, it applies sparse attention to select a subset of tokens that are most relevant to the output, based on their dense attention weights. The final output is a weighted sum of the input tokens, where the weights are determined by the sparse attention mechanism. This approach allows the model to selectively focus on different parts of the input sequence, depending on the specific task and context.',\n",
              "  'source_doc': 'FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf'},\n",
              " {'context': 'length if they can do so for a chain of length two.\\nFigure 11. Pass rates of Codex-12B samples against the number of\\nchained components in the synthetically generated docstring. With\\neach additional component, pass rate drops by roughly a factor of\\n2-3.\\nFurther, just as text-conditional generative models in other\\nmodalities (Ramesh et al., 2021) have difï¬culty with bind-\\ning attributes to objects, Codex can make mistakes binding\\noperations to variables, especially when the number of oper-\\nations and variables in the docstring is large. For instance,\\nin the following prompt, Codex-12B does not decrement the\\nvariable w and also fails to return the product of all numbers.\\ndef do_work(x, y, z, w):\\n\"\"\" Add 3 to y, then subtract 4\\nfrom both x and w. Return the\\nproduct of the four numbers. \"\"\"\\nt = y + 3\\nu = x - 4\\nv = z *w\\nreturn v\\nThis understanding of Codexâ€™s limited system-level synthe-\\nsis capabilities helps inform our assessment of the potential\\nhazards of using it in a generative capacity, as well as the\\nbroader societal impacts that such systems could have.\\n7. Broader Impacts and Hazard Analysis\\nCodex has the potential to be useful in a range of ways.\\nFor example, it could help onboard users to new codebases,\\nreduce context switching for experienced coders, enable\\nnon-programmers to write speciï¬cations and have Codex\\ndraft implementations, and aid in education and exploration.\\nHowever, Codex also raises signiï¬cant safety challenges,\\ndoes not always produce code that is aligned with user intent,Evaluating Large Language Models Trained on Code\\nand has the potential to be misused.\\nTo better understand some of the hazards of using Codex\\nin a generative capacity, we conducted a hazard analysis\\nfocused on identifying risk factors (Leveson, 2019) with\\nthe potential to cause harm.1We outline some of our key\\nï¬ndings across several risk areas below.\\nWhile some of our ï¬ndings about the potential societal\\nimpacts of code generation systems were informed by work',\n",
              "  'question': 'Given the pass rate drops and difficulty with binding operations to variables in Codex-12B, what are the potential hazards of using Codex in a generative capacity and how can they be mitigated?\\n\\n',\n",
              "  'answer': 'The potential hazards of using Codex in a generative capacity include safety challenges, misalignment with user intent, and the potential for misuse. To mitigate these hazards, it is important to conduct a thorough hazard analysis focused on identifying risk factors that have the potential to cause harm. Some key findings across several risk areas include the need for careful consideration of the input data, the importance of testing and validation, and the need for clear guidelines and regulations for the use of code generation systems. Additionally, ongoing monitoring and evaluation of the performance and impact of Codex and other similar systems can help identify and address any potential hazards as they arise.',\n",
              "  'source_doc': 'Evaluating Large Language Models Trained on Code.pdf'},\n",
              " {'context': 'scheme by Shum et al. [60] generates a pool of CoT candi-\\ndates, and selects the best candidate based on whether the\\ncandidates match the ground truth and on a policy gradient-\\nbased method. Automatic prompt generation overcomes the\\nissues of scaling in CoT [58, 42, 41]. Zhou et al. proposes to\\nharness selecting the best prompt out of a candidate set [83].\\nFinally, in prompt chaining, one cascades different LLMs.\\nThis enables prompting different LLMs via different con-\\nIO CoT ToT GoT GoT202468Score (out of 10); the higher the better\\n03691215\\nTotal Cost ($); the lower the betterL=3\\nk=10Aggregation of fully\\nmerged NDAs\\nAggregation\\nof partially\\nmerged\\nNDAsFigure 8: Score and cost in document merging with\\nChatGPT-3.5. Landkindicate the structure of ToT (see Sec-\\ntions 3.2 and 6). Number of samples: 50; context size: 16k\\ntokens.\\ntexts, enabling more powerful reasoning [21, 47, 72, 23, 50,\\n71, 72]. GoT is orthogonal to this class of schemes, as it\\nfocuses on a single context capabilities.\\n8.2 Self-Reflection & Self-Evaluation\\nSelf-reflection and self-evaluation were introduced re-\\ncently [59, 48, 45, 74]. They are used to enhance differ-\\nent tasks, for example for code generation [17] or com-\\nputer operation tasks [39]. In GoT, we partially rely on\\nself-evaluation when taking decisions on how to expand the\\ngraph of thoughts within a prompt.\\n8.3 LLMs & Planning\\nThere are many works recently on how to plan complex\\ntasks with LLMs [36, 80, 77, 75, 67, 37]. GoT could be seenas a generic framework that could potentially be used to en-\\nhance such schemes, by offering a paradigm for generating\\ncomplex graph-based plans.\\n8.4 Graphs and Graph Computing\\nGraphs have become an immensely popular and important\\npart of the general computing landscape [44, 46, 32, 31, 55].\\nRecently, there has been a growing interest in domains such\\nas graph databases [53, 54, 11, 4, 5, 8], graph pattern match-\\ning [25, 18, 61, 10, 2, 1], graph streaming [26, 22, 3],',\n",
              "  'question': 'In \"GoT: A Graph-Based Approach to Task Decomposition and Automatic Prompt Generation\" by Shum et al., what is the main focus of the paper?\\n\\n',\n",
              "  'answer': 'The main focus of the paper is on the development of a graph-based approach to task decomposition and automatic prompt generation called GoT (Graph-based Task Decomposition). GoT generates a pool of candidate LLMs and selects the best candidate based on whether the candidates match the ground truth and on a policy gradient-based method. The paper also introduces self-reflection and self-evaluation to enhance different tasks and proposes to harness selecting the best prompt out of a candidate set to enable more powerful reasoning. Furthermore, the paper explores the use of graphs and graph computing in GoT as a generic framework for generating complex graph-based plans.',\n",
              "  'source_doc': 'Graph of Thoughts_ Solving Elaborate Problems with Large Language Models.pdf'},\n",
              " {'context': 'Adapting unfamiliar inputs to frozen pretrained models. arXiv preprint arXiv:2203.03131 ,\\n2022.\\n[2]A. Askell, Y . Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann,\\nN. DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint\\narXiv:2112.00861 , 2021.\\n[3]S. H. Bach, V . Sanh, Z.-X. Yong, A. Webson, C. Raffel, N. V . Nayak, A. Sharma, T. Kim, M. S.\\nBari, T. Fevry, et al. Promptsource: An integrated development environment and repository for\\nnatural language prompts. arXiv preprint arXiv:2202.01279 , 2022.\\n[4]Y . Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli,\\nT. Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from\\nhuman feedback. arXiv preprint arXiv:2204.05862 , 2022.\\n[5]Y . Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirho-\\nseini, C. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint\\narXiv:2212.08073 , 2022.\\n[6]E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic\\nparrots: Can language models be too big? In Proceedings of the 2021 ACM conference on\\nfairness, accountability, and transparency , pages 610â€“623, 2021.\\n[7]S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. Oâ€™Brien, E. Hallahan, M. A. Khan,\\nS. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models\\nacross training and scaling. arXiv preprint arXiv:2304.01373 , 2023.\\n[8]R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein,\\nJ. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models.\\narXiv preprint arXiv:2108.07258 , 2021.\\n[9]T. Chen, B. Xu, C. Zhang, and C. Guestrin. Training deep nets with sublinear memory cost.\\narXiv preprint arXiv:1604.06174 , 2016.\\n[10] W.-L. Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y . Zhuang, J. E.',\n",
              "  'question': 'What challenges do researchers face when adapting unfamiliar inputs to frozen pretrained models and how can they overcome them?\\n\\n',\n",
              "  'answer': 'Researchers face challenges such as lack of transferability of knowledge learned by pretrained models and difficulty in fine-tuning them on new tasks. To overcome these challenges, researchers use techniques such as transfer learning, domain adaptation, and data augmentation. Additionally, they use large-scale pretrained models that have been trained on diverse data to improve their adaptability to new inputs.',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': 'parameters. On GPT-3 175B, we reduce the VRAM consumption during training from 1.2TB to\\n350GB. With r= 4and only the query and value projection matrices being adapted, the checkpoint\\nsize is reduced by roughly 10,000 \\x02(from 350GB to 35MB)4. This allows us to train with signiï¬-\\ncantly fewer GPUs and avoid I/O bottlenecks. Another beneï¬t is that we can switch between tasks\\nwhile deployed at a much lower cost by only swapping the LoRA weights as opposed to all the\\nparameters. This allows for the creation of many customized models that can be swapped in and out\\non the ï¬‚y on machines that store the pre-trained weights in VRAM. We also observe a 25% speedup\\nduring training on GPT-3 175B compared to full ï¬ne-tuning5as we do not need to calculate the\\ngradient for the vast majority of the parameters.\\nLoRA also has its limitations. For example, it is not straightforward to batch inputs to different tasks\\nwith different AandBin a single forward pass, if one chooses to absorb AandBintoWto eliminate\\nadditional inference latency. Though it is possible to not merge the weights and dynamically choose\\nthe LoRA modules to use for samples in a batch for scenarios where latency is not critical.\\n5 E MPIRICAL EXPERIMENTS\\nWe evaluate the downstream task performance of LoRA on RoBERTa (Liu et al., 2019), De-\\nBERTa (He et al., 2021), and GPT-2 (Radford et al., b), before scaling up to GPT-3 175B (Brown\\net al., 2020). Our experiments cover a wide range of tasks, from natural language understanding\\n(NLU) to generation (NLG). Speciï¬cally, we evaluate on the GLUE (Wang et al., 2019) benchmark\\nfor RoBERTa and DeBERTa. We follow the setup of Li & Liang (2021) on GPT-2 for a direct com-\\nparison and add WikiSQL (Zhong et al., 2017) (NL to SQL queries) and SAMSum (Gliwa et al.,\\n2019) (conversation summarization) for large-scale experiments on GPT-3. See Appendix C for\\nmore details on the datasets we use. We use NVIDIA Tesla V100 for all experiments.\\n5.1 B ASELINES',\n",
              "  'question': \"Given LoRA's ability to reduce the checkpoint size and improve training speed, can you explain how LoRA can be used to create many customized models that can be swapped in and out on machines that store the pre-trained weights in VRAM?\\n\\n\",\n",
              "  'answer': 'LoRA can be used to create many customized models that can be swapped in and out on machines that store the pre-trained weights in VRAM by only swapping the LoRA weights as opposed to all the parameters. This allows for the creation of many customized models that can be swapped in and out on the fly on machines that store the pre-trained weights in VRAM.',\n",
              "  'source_doc': 'LoRA_ Low-Rank Adaptation of Large Language Models.pdf'},\n",
              " {'context': 'A key source of DSPyâ€™s expressive power is its ability to compileâ€”or automatically optimizeâ€”any\\nprogram in this programming model. Compiling relies on a teleprompter, which is an optimizer for\\nDSPy programs that improves the quality (or cost) of modules via prompting or finetuning, which\\nare unified in DSPy. While DSPy does not enforce this when creating new teleprompters, typical\\nteleprompters go through three stages.\\nStage 1: Candidate Generation The compiler first (recursively) finds all unique Predict modules\\n(predictors) in a program, including those nested under other modules. For each unique predictor\\np, the teleprompter may generate candidate values for the parameters of p: the instructions, field\\ndescriptions, orâ€”most importantlyâ€”demonstrations (i.e., example inputâ€“output pairs). In this iter-\\n6Preprint\\nation of DSPy, we focus on demonstrations and find that simple rejection-sampling-like approaches\\ncan help bootstrap highly effective multi-stage systems.\\nConsider the simplest non-trivial teleprompter in DSPy, BootstrapFewShot (simplified pseudocode\\nin Appendix E.1). This teleprompter will simulate a teacher program (or, if unset, the zero-shot\\nversion of the program being compiled) on some training inputs, possibly one or more times with\\na high temperature. When running in compile mode, multi-stage traces are tracked transparently\\nand in a thread-safe fashion throughout execution. The programâ€™s metric is used to filter for multi-\\nstage traces that together help the pipeline pass the metric. We thus obtain potential labels for all\\nsignatures in the program by throwing away the bad examples and using the good examples as\\npotential demonstrations, though these design decisions are under user control.\\nWhile LMs can be highly unreliable, we find they can be rather efficient at searching the space\\nof solutions for multi-stage designs. A well-decomposed program can typically find at least a few',\n",
              "  'question': 'How does the BootstrapFewShot teleprompter in DSPy use rejection sampling to generate candidate values for the parameters of predictors?\\n\\n',\n",
              "  'answer': \"The BootstrapFewShot teleprompter in DSPy uses rejection sampling to generate candidate values for the parameters of predictors by simulating a teacher program on some training inputs, possibly one or more times with a high temperature. When running in compile mode, multi-stage traces are tracked transparently and in a thread-safe fashion throughout execution. The program's metric is used to filter for multi-stage traces that together help the pipeline pass the metric. We thus obtain potential labels for all signatures in the program by throwing away the bad examples and using the good examples as potential demonstrations, though these design decisions are under user control.\",\n",
              "  'source_doc': 'DSPy_  Compiling Declarative Language Model Calls into Self-Improving Pipelines.pdf'},\n",
              " {'context': '\\x03Correspondence to mnye@mit.edu . Max is a student at MIT, but primarily did this work while visiting\\nGoogle Research.\\n1arXiv:2112.00114v1  [cs.LG]  30 Nov 2021Large Language Model  state: {}   line: def remove_Occ(s,ch):    state: {\"remove_Occ\": \"<callable_object remove_Occ>\"}   line: output = remove_Occ(\"PHP\",\"P\")   state: {\"ch\": \"P\", \"s\": \"PHP\"}   line:     for i in range(len(s)):    state: {\"ch\": \"P\", \"s\": \"PHP\", \"i\": 0}   line:         if (s[i] == ch):    state: {\"ch\": \"P\", \"s\": \"PHP\", \"i\": 0}   line:             s = s[0 : i] + s[i + 1:]    state: {\"ch\": \"P\", \"s\": \"HP\", \"i\": 0}   line:             break   state: {\"ch\": \"P\", \"s\": \"HP\", \"i\": 0}   line:     for i in range(len(s) - 1,-1,-1):     state: {\"ch\": \"P\", \"s\": \"HP\", \"i\": 1}   line:         if (s[i] == ch):    state: {\"ch\": \"P\", \"s\": \"HP\", \"i\": 1}   line:             s = s[0 : i] + s[i + 1:]    state: {\"ch\": \"P\", \"s\": \"H\", \"i\": 1}   line:             break   state: {\"ch\": \"P\", \"s\": \"H\", \"i\": 1}   line:     return s    state: {\"remove_Occ\": \"<callable_object remove_Occ>\", \"output\": \"H\"}    Consider the following Python function:          def remove_Occ(s,ch):          for i in range(len(s)):              if (s[i] == ch):                  s = s[0 : i] + s[i + 1:]                  break         for i in range(len(s) - 1,-1,-1):               if (s[i] == ch):                  s = s[0 : i] + s[i + 1:]                  break         return s           output = remove_Occ(\"PHP\",\"P\")          What is the execution trace?    Consider the following Python function:          def remove_Occ(s,ch):          for i in range(len(s)):              if (s[i] == ch):                  s = s[0 : i] + s[i + 1:]                  break         for i in range(len(s) - 1,-1,-1):               if (s[i] == ch):                  s = s[0 : i] + s[i + 1:]                  break         return s      Fill in the ??? below:     assert remove_Occ(\"PHP\",\"P\") == ???  assert remove_Oct(\"PHP\", \"P\") == \"H\"DIRECT EXECUTION PREDICTION',\n",
              "  'question': 'What is the execution trace of the `remove_Occ` function when called with input (\"PHP\", \"P\")?\\n\\n',\n",
              "  'answer': 'The execution trace of the `remove_Occ` function when called with input (\"PHP\", \"P\") is as follows:\\n\\n1. The function is called with arguments \"PHP\" and \"P\".\\n2. The variable `s` is assigned the value \"PHP\".\\n3. The variable `ch` is assigned the value \"P\".\\n4. The for loop iterates through the characters in the string \"PHP\".\\n5. The first character in the string is \"P\", so the loop breaks and the variable `s` is assigned the value \"HP\".\\n6. The for loop iterates through the characters in the string \"HP\" in reverse order.\\n7. The last character in the string is \"P\", so the loop breaks and the variable `s` is assigned the value \"H\".\\n8. The function returns the value \"H\".\\n\\nTherefore, the execution trace of the `remove_Occ` function when called with input (\"PHP\", \"P\") is \"HP\".',\n",
              "  'source_doc': 'Show Your Work_  Scratchpads for Intermediate Computation with Language Models.pdf'},\n",
              " {'context': 'Parisi et al., 2022), where models are provided\\nwith dataset-speciï¬c examples of how a tool can be\\nused to solve a concrete task. We choose the more\\nchallenging zero-shot setup as we are interested\\nin seeing whether Toolformer works in precisely\\nthose cases where a user does not specify in ad-\\nvance which tools should be used in which way for\\nsolving a speciï¬c problem.\\nWe use standard greedy decoding, but with one\\nmodiï¬cation for Toolformer: We let the model start\\nan API call not just when <API> is the most likely\\n5This is achieved by manually setting the probability of\\nthe<API> token to 0.\\n6We use the original davinci variant that is not ï¬netuned\\non any instructions.token, but whenever it is one of the kmost likely\\ntokens. For k= 1, this corresponds to regular\\ngreedy decoding; we instead use k= 10 to in-\\ncrease the disposition of our model to make use of\\nthe APIs that it has access to. At the same time,\\nwe only at most one API call per input to make\\nsure the model does not get stuck in a loop where\\nit constantly calls APIs without producing any ac-\\ntual output. The effect of these modiï¬cations is\\nexplored in Section 5.\\n4.2.1 LAMA\\nWe evaluate our models on the SQuAD, Google-\\nRE and T-REx subsets of the LAMA benchmark\\n(Petroni et al., 2019). For each of these subsets, the\\ntask is to complete a short statement with a miss-\\ning fact (e.g., a date or a place). As LAMA was\\noriginally designed to evaluate masked language\\nmodels (e.g., Devlin et al., 2019), we ï¬lter out ex-\\namples where the mask token is not the ï¬nal token,\\nso that the remaining examples can be processed\\nin a left-to-right fashion. To account for different\\ntokenizations and added complexity from not in-\\nforming the model that a single word is required,\\nwe use a slightly more lenient evaluation criterion\\nthan exact match and simply check whether the\\ncorrect word is within the ï¬rst ï¬ve words predicted\\nby the model. As LAMA is based on statements\\nobtained directly from Wikipedia, we prevent Tool-',\n",
              "  'question': 'In the context of Parisi et al., 2022, how does the use of the <API> token in the greedy decoding process affect the performance of the Toolformer model on the SQuAD, Google-RE, and T-REx subsets of the LAMA benchmark?\\n\\n',\n",
              "  'answer': 'The use of the <API> token in the greedy decoding process allows the Toolformer model to increase its disposition to make use of the APIs that it has access to, which can improve its performance on the SQuAD, Google-RE, and T-REx subsets of the LAMA benchmark. However, to prevent the model from getting stuck in a loop where it constantly calls APIs without producing any actual output, the authors of Parisi et al., 2022, limit the number of API calls per input to one.',\n",
              "  'source_doc': 'Toolformer_ Language Models Can Teach Themselves to Use Tools.pdf'},\n",
              " {'context': '34B in mathematics, code generation, and reasoning benchmarks.\\nModel Modality MMLU HellaSwag WinoG PIQA Arc-e Arc-c NQ TriviaQA HumanEval MBPP MATH GSM8K\\nLLaMA 2 7B Pretrained 44.4% 77.1% 69.5% 77.9% 68.7% 43.2% 24.7% 63.8% 11.6% 26.1% 3.9% 16.0%\\nLLaMA 2 13B Pretrained 55.6% 80.7% 72.9% 80.8% 75.2% 48.8% 29.0% 69.6% 18.9% 35.4% 6.0% 34.3%\\nCode-Llama 7B Finetuned 36.9% 62.9% 62.3% 72.8% 59.4% 34.5% 11.0% 34.9% 31.1% 52.5% 5.2% 20.8%\\nMistral 7B Pretrained 60.1% 81.3% 75.3% 83.0% 80.0% 55.5% 28.8% 69.9% 30.5% 47.5% 13.1% 52.2%\\nTable 2: Comparison of Mistral 7B with Llama. Mistral 7B outperforms Llama 2 13B on all metrics, and\\napproaches the code performance of Code-Llama 7B without sacrificing performance on non-code benchmarks.\\nSize and Efficiency. We computed â€œequivalent model sizesâ€ of the Llama 2 family, aiming to\\nunderstand Mistral 7B modelsâ€™ efficiency in the cost-performance spectrum (see Figure 5). When\\nevaluated on reasoning, comprehension, and STEM reasoning (specifically MMLU), Mistral 7B\\nmirrored performance that one might expect from a Llama 2 model with more than 3x its size. On\\nthe Knowledge benchmarks, Mistral 7Bâ€™s performance achieves a lower compression rate of 1.9x,\\nwhich is likely due to its limited parameter count that restricts the amount of knowledge it can store.\\nEvaluation Differences. On some benchmarks, there are some differences between our evaluation\\nprotocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2)\\non TriviaQA, we do not provide Wikipedia contexts.\\n4 Instruction Finetuning\\nModelChatbot Arena\\nELO RatingMT Bench\\nWizardLM 13B v1.2 1047 7.2\\nMistral 7B Instruct 1031 6.84 +/- 0.07\\nLlama 2 13B Chat 1012 6.65\\nVicuna 13B 1041 6.57\\nLlama 2 7B Chat 985 6.27\\nVicuna 7B 997 6.17\\nAlpaca 13B 914 4.53\\nTable 3: Comparison of Chat models. Mistral 7B â€“\\nInstruct outperforms all 7B models on MT-Bench, and\\nis comparable to 13B â€“ Chat models.To evaluate the generalization capabilities of',\n",
              "  'question': 'In comparison to the Llama 2 family, how does the Mistral 7B model perform on reasoning, comprehension, and STEM reasoning benchmarks?\\n\\n',\n",
              "  'answer': 'The Mistral 7B model mirrors the performance of a Llama 2 model with more than 3x its size on reasoning, comprehension, and STEM reasoning benchmarks. However, its performance achieves a lower compression rate of 1.9x, likely due to its limited parameter count that restricts the amount of knowledge it can store.',\n",
              "  'source_doc': 'Mistral7B.pdf'},\n",
              " {'context': 'Ä®Ê«Ê‚Ë¤Å¿ÄµÅ¤Ã²Å\\x89Ä¦ÃŠÄ®Ë¤É¿Ë¤Ë¤Ë¤\\n\\x01Ä®ÃŠÄ¦Æ†ÆÄ‘Ä®ÄˆË¤Ã²ÃŠÃ§ÄË¤Ã§ÄÄµÆ“Ã§Ã²Ë¤Ä‘Ä®Ë¤Ã­Ã²Å¤ÃŠÆ“Ä¦ÊË¤Ë¤\\x1dÄÄµÆ“Ã§Ã²Ë¤É¾ÊœË¤Æ€ÄÆ“Ä¦Ã²Ë¤Ä‘Ä®Ã§ÄµÅ—Å”ÄµÅ—ÃŠÆœÄ‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤Å—Ã²Å–Å©Ä‘Å—Ã²Ã­Ë¤Ã²Ä®Ã­Ë¤ÅÃ²Ä®Å¤Ã²Ä®Ã§Ã²ÅÊœË¤ÅÃ²Ã²Ä­ÅË¤Æ˜ÄµË¤Ä¦ÃŠÃ§Ä£Ë¤ÃŠË¤Ã§Ä¦Ã²ÃŠÅ—Ë¤Ã§ÄµÄ®Ä®Ã²Ã§ÆœÆ“ÄµÄ®Ë¤Ã¦Ã²Å¤Æ€Ã²Ã²Ä®Ë¤Æ›ÄÃ²Ë¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄÅË¤Ê³Ê›Ê›Ê›Ê´Ë¤\\x1dÄÄµÆ“Ã§Ã²Ë¤É¿Ë¤ÄµÆ™Ä‡Ã²Å—ÅË¤ÃŠÄ®Ë¤Ä‘Ä®Å¤Ã²Å—Ã²ÅÆœÄ‘Ä®ÄˆË¤Å”Ã²Å—ÅÅ”Ã²Ã§ÆœÆ“Å¿Ã²Ë¤Ã¦Æ†Ë¤Å©ÅÄ‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤Å—Ã²Å–Å©Ä‘Å—Ã²Ã­Ë¤Ã²Ä®Ã­Ë¤ÅÃ²Ä®Å¤Ã²Ä®Ã§Ã²ÅË¤Æ˜ÄµË¤Å”Å—Ã²ÅÃ²Ä®ÆœË¤ÃŠË¤ÅÃ²Ä¦Æ™ËŠÄÃ²Ä¦Å”Ë¤Ã¦ÄµÄµÄ£Ë™ÅË¤Ã§ÄµÄ®Å¤Ã²Ä®ÆœÊ›Ë¤GÆœË¤Ã§ÄµÄ®Ä®Ã²Ã§Å¤ÅË¤Æ›ÄÃ²Ë¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄÅË¤Æ€Æ“ÆœÄË¤Æ›ÄÃ²Ë¤Æ›ÄÃ²Ä­Ã²Ë¤ÄµÆ™Ë¤ÅÃ²Ä¦Æ™ËŠÄ‘Ä­Å”Å—ÄµÅ¿Ã²Ä­Ã²Ä®ÆœË¤ÃŠÄ®Ã­Ë¤Ã²Ä­Ã¦Å—ÃŠÃ§Ä‘Ä®ÄˆË¤Ã§ÄÃŠÄ¦Ä¦Ã²Ä®ÄˆÃ²ÅÊœË¤Ä­ÃŠÄ£Ä‘Ä®ÄˆË¤Æ—ÄµÅ—Ë¤ÃŠË¤Ã§ÄµÄÃ²Å—Ã²Ä®ÆœË¤Å”ÃŠÅÅÃŠÄˆÃ²Ê›Ë¤Ë¤Ê³Ê›Ê›Ê›Ê´Ë¤\\x9aÄÃ²Ë¤Ã¦Ã²ÅÆœË¤Ã§ÄÄµÆ“Ã§Ã²Ë¤Æ“ÅË¤É¿Ê›\\x01Ä®ÃŠÄ¦Æ†ÆÄ‘Ä®ÄˆË¤Ã²ÃŠÃ§ÄË¤Ã§ÄÄµÆ“Ã§Ã²Ë¤Ä‘Ä®Ë¤Ã­Ã²Å¤ÃŠÆ“Ä¦ÊË¤Ë¤\\x1dÄÄµÆ“Ã§Ã²Ë¤É¾ÊœË¤Æ€ÄÆ“Ä¦Ã²Ë¤Ä‘Ä®Ã§ÄµÅ—Å”ÄµÅ—ÃŠÆœÄ‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤Å—Ã²Å–Å©Ä‘Å—Ã²Ã­Ë¤Ã²Ä®Ã­Ë¤ÅÃ²Ä®Å¤Ã²Ä®Ã§Ã²ÅÊœË¤ÅÃ²Ã²Ä­ÅË¤Æ˜ÄµË¤Ä¦ÃŠÃ§Ä£Ë¤ÃŠË¤Ã§Ä¦Ã²ÃŠÅ—Ë¤Ã§ÄµÄ®Ä®Ã²Ã§ÆœÆ“ÄµÄ®Ë¤Ã¦Ã²Å¤Æ€Ã²Ã²Ä®Ë¤Æ›ÄÃ²Ë¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄÅË¤Ê³Ê›Ê›Ê›Ê´Ë¤\\x1dÄÄµÆ“Ã§Ã²Ë¤É¿Ë¤ÄµÆ™Ä‡Ã²Å—ÅË¤ÃŠÄ®Ë¤Ä‘Ä®Å¤Ã²Å—Ã²ÅÆœÄ‘Ä®ÄˆË¤Å”Ã²Å—ÅÅ”Ã²Ã§ÆœÆ“Å¿Ã²Ë¤Ã¦Æ†Ë¤Å©ÅÄ‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤Å—Ã²Å–Å©Ä‘Å—Ã²Ã­Ë¤Ã²Ä®Ã­Ë¤ÅÃ²Ä®Å¤Ã²Ä®Ã§Ã²ÅË¤Æ˜ÄµË¤Å”Å—Ã²ÅÃ²Ä®ÆœË¤ÃŠË¤ÅÃ²Ä¦Æ™ËŠÄÃ²Ä¦Å”Ë¤Ã¦ÄµÄµÄ£Ë™ÅË¤Ã§ÄµÄ®Å¤Ã²Ä®ÆœÊ›Ë¤GÆœË¤Ã§ÄµÄ®Ä®Ã²Ã§Å¤ÅË¤Æ›ÄÃ²Ë¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄÅË¤Æ€Æ“ÆœÄË¤Æ›ÄÃ²Ë¤Æ›ÄÃ²Ä­Ã²Ë¤ÄµÆ™Ë¤ÅÃ²Ä¦Æ™ËŠÄ‘Ä­Å”Å—ÄµÅ¿Ã²Ä­Ã²Ä®ÆœË¤ÃŠÄ®Ã­Ë¤Ã²Ä­Ã¦Å—ÃŠÃ§Ä‘Ä®ÄˆË¤Ã§ÄÃŠÄ¦Ä¦Ã²Ä®ÄˆÃ²ÅÊœË¤Ä­ÃŠÄ£Ä‘Ä®ÄˆË¤Æ—ÄµÅ—Ë¤ÃŠË¤Ã§ÄµÄÃ²Å—Ã²Ä®ÆœË¤Å”ÃŠÅÅÃŠÄˆÃ²Ê›Ë¤Ë¤Ê³Ê›Ê›Ê›Ê´Ë¤\\x9aÄÃ²Ë¤Ã¦Ã²ÅÆœË¤Ã§ÄÄµÆ“Ã§Ã²Ë¤Æ“ÅË¤É¿Ê›\\x01Ä®ÃŠÄ¦Æ†ÆÄ‘Ä®ÄˆË¤Ã²ÃŠÃ§ÄË¤Ã§ÄÄµÆ“Ã§Ã²Ë¤Ä‘Ä®Ë¤Ã­Ã²Å¤ÃŠÆ“Ä¦ÊË¤Ë¤\\x1dÄÄµÆ“Ã§Ã²Ë¤É¾ÊœË¤Æ€ÄÆ“Ä¦Ã²Ë¤Ä‘Ä®Ã§ÄµÅ—Å”ÄµÅ—ÃŠÆœÄ‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤Å—Ã²Å–Å©Ä‘Å—Ã²Ã­Ë¤Ã²Ä®Ã­Ë¤ÅÃ²Ä®Å¤Ã²Ä®Ã§Ã²ÅÊœË¤ÅÃ²Ã²Ä­ÅË¤Æ˜ÄµË¤Ä¦ÃŠÃ§Ä£Ë¤ÃŠË¤Ã§Ä¦Ã²ÃŠÅ—Ë¤Ã§ÄµÄ®Ä®Ã²Ã§ÆœÆ“ÄµÄ®Ë¤Ã¦Ã²Å¤Æ€Ã²Ã²Ä®Ë¤Æ›ÄÃ²Ë¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄÅË¤Ê³Ê›Ê›Ê›Ê´Ë¤\\x1dÄÄµÆ“Ã§Ã²Ë¤É¿Ë¤ÄµÆ™Ä‡Ã²Å—ÅË¤ÃŠÄ®Ë¤Ä‘Ä®Å¤Ã²Å—Ã²ÅÆœÄ‘Ä®ÄˆË¤Å”Ã²Å—ÅÅ”Ã²Ã§ÆœÆ“Å¿Ã²Ë¤Ã¦Æ†Ë¤Å©ÅÄ‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤Å—Ã²Å–Å©Ä‘Å—Ã²Ã­Ë¤Ã²Ä®Ã­Ë¤ÅÃ²Ä®Å¤Ã²Ä®Ã§Ã²ÅË¤Æ˜ÄµË¤Å”Å—Ã²ÅÃ²Ä®ÆœË¤ÃŠË¤ÅÃ²Ä¦Æ™ËŠÄÃ²Ä¦Å”Ë¤Ã¦ÄµÄµÄ£Ë™ÅË¤Ã§ÄµÄ®Å¤Ã²Ä®ÆœÊ›Ë¤GÆœË¤Ã§ÄµÄ®Ä®Ã²Ã§Å¤ÅË¤Æ›ÄÃ²Ë¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄÅË¤Æ€Æ“ÆœÄË¤Æ›ÄÃ²Ë¤Æ›ÄÃ²Ä­Ã²Ë¤ÄµÆ™Ë¤ÅÃ²Ä¦Æ™ËŠÄ‘Ä­Å”Å—ÄµÅ¿Ã²Ä­Ã²Ä®ÆœË¤ÃŠÄ®Ã­Ë¤Ã²Ä­Ã¦Å—ÃŠÃ§Ä‘Ä®ÄˆË¤Ã§ÄÃŠÄ¦Ä¦Ã²Ä®ÄˆÃ²ÅÊœË¤Ä­ÃŠÄ£Ä‘Ä®ÄˆË¤Æ—ÄµÅ—Ë¤ÃŠË¤Ã§ÄµÄÃ²Å—Ã²Ä®ÆœË¤Å”ÃŠÅÅÃŠÄˆÃ²Ê›Ë¤Ë¤Ê³Ê›Ê›Ê›Ê´Ë¤\\x9aÄÃ²Ë¤Ã¦Ã²ÅÆœË¤Ã§ÄÄµÆ“Ã§Ã²Ë¤Æ“ÅË¤É¿Ê›\\x01Ä®ÃŠÄ¦Æ†ÆÄ‘Ä®ÄˆË¤Ã²ÃŠÃ§ÄË¤Ã§ÄÄµÆ“Ã§Ã²Ë¤Ä‘Ä®Ë¤Ã­Ã²Å¤ÃŠÆ“Ä¦ÊË¤Ë¤\\x1dÄÄµÆ“Ã§Ã²Ë¤É¾ÊœË¤Æ€ÄÆ“Ä¦Ã²Ë¤Ä‘Ä®Ã§ÄµÅ—Å”ÄµÅ—ÃŠÆœÄ‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤Å—Ã²Å–Å©Ä‘Å—Ã²Ã­Ë¤Ã²Ä®Ã­Ë¤ÅÃ²Ä®Å¤Ã²Ä®Ã§Ã²ÅÊœË¤ÅÃ²Ã²Ä­ÅË¤Æ˜ÄµË¤Ä¦ÃŠÃ§Ä£Ë¤ÃŠË¤Ã§Ä¦Ã²ÃŠÅ—Ë¤Ã§ÄµÄ®Ä®Ã²Ã§ÆœÆ“ÄµÄ®Ë¤Ã¦Ã²Å¤Æ€Ã²Ã²Ä®Ë¤Æ›ÄÃ²Ë¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄÅË¤Ê³Ê›Ê›Ê›Ê´Ë¤\\x1dÄÄµÆ“Ã§Ã²Ë¤É¿Ë¤ÄµÆ™Ä‡Ã²Å—ÅË¤ÃŠÄ®Ë¤Ä‘Ä®Å¤Ã²Å—Ã²ÅÆœÄ‘Ä®ÄˆË¤Å”Ã²Å—ÅÅ”Ã²Ã§ÆœÆ“Å¿Ã²Ë¤Ã¦Æ†Ë¤Å©ÅÄ‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤Å—Ã²Å–Å©Ä‘Å—Ã²Ã­Ë¤Ã²Ä®Ã­Ë¤ÅÃ²Ä®Å¤Ã²Ä®Ã§Ã²ÅË¤Æ˜ÄµË¤Å”Å—Ã²ÅÃ²Ä®ÆœË¤ÃŠË¤ÅÃ²Ä¦Æ™ËŠÄÃ²Ä¦Å”Ë¤Ã¦ÄµÄµÄ£Ë™ÅË¤Ã§ÄµÄ®Å¤Ã²Ä®ÆœÊ›Ë¤GÆœË¤Ã§ÄµÄ®Ä®Ã²Ã§Å¤ÅË¤Æ›ÄÃ²Ë¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄÅË¤Æ€Æ“ÆœÄË¤Æ›ÄÃ²Ë¤Æ›ÄÃ²Ä­Ã²Ë¤ÄµÆ™Ë¤ÅÃ²Ä¦Æ™ËŠÄ‘Ä­Å”Å—ÄµÅ¿Ã²Ä­Ã²Ä®ÆœË¤ÃŠÄ®Ã­Ë¤Ã²Ä­Ã¦Å—ÃŠÃ§Ä‘Ä®ÄˆË¤Ã§ÄÃŠÄ¦Ä¦Ã²Ä®ÄˆÃ²ÅÊœË¤Ä­ÃŠÄ£Ä‘Ä®ÄˆË¤Æ—ÄµÅ—Ë¤ÃŠË¤Ã§ÄµÄÃ²Å—Ã²Ä®ÆœË¤Å”ÃŠÅÅÃŠÄˆÃ²Ê›Ë¤Ë¤Ê³Ê›Ê›Ê›Ê´Ë¤\\x9aÄÃ²Ë¤Ã¦Ã²ÅÆœË¤Ã§ÄÄµÆ“Ã§Ã²Ë¤Æ“ÅË¤É¿Ê›Figure 4: A step of deliberate search in a randomly picked Creative Writing task. Given the input, the\\nLM samples 5 different plans, then votes 5 times to decide which plan is best. The majority choice is\\nused to consequently write the output passage with the same sample-vote procedure.\\nIO CoT ToT IO',\n",
              "  'question': 'Given an input for a creative writing task, what is the process used to generate a sample plan and select the best plan through voting?\\n\\n',\n",
              "  'answer': 'The process used to generate a sample plan and select the best plan through voting involves the following steps:\\n\\n1. LM samples 5 different plans.\\n2. The output passage is written using the same sample-vote procedure.\\n3. The majority choice is used to decide which plan is best.\\n\\nThis process is known as Iterative Optimization (IO) and Threshold Testing (ToT).',\n",
              "  'source_doc': 'Tree of Thoughts_  Deliberate Problem Solving with Large Language Models.pdf'},\n",
              " {'context': 'and weâ€™ve got great, fast-charging options in the market, weâ€™ve had the opportunity\\nfor years and we have a great quality product, in this case, and I think we are very\\npleased to have Executive Chairman Jeffrey \"Mugster\" Marcus as Chairman and CEO of Toyota\\nMotor. Weâ€™re thrilled to have Jeff again joining us as the President of Toyota Motor\\nCorporation. In the introduction to the presentation at the event, Merriam-Webster shares\\nexecutivesâ€™ priorities on developing electric vehicle hybrid vehicles. In total, CEO\\nMusk noted that it is a \"science... to build 50,000 EVs in the next three years,\" which\\ntranslates to approximately one-quarter of vehicles on the road. This industry is already\\na technological majority, and manufacturers are rapidly showing off both a range and\\navailability of new technology. However, the goal of Toyotaâ€™s EVs strategy continues\\nto be to deliver electric cars. The company has spent the last five years pursuing this\\ngoal. To date, it has considered electric vehicles in 30 cities, but has yet to deliver\\nthem to consumers. At the moment, the company has three or four EV prototypes on the road\\nwith another two of them pending production. Two of those prototypes are on the road for\\nsale to start next year in California; in the end, the company is committed to continue\\nthe conversation by developing test vehicles with multiple clean-energy and internal\\ncombustion engines. In the new environment as the company heads into 2020, a significant\\namount of executive leadership will be needed to work with executives to develop electric\\nvehicles. Will the changes to executivesâ€™ areas of responsibility make its employees more\\nfocused on customer service? Mugster: There has been some focus recently on our core\\ncustomers. Weâ€™re committed to winning in every community to be a leader in theTable 8: Last Model Prompt and Completion for the 1024 MLE trained model. In this setting we simply',\n",
              "  'question': \"Will the changes to executives' areas of responsibility lead to a more customer-focused approach at Toyota Motor Corporation?\\n\\n\",\n",
              "  'answer': \"The changes to executives' areas of responsibility at Toyota Motor Corporation may lead to a more customer-focused approach if the company prioritizes customer service and addresses the needs of its core customers. However, it ultimately depends on the company's commitment to delivering electric vehicles and its ability to adapt to the changing market.\",\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'also follows a tree-search procedure with leaves sampled from stochastic beam search decoding,\\nwhich are then evaluated by LLM itself with carefully prepared self-eval prompts. Their approach\\nhowever, uses the PAL formulation [ 7] which represents thoughts as codes, which makes it difï¬cult\\nto tackle challenging tasks like creative writing which we consider in this paper. Our Tree-of-Thought\\nformulation is thus more versatile and handles challenging tasks on which GPT-4 only achieves very\\nlow accuracy with standard prompts.\\nProgram-guided LLM generation. Our proposal is also related to recent advancements that or-\\nganize LMâ€™s behavior with symbolic program guidance. For example [ 24] embeds LMs in an\\nalgorithmic search procedure to help solve problems like question answering step-by-step, in which\\nthe search trees are expanded by relevant paragraphs that might provide answers. This approach\\nhowever differs from ours in that trees are expanded by sampling external paragraphs instead of the\\nLMâ€™s own thoughts, and there is no reï¬‚ection or voting steps. Another approach, LLM+P [ 15], goes\\none step further and delegates the actual planning process to a classical planner.\\nClassical search methods. Last but not least, our approach can be treated as a modern rendition\\nof classical search methods for problem solving. For example it can be considered as a heuristic\\nsearch algorithm like A* [ 8], in which the heuristic at each search node is provided by the LMâ€™s\\nself-assessment. From this perspective, our method is also related to NeuroLogic A*esque decoding\\nproposed in [ 16], which is inspired by A* search but introduces look-ahead heuristics that are\\nefï¬cient for LMs to improve the beam-search or top-k sampling decoding. This method however is\\nconstrained to sentence generation tasks, whereas our framework are designed for complex, multi-step\\nproblem solving guarded by value feedback.\\n6 Discussion',\n",
              "  'question': 'What is the main difference between the Tree-of-Thought formulation and the PAL formulation in LLM generation?\\n\\n',\n",
              "  'answer': 'The main difference between the Tree-of-Thought formulation and the PAL formulation in LLM generation is that the Tree-of-Thought formulation is more versatile and handles challenging tasks on which GPT-4 only achieves very low accuracy with standard prompts, while the PAL formulation represents thoughts as codes, making it difficult to tackle challenging tasks like creative writing.',\n",
              "  'source_doc': 'Tree of Thoughts_  Deliberate Problem Solving with Large Language Models.pdf'},\n",
              " {'context': 'Prompt engineering is a resource-efficient approach for\\nsolving different LLM tasks. In brief, one includes the task\\ndescription within the input sent to an LLM. If this descrip-\\ntion is appropriately formulated, the LLM solves the task\\nusing its autoregressive token-based mechanism for gener-\\nating text. Such prompts may contain example tasks with\\nsolutions (few-shot prompting, also referred to as in-context\\nlearning (ICL)), or even no example tasks at all (zero-shot\\nprompting). Recent years shown that this mechanism can be\\nused to solve a broad set of tasks that involve mathematical,\\ncommonsense, or symbolic reasoning.\\nChain-of-Thought (CoT) [70] is an approach for prompt-\\ning, in which one includes the intermediate steps of rea-\\nsoning within the prompt (intermediate â€œthoughtsâ€), besides\\nthe task input/output. CoT was shown to significantly im-\\nprove the capability of LLMs to solve problems without re-\\nsorting to any model updates. One major improvement over\\n*Equal contributionCoT, Self-Consistency with CoT (CoT-SC) [66], is a scheme\\nwhere multiple CoTs are generated, and then the best one is\\nselected as the outcome. More recently, CoT and CoT-SC\\nwere extended with Tree of Thoughts (ToT) [43, 76, 74],\\nwhich models the LLM reasoning process with a tree. This\\nfacilitates using different paths of thoughts, and offers novel\\ncapabilities such as backtracking from non-promising out-\\ncomes. Unfortunately, the ToT approaches still fundamen-\\ntally limit the reasoning abilities within a prompt by impos-\\ning the rigid tree structure on the thought process.\\nIn this work, we argue that fundamentally more power-\\nful prompting can be achieved by enabling LLM thoughts to\\nform an arbitrary graph structure. This is motivated by nu-\\nmerous phenomena such as human reasoning, brain struc-\\nture, or algorithmic execution. When working on a novel\\nidea, a human would not only follow a chain of thoughts\\n(as in CoT) or try different separate ones (as in ToT), but',\n",
              "  'question': 'Can a graph structure be used to enable LLM thoughts to form a more powerful prompt than a rigid tree structure?\\n\\n',\n",
              "  'answer': 'Yes, a graph structure can be used to enable LLM thoughts to form a more powerful prompt than a rigid tree structure. This is motivated by numerous phenomena such as human reasoning, brain structure, or algorithmic execution. When working on a novel idea, a human would not only follow a chain of thoughts (as in CoT) or try different separate ones (as in ToT), but would also consider multiple possible paths simultaneously. By allowing LLM thoughts to form an arbitrary graph structure, the reasoning abilities within a prompt can be significantly improved, enabling LLMs to solve complex problems more efficiently.',\n",
              "  'source_doc': 'Graph of Thoughts_ Solving Elaborate Problems with Large Language Models.pdf'},\n",
              " {'context': 'intermediate attention matrix for the backward pass. We apply two well-established techniques to address\\nthese challenges. (i) We restructure the attention computation to split the input into blocks and make several\\npasses over input blocks, thus incrementally performing the softmax reduction (also known as tiling). (ii) We\\nstore the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the\\nbackward pass, which is faster than the standard approach of reading the intermediate attention matrix from\\nHBM. We implement FlashAttention in CUDA to achieve ï¬ne-grained control over memory access and\\nfuse all the attention operations into one GPU kernel. Even with the increased FLOPs due to recomputation,\\nour algorithm both runs faster (up to 7.6x on GPT-2 [ 67], Figure 1 right) and uses less memory â€”linear\\nin sequence lengthâ€”than standard attention, thanks to the massively reduced amount of HBM access.\\nWe analyze the IO complexity [ 1] ofFlashAttention , proving that it requires ğ‘‚Â¹ğ‘2ğ‘‘2ğ‘€\\x001ÂºHBM\\naccesses where ğ‘‘is the head dimension and ğ‘€is the size of SRAM, as compared to Î©Â¹ğ‘ğ‘‘Â¸ğ‘2Âºof standard\\nattention. For typical values of ğ‘‘andğ‘€,FlashAttention requires many times fewer HBM accesses\\ncompared to standard attention (up to 9 \\x02fewer, as shown in Fig. 2). Moreover, we provide a lower bound,\\nshowing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over\\nall SRAM sizes.\\nWe also show that FlashAttention can serve as a useful primitive for realizing the potential of\\napproximate attention algorithms by overcoming their issues with memory access overhead. As a proof of\\nconcept, we implement block-sparse FlashAttention , a sparse attention algorithm that is 2-4 \\x02faster than\\nevenFlashAttention , scaling up to sequence length of 64k. We prove that block-sparse FlashAttention\\nhas better IO complexity than FlashAttention by a factor proportional to the sparsity ratio. We discuss',\n",
              "  'question': 'Given that FlashAttention restructures the attention computation to split the input into blocks and make several passes over input blocks, how does this approach improve the efficiency of the softmax reduction (also known as tiling) during the backward pass?\\n\\n',\n",
              "  'answer': 'This approach allows for incremental performance of the softmax reduction (also known as tiling) during the backward pass, rather than performing the entire operation in a single pass. This can improve the efficiency of the backward pass by reducing the amount of memory required and allowing for faster computation.',\n",
              "  'source_doc': 'FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf'},\n",
              " {'context': 'B.4 What is more important: instruction finetuning dataset size or dataset quality?\\nData set suitability is more important than dataset size. To understand the effects of dataset\\nquality vs. dataset size, we experiment with subsampling large datasets with at least 150,000 samples\\n(Chip2, FLAN v2, Unnatural Instructions), into datasets of size 50,000, 100,000 and 150,000 and\\nexamine the resulting trends, as shown in Table 11. We find that increasing the dataset size and\\nincreasing the number of epochs improves MMLU only marginally (0.0 - 0.5 MMLU), while the\\ndifference between datasets is up to 40x larger (1.5 - 8.0 MMLU). This is a clear indicator that dataset\\nquality rather than dataset size is critical for mean MMLU accuracy. We obtain similar findings for\\nchatbot performance as discussed in .\\nC Human Evaluation\\nWe conduct a human evaluation with the same wording given to GPT-4 in the original Vicuna\\nevaluation [10], adjusted for an Amazon Mechanical Turk form as show in Figure 5.\\nD Pairwise Evaluation with GPT-4\\nWhile we found that the GPT-4 evaluation gave different results depend on which system was\\npresented first, when averaged over both options the pairwise results were well-ordered. The\\naggregated pairwise judgments are hown in Table 12. On inspection, it is clear these judgments are\\ntransitive, i.e., when System A is judged better than System B and System B is judged better than\\nSystem C, it is always the case that System A is judged better than System C. This yields a complete\\nordering, given in Table 13.\\nE NormalFloat 4-bit data type\\nThe exact values of the NF4 data type are as follows:\\n[-1.0, -0.6961928009986877, -0.5250730514526367,\\n-0.39491748809814453, -0.28444138169288635, -0.18477343022823334,\\n-0.09105003625154495, 0.0, 0.07958029955625534, 0.16093020141124725,\\n0.24611230194568634, 0.33791524171829224, 0.44070982933044434,\\n0.5626170039176941, 0.7229568362236023, 1.0]\\nF Normality of Trained Neural Network Weights',\n",
              "  'question': 'Given the context, what is the relationship between dataset size and quality in terms of mean MMLU accuracy?\\n\\n',\n",
              "  'answer': 'The context indicates that dataset quality is more important than dataset size for mean MMLU accuracy.',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': '1-3 missing entities from the previous summary with-\\nout increasing the overall length ( 5xoverall). Each\\nsummary has a higher ratio of entities to tokens than\\nthe previous one. Based on human preference data, we\\ndetermine that humans prefer summaries that are al-\\nmost as dense as human-written summaries and morearXiv:2309.04269v1  [cs.CL]  8 Sep 2023Article: \\n{{ARTICLE}}\\nYou \\nwill \\ngenerate \\nincreasingly \\nconcise, \\nentity-dense \\nsummaries \\nof \\nthe \\nabove \\nArticle.\\nRepeat \\nthe \\nfollowing \\n2 \\nsteps \\n5 \\ntimes.\\nStep \\n1. \\nIdentify \\n1-3 \\ninformative \\nEntities \\n(\";\" \\ndelimited) \\nfrom \\nthe \\nArticle \\nwhich \\nare \\nmissing \\nfrom \\nthe \\npreviously \\ngenerated \\nsummary.\\nStep \\n2. \\nWrite \\na \\nnew, \\ndenser \\nsummary \\nof \\nidentical \\nlength \\nwhich \\ncovers \\nevery \\nentity \\nand \\ndetail \\nfrom \\nthe \\nprevious \\nsummary \\nplus \\nthe \\nMissing \\nEntities.\\nA \\nMissing \\nEntity \\nis:\\n- \\nRelevant\\n: \\nto \\nthe \\nmain \\nstory.\\n- \\nSpecific\\n: \\ndescriptive \\nyet \\nconcise \\n(5 \\nwords \\nor \\nfewer).\\n- \\nNovel\\n: \\nnot \\nin \\nthe \\nprevious \\nsummary.\\n- \\nFaithful\\n: \\npresent \\nin \\nthe \\nArticle.\\n- \\nAnywhere\\n: \\nlocated \\nanywhere \\nin \\nthe \\nArticle.\\nGuidelines:\\n- \\nThe \\nfirst \\nsummary \\nshould \\nbe \\nlong \\n(4-5 \\nsentences, \\n~80 \\nwords) \\nyet \\nhighly \\nnon-specific, \\ncontaining \\nlittle \\ninformation \\nbeyond \\nthe \\nentities \\nmarked \\nas \\nmissing. \\nUse \\noverly \\nverbose \\nlanguage \\nand \\nfillers \\n(e.g., \\n\"this \\narticle \\ndiscusses\") \\nto \\nreach \\n~80 \\nwords.\\n- \\nMake \\nevery \\nword \\ncount: \\nre-write \\nthe \\nprevious \\nsummary \\nto \\nimprove \\nflow \\nand \\nmake \\nspace \\nfor \\nadditional \\nentities.\\n- \\nMake \\nspace \\nwith \\nfusion, \\ncompression, \\nand \\nremoval \\nof \\nuninformative \\nphrases \\nlike \\n\"the \\narticle \\ndiscusses\".\\n- \\nThe \\nsummaries \\nshould \\nbecome \\nhighly \\ndense \\nand \\nconcise \\nyet \\nself-contained, \\ne.g., \\neasily \\nunderstood \\nwithout \\nthe \\nArticle.\\n- \\nMissing \\nentities \\ncan \\nappear \\nanywhere \\nin \\nthe \\nnew \\nsummary.\\n- \\nNever \\ndrop \\nentities \\nfrom \\nthe \\nprevious \\nsummary. \\nIf \\nspace \\ncannot \\nbe \\nmade, \\nadd \\nfewer \\nnew \\nentities.\\nRemember, \\nuse \\nthe \\nexact \\nsame',\n",
              "  'question': 'What entities are missing from the previous summary and how can they be incorporated into a new, denser summary?\\n\\n',\n",
              "  'answer': 'The entities missing from the previous summary are \"Relevant\", \"Specific\", \"Novel\", and \"Faithful\". These entities can be incorporated into a new, denser summary by adding specific details about the main story, using descriptive language, highlighting any new or unique information, and ensuring that the summary is faithful to the article. The summary should be self-contained and easily understood without the article. The missing entities can appear anywhere in the new summary, but they should not be dropped from the previous summary. If space cannot be made, fewer new entities can be added.',\n",
              "  'source_doc': 'From Sparse to Dense_  GPT-4 Summarization with Chain of Density Prompting.pdf'},\n",
              " {'context': 'subproblems, whereby solving a given subproblem is facilitated by the answers to previously solved\\n\\x03Corresponding to: dennyzhou@google.com\\n1arXiv:2205.10625v3  [cs.AI]  16 Apr 2023Published as a conference paper at ICLR 2023\\nsubproblems. Both stages are implemented by few-shot prompting, so that there is no training or\\nï¬netuning in either stage. An example usage of least-to-most prompting is illustrated in Figure 1.\\nThe term least-to-most prompting is borrowed from educational psychology (Libby et al., 2008),\\nwhere it is used to denote the technique of using a progressive sequence of prompts to help a student\\nto learn a new skill. Here we apply this technique for teaching humans to teach language models.\\nEmpirical results on symbolic manipulation, compositional generalization, and math reasoning show\\nthat least-to-most prompting can indeed generalize to problems harder than those demonstrated.\\nFigure 1: Least-to-most prompting solving a math word problem in two stages: (1) query the lan-\\nguage model to decompose the problem into subproblems; (2) query the language model to sequen-\\ntially solve the subproblems. The answer to the second subproblem is built on the answer to the ï¬rst\\nsubproblem. The demonstration examples for each stageâ€™s prompt are omitted in this illustration.\\n2 L EAST -TO-MOST PROMPTING\\nLeast-to-most prompting teaches language models how to solve a complex problem by decomposing\\nit to a series of simpler subproblems. It consists of two sequential stages:\\n1.Decomposition . The prompt in this stage contains constant examples that demonstrate the\\ndecomposition, followed by the speciï¬c question to be decomposed.\\n2.Subproblem solving . The prompt in this stage consists of three parts: (1) constant exam-\\nples demonstrating how subproblems are solved; (2) a potentially empty list of previously\\nanswered subquestions and generated solutions, and (3) the question to be answered next.',\n",
              "  'question': 'What is the purpose of least-to-most prompting in teaching language models how to solve complex problems?\\n\\n',\n",
              "  'answer': 'The purpose of least-to-most prompting is to decompose a complex problem into a series of simpler subproblems to facilitate the solution of the original problem. It consists of two sequential stages: decomposition and subproblem solving. In the decomposition stage, the prompt contains constant examples that demonstrate the decomposition, followed by the specific question to be decomposed. In the subproblem solving stage, the prompt consists of three parts: constant examples demonstrating how subproblems are solved, a potentially empty list of previously answered subquestions and generated solutions, and the question to be answered next.',\n",
              "  'source_doc': 'Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.pdf'},\n",
              " {'context': 'the number of preceding LLM thoughts that could have im-\\npacted t. Formally, the volume of tis the number of thoughts\\nfrom which there exists a path to tin the graph of thoughts.\\nWe assume that outputting a single thought costs O(1)time\\nand fix the total cost to Î˜(n)for each prompting scheme.The structure of the schemes is as follows. CoT-SC con-\\nsists of kindependent chains originating from a single start-\\ning thought. ToT is a complete k-ary tree. Finally, in GoT, a\\ncomplete k-ary tree is joined at its leaves with a â€œmirroredâ€\\nk-ary tree of the same size but with its edges reversed.\\nThe analysis is detailed in Table 2. CoT offers a large vol-\\nume of up to N, but at the cost of a high latency of N. CoT-\\nSC reduces the latency by a factor of k(which corresponds\\nto its branching factor), but it simultaneously decreases the\\nvolume by kas well. ToT offers a latency of logkNbut\\nalso has low volume. GoT is the only scheme to come with\\nboth a low latency of logkNand a high volume N. This\\nis enabled by the fact that GoT harnesses aggregations of\\nthoughts, making it possible to reach the final thought from\\nany other intermediate thought in the graph decomposition.\\nScheme Latency Volume\\nChain-of-Thought (CoT) N N\\nSelf-Consistency with CoT (CoT-SC) N/k N/k\\nTree of Thoughts (ToT) logkN O (logkN)\\nGraph of Thoughts (GoT) logkN N\\nTable 2: Comparison of prompting schemes, with respect\\nto their fundamental tradeoff between latency and volume.\\nGoT offers the best tradeoff.\\n7 Evaluation\\nWe show the advantages of GoT over the state of the art. We\\nfocus on comparing GoT to ToT, as it was shown to consis-\\ntently outperform other schemes. Still, for a broad compari-\\nson, we also experiment with IO, CoT, and CoT-SC. As our\\nanalysis results in a large evaluation space, we present rep-\\nresentative results and omit data that does not bring relevant\\ninsights (e.g., CoT-SC).\\n7.1 Evaluation Methodology\\nWe use 100 input samples for each task and comparison',\n",
              "  'question': 'Suppose the prompting scheme used is Graph of Thoughts (GoT). What is the expected volume of thoughts in GoT?\\n\\n',\n",
              "  'answer': 'The expected volume of thoughts in GoT is N.',\n",
              "  'source_doc': 'Graph of Thoughts_ Solving Elaborate Problems with Large Language Models.pdf'},\n",
              " {'context': 'SCRATCHPAD TRACINGLarge Language ModelFigure 1: Overview of our scratchpad approach applied to predicting code execution and comparison\\nto direct execution prediction. Top: Previous work has shown that large pre-trained models achieve\\npoor performance when asked to directly predict the result of executing given computer code (Austin\\net al., 2021). Bottom: In this work, we show that training models to use a scratchpad and predict\\nthe program execution trace line-by-line can lead to large improvements in execution prediction\\nperformance. N.B. Although the example above only has one loop iteration for each loop, all loops\\nare unrolled across time.\\nmodel is asked to perform these tasks in one forward pass. Given a ï¬xed number of layers and a ï¬xed\\namount of computation time, the model cannot adapt the amount of compute spent on a problem\\nto its difï¬culty before producing an output.1Prior work (Graves, 2016; Banino et al., 2021) has\\nexplored neural architectures that explicitly allow for dynamically chosen amounts of computation\\ntime to be dedicated to different sub-tasks. In this work, we propose a different approachâ€”one that\\ncan exploit existing Transformer architectures and large few-shot-capable language modelsâ€”we\\nmodify the task design rather than the model or training procedure.\\nOur proposal is simple: Allow the model to produce an arbitrary sequence of intermediate tokens,\\nwhich we call a scratchpad , before producing the ï¬nal answer. For example, on addition problems,\\nthe scratchpad contains the intermediate results from a standard long addition algorithm (see Figure\\n2). To train the model, we encode the intermediate steps of the algorithm as text and use standard\\nsupervised training.\\nThis paper makes the following contributions:\\nâ€¢ We introduce (Section 2) the notion of a â€œscratchpadâ€ for Transformers, in order to make them\\nbetter at performing complex discrete computations without modifying the underlying architec-\\nture.',\n",
              "  'question': 'What is the proposed approach in this work to improve execution prediction performance using large language models?\\n\\n',\n",
              "  'answer': 'The proposed approach in this work is to train models to use a scratchpad and predict the program execution trace line-by-line. This is done by allowing the model to produce an arbitrary sequence of intermediate tokens, which are called a scratchpad, before producing the final answer. The intermediate steps of the algorithm are encoded as text and used for standard supervised training. This approach allows the model to exploit existing Transformer architectures and large few-shot-capable language models, without modifying the underlying architecture or training procedure.',\n",
              "  'source_doc': 'Show Your Work_  Scratchpads for Intermediate Computation with Language Models.pdf'},\n",
              " {'context': '0.24611230194568634, 0.33791524171829224, 0.44070982933044434,\\n0.5626170039176941, 0.7229568362236023, 1.0]\\nF Normality of Trained Neural Network Weights\\nWhile it is common knowledge that trained neural network weights are mostly normally distributed,\\nwe perform statistical testing to verify this. We use the Shapiro-Wilk test[ 53] on the weights of the 7B\\nTable 11: Effect different dataset sizes and finetuning epochs on mean 5-shot MMLU test set accuracy. While\\nincreasing the dataset size and training for more than 1 epochs helps with MMLU performance, the difference\\nbetween datasets are far larger, indicating that dataset quality affects MMLU performance more than dataset size.\\nChip Unnatural Instructions FLAN v2\\nDatapoints â†“Epochs â†’ 1 2 3 1 2 3 1 2 3 Mean\\n50000 34.50 35.30 34.70 38.10 42.20 38.10 43.00 43.50 44.10 39.28\\n100000 33.70 33.90 34.00 40.10 41.20 37.00 43.90 43.70 44.90 39.16\\n150000 34.40 34.80 35.10 39.70 41.10 41.50 44.60 45.50 43.50 40.02\\nMean 34.20 34.67 34.60 39.30 41.50 38.87 43.83 44.23 44.17\\n24Figure 5: The crowdsourcing form used by human annotators.\\nLLaMA model [ 57]. We find that the weights of each hidden unit have different normal distributions.\\nAs such, we test he weights of each individual hidden unit. This mean for weight Wâˆˆ RinÃ—out\\nwe perform tests over the outdimension. Using a 5% significance threshold, we find that 7.5% of\\nneurons are non-normally distributed which is about 2.5% more than the expected false-positive\\nrate. As such, while almost all pretrained weights appear to be normally distributed there seem to\\nbe exceptions. Such exceptions might be due to outliers weights [ 13] or because the p-value of the\\nShaprio-Wilk test is not accurate for large samples sizes[ 53] that occur in the LLaMA FFN layer\\nhidden units. this verifies the claim that neural network weights.\\nTable 12: Aggregated pairwise GPT-4 judgments between systems where the value of a cell at row xand column',\n",
              "  'question': 'Do the weights of the hidden units in the LLaMA model have different normal distributions?\\n\\n',\n",
              "  'answer': 'Yes, the weights of each hidden unit in the LLaMA model have different normal distributions.',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': 'higher scores to its own outputs compared to human ratings, Elo of 1348 vs 1176, which represent an\\nadditional 20% probability of winning against an opponent. Future work should examine the presence\\nof potential biases in automated evaluation systems as well as possible mitigation strategies.\\nData & Training We note that the OASST1 dataset on which Guanaco models are trained is\\nmultilingual and that the OA benchmark also contains prompts in different languages. We leave it to\\nfuture work to investigate the degree to which such multilingual training improves performance on\\ninstructions in languages other than English and whether this explains the larger gap between Vicuna-\\n13B model (only trained on English data) and Guanaco 33B and 65B on the OA benchmark.\\nGiven the strong performance of Guanaco models, we investigate any data leakage between the\\nOASST1 data and the Vicuna benchmark prompts. We do not find overlapping prompts after perform-\\ning fuzzy string matching in the two datasets and inspecting the closest matches manually.\\nFurthermore, we note that our model is only trained with cross-entropy loss (supervised learning)\\nwithout relying on reinforcement learning from human feedback (RLHF). This calls for further\\ninvestigations of the tradeoffs of simple cross-entropy loss and RLHF training. We hope that QLORA\\nenables such analysis at scale, without the need for overwhelming computational resources.\\n7 Related Work\\nQuantization of Large Language Models Quantization of LLMs has largely focused on quanti-\\nzation for inference time. Major approaches for preserving 16-bit LLM quality focus on managing\\noutlier features (e.g., SmoothQuant [ 66] and LLM.int8() [ 14]) while others use more sophisticated\\ngrouping methods [ 44,69]. Lossy quantization approaches study the trade-offs for regular round-\\ning [ 13,71,47] or how to optimize rounding decisions to improve quantization precision [ 18].',\n",
              "  'question': 'Does the presence of multilingual training in Guanaco models improve their performance on instructions in languages other than English?\\n\\n',\n",
              "  'answer': 'It is not clear from the provided context whether the presence of multilingual training in Guanaco models improves their performance on instructions in languages other than English. The context notes that the OASST1 dataset on which Guanaco models are trained is multilingual and that the OA benchmark also contains prompts in different languages. However, it is left to future work to investigate the degree to which such multilingual training improves performance on instructions in languages other than English and whether this explains the larger gap between Vicuna-13B model (only trained on English data) and Guanaco 33B and 65B on the OA benchmark.',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': 'Llama 65B CoVe (factored) 63.7 11.7\\nLlama 65B CoVe (factor+revise) 71.4 12.3\\nTable 3: Longform generation of biographies with metrics defined from Min et al. (2023). Models\\nmarked with âˆ—are reported from previous work. FACTSCORE automatically computed using â€œInstruct-\\nLlamaâ€ ( Retrieve â†’LM + NP), the best open-access model.\\nvery rare rare medium freq very freq\\nRarity0.00.20.40.60.8FactScoreLlama Few-shot\\nLlama CoVe (joint)\\nLlama CoVe (factored)\\nLlama CoVe (factor+revise)\\nInstructGPT\\nChatGPT\\nPerplexity.ai\\nLlama 2 Chat Zero-shot\\nLlama 2 Chat CoT\\nvery rare rare medium freq very freq\\nRarity0.00.20.40.60.8FactScore\\nFigure 2: FACTSCORE performance distribution across head, torso and tail facts for CoVe variants\\nand various baselines on longform generation of biographies.\\nOur main results across the four benchmark tasks are given in Table 1, Table 2 and Table 3, and our\\nmain findings are as follows.\\nCoVe improves precision on list-based answer tasks We find that CoVe provides large gains in\\nprecision on the list-based tasks, e.g. more than doubles the precision from the Llama 65B few-shot\\nbaseline for the Wikidata task (from 0.17 to 0.36). We find from the positive and negative breakdown\\nthat there is a large reduction in the number of hallucinated answers (negatives: 2.95 â†’0.68) while\\nonly a relatively small reduction in the number of non-hallucinations (positives: 0.59 â†’0.38).\\nCoVe improves performance on closed book QA We also find that CoVe brings improvements in\\ngeneral QA problems, as measured on MultiSpanQA. We observe a 23% improvement in F1 over the\\nfew-shot baseline (0.39 â†’0.48), where the improvements come from gains in both precision and\\nrecall.\\n7Verification Execution\\nCoVe (joint) CoVe (factored)\\nVerification Plan Prec. Prec.\\nRule-based questions 0.13 0.16\\nGenerated by model:\\nyes/no questions 0.15 0.19\\ngeneral questions 0.15 0.22\\nTable 4: Comparison of various CoVe verification plan strategies (rows) and verification execution',\n",
              "  'question': 'How does the verification plan performance of CoVe (joint) and CoVe (factored) vary across different types of questions?\\n\\n',\n",
              "  'answer': 'The verification plan performance of CoVe (joint) and CoVe (factored) varies across different types of questions. For rule-based questions, CoVe (joint) has a precision of 0.13 and CoVe (factored) has a precision of 0.16. For generated yes/no questions, CoVe (joint) has a precision of 0.15 and CoVe (factored) has a precision of 0.19. For general questions, CoVe (joint) has a precision of 0.15 and CoVe (factored) has a precision of 0.22.',\n",
              "  'source_doc': 'Chain-of-Verification Reduces Hallucination in Large Language Models.pdf'},\n",
              " {'context': 'B.1 Datasets\\nWe describe the datasets used for QL ORA finetuning experiments outlined in Section 5.\\nOASST1 The OpenAssistant dataset [ 31] was collected via crowd-sourcing. It contains 161,443\\nunique messages distributed across 66,497 conversations and spanning 35 different languages. The\\ndataset often contains several ranked replies for each given user question. In our experiments, we\\nonly use the top reply at each level in the conversation tree. This limits the dataset to 9,209 examples.\\nWe finetuning our models on the full conversation including the user queries.\\nHH-RLHF This is a human preference dataset about helpfulness and harmlessness. Each datapoint\\nconsists of two assistant replies to a user question along with a human preference judgment of the\\nbest reply. The dataset contains 160,800 examples. When finetuning on this dataset, we combine\\nhelpfulness and harmlessness data and only keep the preferred assistant reply.\\nFLAN v2 The FLAN v2 collection [ 39] is a collection of 1836 tasks augmented with hundreds\\nof manually curated templates and rich formatting patterns into over 15M examples. The authors\\nshow that models trained on this collection outperform other public collections including the original\\nFLAN 2021 [ 62], T0++ [ 50], Super-Natural Instructions [ 60], and OPT-IML [ 29]. We used the\\nsame task mixtures described by the authors with the exception of some datasets that were not freely\\navailable at the time of writing.\\n22Parameters Dataset Batch size LR Steps Source Length Target Length\\n7B All 16 2e-4 10000 384 128\\n7B OASST1 16 2e-4 1875 - 512\\n7B HH-RLHF 16 2e-4 10000 - 768\\n7B Longform 16 2e-4 4000 512 1024\\n13B All 16 2e-4 10000 384 128\\n13B OASST1 16 2e-4 1875 - 512\\n13B HH-RLHF 16 2e-4 10000 - 768\\n13B Longform 16 2e-4 4000 512 1024\\n33B All 32 1e-4 5000 384 128\\n33B OASST1 16 1e-4 1875 - 512\\n33B HH-RLHF 32 1e-4 5000 - 768\\n33B Longform 32 1e-4 2343 512 1024\\n65B All 64 1e-4 2500 384 128\\n65B OASST1 16 1e-4 1875 - 512\\n65B HH-RLHF 64 1e-4 2500 - 768',\n",
              "  'question': 'Given the context, what are the datasets used for QL ORA finetuning experiments and how are they preprocessed?\\n\\n',\n",
              "  'answer': 'The datasets used for QL ORA finetuning experiments are OASST1, HH-RLHF, and FLAN v2. OASST1 is a crowd-sourced dataset containing 161,443 unique messages distributed across 66,497 conversations and spanning 35 different languages. HH-RLHF is a human preference dataset about helpfulness and harmlessness. Each datapoint consists of two assistant replies to a user question along with a human preference judgment of the best reply. FLAN v2 is a collection of 1836 tasks augmented with hundreds of manually curated templates and rich formatting patterns into over 15M examples.\\n\\nFor preprocessing, in the experiments, the top reply at each level in the conversation tree is used for OASST1, and the preferred assistant reply is kept for HH-RLHF. For FLAN v2, the same task mixtures are used as described by the authors, with the exception of some datasets that were not freely available at the time of writing. The batch size, learning rate, number of steps, source length, target length, and other parameters are also specified in the table.',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': 'training data are sampled independently.\\nForDATESET , on the other hand, the consider-\\nable improvement of Toolformer compared to other\\nmodels can be fully accredited to the calendar tool,\\nwhich it makes use of for 54.8% of all examples.\\n4.3 Language Modeling\\nIn addition to verifying improved performance on\\nvarious downstream tasks, we also want to ensure\\nthat language modeling performance of Toolformer\\ndoes not degrade through our ï¬netuning with API\\ncalls. To this end, we evaluate our models on\\ntwo language modeling datasets: WikiText (Mer-\\nity et al., 2017) and a subset of 10,000 randomly\\nselected documents from CCNet (Wenzek et al.,\\n2020) that were not used during training. Perplex-\\nities of various models are shown in Table 8. As\\none would expect, ï¬netuning on CCNet leads to\\nslightly improved performance on a different CC-\\nNet subset, but it slightly deteriorates performance\\non WikiText, presumably because the original pre-Model WikiText CCNet\\nGPT-J 9.9 10.6\\nGPT-J + CC 10.3 10.5\\nToolformer (disabled) 10.3 10.5\\nTable 8: Perplexities of different models on WikiText\\nand our validation subset of CCNet. Adding API calls\\ncomes without a cost in terms of perplexity for lan-\\nguage modeling without any API calls.\\ntraining data for GPT-J is more similar to Wiki-\\nText than our randomly selected subset of CCNet.\\nMost importantly, however, training on C\\x03(our\\ndataset annotated with API calls) does not lead to\\nan increase in perplexity compared to training on\\nCwhen API calls are disabled at inference time.8\\n4.4 Scaling Laws\\nWe investigate how the ability to ask external tools\\nfor help affects performance as we vary the size\\nof our LM. To this end, we apply our approach\\nnot just to GPT-J, but also to four smaller mod-\\nels from the GPT-2 family (Radford et al., 2019),\\nwith 124M, 355M, 775M and 1.6B parameters, re-\\nspectively. We do so using only a subset of three\\ntools: the question answering system, the calcula-\\ntor, and the Wikipedia search engine. Apart from',\n",
              "  'question': \"Given a model that uses external tools for language modeling, how does the ability to ask these tools for help affect the model's performance as the size of the language model increases?\\n\\n\",\n",
              "  'answer': \"The ability to ask external tools for help does not have a significant impact on the performance of the language model as the size of the model increases. This is because the model's performance on the validation subset of CCNet and WikiText does not deteriorate when training on a dataset annotated with API calls compared to training on a dataset without API calls. Additionally, the perplexities of the models on WikiText and the validation subset of CCNet do not increase when API calls are enabled at inference time.\",\n",
              "  'source_doc': 'Toolformer_ Language Models Can Teach Themselves to Use Tools.pdf'},\n",
              " {'context': 'each LLaMA architecture on NVIDIA A100. The table contains the architectureâ€™s (1) latency for\\nprefilling sequences of length 1 to 700 with different batch sizes (from 1 to 16), and (2) decoding\\none token with a context of length 1 to 1024 with different batch sizes (from 1 to 16). With these\\nthree latency profiling tables, given the number of points B, the token lengths of the requests and\\nresponses in the skeleton and point-expanding stages, we can quickly estimate the SoT latency\\nby simply looking up entries in the tables and adding them up. See App. F for a more detailed\\ndescription of how we conduct the profiling and estimate the latency.\\nIn addition to the above approach, we also compare the actual latency of SoT and normal sequential\\ngeneration (abbreviated as â€œnormalâ€ in the following discussion) in App. G.1.4.\\nThe rest of this section shows the speed-ups of SoT on different models (Â§ 3.1.1) and question\\ncategories (Â§ 3.1.2). In addition, we also report the latency breakdown of SoT stages in App. G.1.2\\nand the SoT speed-ups on an RTX 3090 GPU in App. G.1.3.\\n3.1.1 S PEED -UPBREAKDOWN : M ODELS\\nWe investigate how SoT reduces the end-to-end latency on different models. Fig. 2a shows the\\naverage speed-up for each model across all question categories. We can see that SoT obtains a >2Ã—\\nspeed-up (up to 2.39 Ã—) on 8 out of 12 models.\\nWe report the detailed statistics about token lengths and numbers of points in Fig. 11. (1) In terms\\nofthe point number B(Fig. 11a), LLaMA2, Vicuna-7B V1.1, Vicuna-7B V1.3, and ChatGPT-3.5\\nyield relatively fewer points ( <6), while GPT-4 and StableVicuna-13B generates the largest number\\nof points on average ( â‰ˆ9). (2) Regarding the point-expanding response length , Figs. 11b to 11d\\nshow that the API-based models, ChatGPT-3.5, Claude, and GPT-4, follow the point-expanding\\nrequest better and generate shorter point-expanding responses than the open-source models. One',\n",
              "  'question': 'How does the SoT approach reduce the end-to-end latency on different LLM architectures?\\n\\n',\n",
              "  'answer': \"The SoT approach reduces the end-to-end latency on different LLM architectures by using three latency profiling tables and estimating the SoT latency by looking up entries in the tables and adding them up. The profiling tables contain the architecture's latency for prefilling sequences of length 1 to 700 with different batch sizes and decoding one token with a context of length 1 to 1024 with different batch sizes. The SoT approach achieves a >2Ã— speed-up on 8 out of 12 models, with GPT-4 and StableVicuna-13B generating the largest number of points on average. The API-based models, ChatGPT-3.5, Claude, and GPT-4, follow the point-expanding request better and generate shorter point-expanding responses than the open-source models.\",\n",
              "  'source_doc': 'Skeleton-of-Thought_ Large Language Models Can Do Parallel Decoding.pdf'},\n",
              " {'context': 'documents to 20. We detail the hyperparameters used for the training of our ï¬nal model at the beginning of\\nSection 4.5.\\n9Table 1:Retriever loss ablation. We compare diï¬€erent loss functions to pre-train the retriever jointly\\nwith the language model. We use the preï¬x MLM task, and the December 2021 Wikipedia dump for both\\nthe index and pre-training data. Fine-tuning is performed with query-side ï¬ne-tuning and the loss used for\\npre-training. Best result is bold, second highest underlined.\\n64-shot 1024-shot\\nMLM NQ WoW FEVER Avg. NQ WoW FEVER Avg.\\nClosed-book 1.083 6.5 14.1 59.0 26.5 10.7 16.5 75.3 34.2\\nNo Joint pre-training - 9.0 14.1 67.0 30.0 9.9 16.6 78.3 34.9\\nFixed retriever 0.823 39.9 14.3 72.4 42.2 45.3 17.9 90.0 51.1\\nADist 0.780 40.9 14.4 73.8 43.0 46.2 17.290.9 51.4\\nEMDR20.783 43.3 14.6 72.1 43.3 44.9 18.3 85.7 49.6\\nPDist 0.783 45.0 15.0 77.0 45.7 44.9 17.9 90.2 51.0\\nLOOP 0.766 41.815.0 74.4 43.747.117.9 87.5 50.8\\nFine-tuning. When performing a downstream task, either in a few-shot setting or with a large training set,\\nwe employ ï¬ne-tuning to adapt our models to these tasks. For the few-shot KILT ablation experiments, we\\nperform a ï¬xed number of ï¬ne-tuning iterations, instead of using early-stopping. More precisely, we decided\\nto use 50 iterations for the 64-shot setting and 200 iterations in the 1024-shot setting. In both cases, we use\\na batch size of 32examples, a learning rate of 4Ã—10âˆ’5with linear decay and 5 warmup steps for both the\\nreader and the retriever.\\nUnlabeled datasets. Finally, we discuss the unlabeled text datasets that we use to train our models,\\nwhich form the retrieval index. First, we consider the Dec. 20, 2021 Wikipedia dump, for which we keep\\nthe lists and infoboxes, which are linearized by adding a semi-colon separator between the entries. We split\\narticles by section, and split long sections into passages of equal sizes and containing less than 200 words.',\n",
              "  'question': 'What was the number of iterations used for fine-tuning in the 64-shot setting for the few-shot KILT ablation experiments?\\n\\n',\n",
              "  'answer': '50 iterations',\n",
              "  'source_doc': 'Atlas_  Few-shot Learning with Retrieval Augmented Language Models.pdf'},\n",
              " {'context': 'Atlas: Few-shot Learning with\\nRetrieval Augmented Language Models\\nGautier Izacardâˆ— âˆ—,â™¦,â™£,â™¥gizacard@fb.com\\nPatrick Lewisâˆ—,â™¦plewis@fb.com\\nMaria Lomeliâ™¦marialomeli@fb.com\\nLucas Hosseiniâ™¦hoss@fb.com\\nFabio Petroniâ™¦fabiopetroni@fb.com\\nTimo Schickâ™¦schick@fb.com\\nJane Dwivedi-Yuâ™¦janeyu@fb.com\\nArmand Joulinâ™¦ajoulin@fb.com\\nSebastian Riedelâ™¦,â™ sriedel@fb.com\\nEdouard Graveâ™¦egrave@fb.com\\nâ™¦Meta AI Research,â™£ENS, PSL University,â™¥Inria,â™ University College London\\nAbstract\\nLarge language models have shown impressive few-shot results on a wide range of tasks.\\nHowever, when knowledge is key for such results, as is the case for tasks such as question\\nanswering and fact checking, massive parameter counts to store knowledge seem to be needed.\\nRetrieval augmented models are known to excel at knowledge intensive tasks without the\\nneed for as many parameters, but it is unclear whether they work in few-shot settings. In this\\nwork we present Atlas, a carefully designed and pre-trained retrieval augmented language\\nmodel able to learn knowledge intensive tasks with very few training examples. We perform\\nevaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and\\nstudy the impact of the content of the document index, showing that it can easily be updated.\\nNotably, Atlasreaches over 42% accuracy on Natural Questions using only 64 examples,\\noutperforming a 540B parameters model by 3% despite having 50x fewer parameters.\\n1 Introduction\\nLarge language models (LLMs) are impressive few-shot learners (Brown et al., 2020; Rae et al., 2021; Hoï¬€mann\\net al., 2022; Chowdhery et al., 2022). They are able to learn new tasks with very few examples or even\\nfrom instructions alone. For this generalisation ability to emerge, the key ingredients are scaling both the\\nparameter count of the model, and the size of the training data. Large language models owe this improvement\\nto both a larger computational budget, enabling more complex reasoning, and the ability to memorize more',\n",
              "  'question': 'Can a retrieval augmented language model be used for few-shot learning tasks without massive parameter counts?\\n\\n',\n",
              "  'answer': 'Yes, a retrieval augmented language model can be used for few-shot learning tasks without massive parameter counts, as demonstrated by the Atlas model in the paper \"Atlas: Few-shot Learning with Retrieval Augmented Language Models\". The Atlas model was able to achieve over 42% accuracy on the NaturalQuestions task using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.',\n",
              "  'source_doc': 'Atlas_  Few-shot Learning with Retrieval Augmented Language Models.pdf'},\n",
              " {'context': 'mance on knowledge tasks, and uses retrieval during both pre-training and ï¬ne-tuning.\\ndemonstrate compelling few-shot learning capabilities. In this work we address this gap, and present Atlas,\\na retrieval-augmented language model capable of strong few-shot learning, despite having lower parameter\\ncounts than other powerful recent few-shot learners.\\nAtlasretrieves relevant documents based on the current context by using a general-purpose dense retriever\\nusing a dual-encoder architecture, based on the Contriever (Izacard et al., 2022). The retrieved documents\\nare processed, along with the current context, by a sequence-to-sequence model using the Fusion-in-Decoder\\narchitecture (Izacard & Grave, 2020) that generates the corresponding output. We study the impact of\\ndiï¬€erent techniques to train Atlason its few-shot performance on a range of downstream tasks, including\\nquestion answering and fact checking. We ï¬nd that jointly pre-training the components is crucial for few-shot\\nperformance, and we carefully evaluate a number of existing and novel pre-training tasks and schemes for\\nthis purpose. Atlasachieves strong downstream performance in both few-shot and resource-rich settings.\\nFor example, with only 11B parameters, Atlasachieves an accuracy of 42.4% on NaturalQuestions using\\n64 training examples (45.1% with a Wikipedia-only index), outperforming PaLM (Chowdhery et al., 2022),\\na 540B parameter model by almost 3 points, and 64.0% in a full-dataset setting with a Wikipedia index,\\nestablishing a new state of the art by 8 points.\\nIn summary we make the following contributions:\\nâ€¢A thorough study on how to design and train retrieval-augmented language models, with a focus on\\ndownstream few-shot learning and sample eï¬ƒciency.\\nâ€¢The ï¬ndings of this study lead to a retrieval-augmented language model, called Atlas, that exhibits\\nfew-shot abilities that emerge at lower scale than standard LLM.',\n",
              "  'question': 'What techniques were used to train Atlas, a retrieval-augmented language model, to achieve strong few-shot learning capabilities?\\n\\n',\n",
              "  'answer': \"Atlas was trained using a combination of pre-training tasks and schemes, including joint pre-training of its components. The retrieval component used a general-purpose dense retriever based on the Contriever architecture, while the sequence-to-sequence model used the Fusion-in-Decoder architecture. The impact of different techniques on Atlas's few-shot performance was studied on a range of downstream tasks, including question answering and fact checking. Atlas achieved strong downstream performance in both few-shot and resource-rich settings, outperforming other models in terms of accuracy and sample efficiency.\",\n",
              "  'source_doc': 'Atlas_  Few-shot Learning with Retrieval Augmented Language Models.pdf'},\n",
              " {'context': 'ing, generating political advertisements, and law enforcement. If these models are open-sourced,\\nit becomes challenging to limit harmful applications in these and other domains without proper\\nregulation. On the other hand, if large language model access is restricted to a few organizations\\nwith the resources required to train them, this excludes most people from access to cutting-edge ML\\ntechnology. Another option is for an organization to own the end-to-end infrastructure of model\\ndeployment, and make it accessible via an API. This allows for the implementation of safety protocols\\nlike use case restriction (only allowing the model to be used for certain applications), monitoring\\nfor misuse and revoking access to those who misuse the system, and rate limiting to prevent the\\ngeneration of large-scale misinformation. However, this can come at the cost of reduced transparency\\nand increased centralization of power because it requires the API provider to make decisions on\\nwhere to draw the line on each of these questions.\\nFinally, as discussed in Section 5.2, the question of who these models are aligned to is extremely\\nimportant, and will signiï¬cantly affect whether the net impact of these models is positive or negative.\\n20Acknowledgements\\nFirst, we would like to thank Lilian Weng, Jason Kwon, Boris Power, Che Chang, Josh Achiam,\\nSteven Adler, Gretchen Krueger, Miles Brundage, Tyna Eloundou, Gillian Hadï¬eld, Irene Soliaman,\\nChristy Dennison, Daniel Ziegler, William Saunders, Beth Barnes, Cathy Yeh, Nick Cammaratta,\\nJonathan Ward, Matt Knight, Pranav Shyam, Alec Radford, and others at OpenAI for discussions\\nthroughout the course of the project that helped shape our research direction. We thank Brian Green,\\nIrina Raicu, Subbu Vincent, Varoon Mathur, Kate Crawford, Su Lin Blodgett, Bertie Vidgen, and Paul\\nRÃ¶ttger for discussions and feedback on our approach. Finally, we thank Sam Bowman, Matthew',\n",
              "  'question': 'What measures can be taken to ensure the safe and ethical use of large language models in the domains of political advertisements, law enforcement, and other applications without proper regulation?\\n\\n',\n",
              "  'answer': 'One measure that can be taken is for an organization to own the end-to-end infrastructure of model deployment and make it accessible via an API. This allows for the implementation of safety protocols like use case restriction, monitoring for misuse, and revoking access to those who misuse the system. However, this can come at the cost of reduced transparency and increased centralization of power because it requires the API provider to make decisions on where to draw the line on each of these questions. Another option is to restrict large language model access to a few organizations with the resources required to train them, which excludes most people from access to cutting-edge ML technology. Additionally, proper regulation is necessary to limit harmful applications in these and other domains. The question of who these models are aligned to is also extremely important and will significantly affect whether the net impact of these models is positive or negative.',\n",
              "  'source_doc': 'Training language models to follow instructions with human feedback.pdf'},\n",
              " {'context': 'intermediate tokens) on harder problems. To isolate the effect\\nof variable computation from chain-of-thought reasoning, we\\ntest a conï¬guration where the model is prompted to output a\\nonly sequence of dots ( :::) equal to the number of characters in\\nthe equation needed to solve the problem. This variant performs\\nabout the same as the baseline, which suggests that variable\\ncomputation by itself is not the reason for the success of chain-\\nof-thought prompting, and that there appears to be utility from\\nexpressing intermediate steps via natural language.\\nChain of thought after answer. Another potential beneï¬t of\\nchain-of-thought prompting could simply be that such prompts\\nallow the model to better access relevant knowledge acquired\\nduring pretraining. Therefore, we test an alternative conï¬gura-\\ntion where the chain of thought prompt is only given after the\\nanswer, isolating whether the model actually depends on the\\nproduced chain of thought to give the ï¬nal answer. This variant\\nperforms about the same as the baseline, which suggests that\\nthe sequential reasoning embodied in the chain of thought is\\nuseful for reasons beyond just activating knowledge.\\n3.4 Robustness of Chain of Thought\\nGSM8K05101520Solve rate (%)Standard prompting\\nChain-of-thought prompting\\n\\x01different annotator (B)\\n\\x01different annotator (C)\\n\\x01intentionally concise style\\n\\x01exemplars from GSM8K ( \\x0b)\\n\\x01exemplars from GSM8K ( \\x0c)\\n\\x01exemplars from GSM8K ( \\r)\\nMAWPS0204060\\nFigure 6: Chain-of-thought prompting\\nhas variance for different prompt exam-\\nples (as expected) but outperforms stan-\\ndard prompting for various annotators as\\nwell as for different exemplars.Sensitivity to exemplars is a key consideration of prompt-\\ning approachesâ€”for instance, varying the permutation of\\nfew-shot exemplars can cause the accuracy of GPT-3 on\\nSST-2 to range from near chance (54.3%) to near state of\\nthe art (93.4%) (Zhao et al., 2021). In this ï¬nal subsec-\\ntion, we evaluate robustness to chains of thought written',\n",
              "  'question': 'What is the purpose of testing a configuration where the model is prompted to output a sequence of dots equal to the number of characters in the equation needed to solve the problem?\\n\\n',\n",
              "  'answer': 'The purpose of testing this configuration is to isolate the effect of variable computation from chain-of-thought reasoning. It performs about the same as the baseline, which suggests that variable computation by itself is not the reason for the success of chain-of-thought prompting, and that there appears to be utility from expressing intermediate steps via natural language.',\n",
              "  'source_doc': 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.pdf'},\n",
              " {'context': 'as instances requiring knowledge not present in the August 2019 Wikipedia dump have been removed.\\nMassively-Multitask Language Understanding (MMLU). Our second main evaluation benchmark\\nis MMLU (Hendrycks et al., 2021), which contains 57 multi-choice question answering datasets (referred\\nto as domains), sourced from real examinations designed for humans. These cover a very broad range of\\ntopics, e.g. high school mathematics, professional law, logical fallacies and clinical knowledge and can be\\nbroadly categorized in four subsets: humanities, social sciences, STEM and â€œotherâ€. We focus on few-shot\\nlearning, and the authors of the benchmarks suggest to use 5 training examples per domain. Beyond the\\n5-shot setting, We also consider three additional settings. The ï¬rst is a zero-shot setting, with no training\\ndata at all. The second, which we call multi-task few-shot , is where we train a single model on the 6-shot\\ndata from all tasks, hence leading to a training set of 285 examples. The last, which we call transfer learning ,\\nleverages additional training examples from other multiple-choice QA tasks provided by the MMLU authors,\\nnamely MCTest (Richardson, 2013), RACE (Lai et al., 2017), ARC (Clark et al., 2018) and OBQA (Mihaylov\\net al., 2018) leading to a training set of 95k examples.\\nAdditional benchmarks. Additionally, we report results on the original open-domain versions of the\\npopular NaturalQuestions (Kwiatkowski et al., 2019), and TriviaQA (Joshi et al., 2017) datasets. We also\\nevaluate our model on the original version of FEVER (Thorne et al., 2018), which presents fact checking as a\\nthree-way classiï¬cation problem for textual claims (either â€œSupportedâ€: the text is supported by evidence in\\nWikipedia, â€œrefutedâ€: the claim is not consistent with evidence in Wikipedia, or â€œnot enough infoâ€, where\\nthere is insuï¬ƒcient evidence to make a judgement). We also perform experiments to assess temporal sensitivity',\n",
              "  'question': 'Can MMLU benchmark be used to evaluate the performance of a language model in answering multi-choice question answering tasks?\\n\\n',\n",
              "  'answer': \"Yes, MMLU benchmark can be used to evaluate the performance of a language model in answering multi-choice question answering tasks. The benchmark contains 57 multi-choice question answering datasets sourced from real examinations designed for humans, covering a broad range of topics such as high school mathematics, professional law, logical fallacies, and clinical knowledge. The benchmark focuses on few-shot learning and suggests using 5 training examples per domain. Additionally, the benchmark considers three additional settings: zero-shot, multi-task few-shot, and transfer learning. The zero-shot setting has no training data at all, the multi-task few-shot setting trains a single model on the 6-shot data from all tasks, leading to a training set of 285 examples, and the transfer learning setting leverages additional training examples from other multiple-choice QA tasks provided by the MMLU authors. Overall, MMLU benchmark provides a comprehensive evaluation of a language model's ability to answer multi-choice question answering tasks.\",\n",
              "  'source_doc': 'Atlas_  Few-shot Learning with Retrieval Augmented Language Models.pdf'},\n",
              " {'context': 'T0 (Sanh et al., 2021) datasets (see Appendix C for details). We ï¬nd that these models perform better\\nthan GPT-3, on par with GPT-3 with a well-chosen prompt, and worse than our SFT baseline. This\\nindicates that these datasets are not sufï¬ciently diverse to improve performance on our API prompt\\n12distribution. In a head to head comparison, our 175B InstructGPT model outputs were preferred over\\nour FLAN model 78 \\x064% of the time and over our T0 model 79 \\x064% of the time. Likert scores for\\nthese models are shown in Figure 5.\\nWe believe our InstructGPT model outperforms FLAN and T0 for two reasons. First, public NLP\\ndatasets are designed to capture tasks that are easy to evaluate with automatic metrics, such as\\nclassiï¬cation, question answering, and to a certain extent summarization and translation. However,\\nclassiï¬cation and QA are only a small part (about 18%) of what API customers use our language\\nmodels for, whereas open-ended generation and brainstorming consist of about 57% of our prompt\\ndataset according to labelers (see Table 1). Second, it can be difï¬cult for public NLP datasets to\\nobtain a very high diversity of inputs (at least, on the kinds of inputs that real-world users would be\\ninterested in using). Of course, tasks found in NLP datasets do represent a kind of instruction that\\nwe would like language models to be able to solve, so the broadest type instruction-following model\\nwould combine both types of datasets.\\n4.2 Results on public NLP datasets\\nInstructGPT models show improvements in truthfulness over GPT-3. As measured by human\\nevaluatoins on the TruthfulQA dataset, our PPO models show small but signiï¬cant improvements\\nin generating truthful and informative outputs compared to GPT-3 (see Figure 6). This behavior is\\nthe default: our models do not have to be speciï¬cally instructed to tell the truth to exhibit improved\\ntruthfulness. Interestingly, the exception is our 1.3B PPO-ptx model, which performs slightly worse',\n",
              "  'question': 'How does the performance of language models on open-ended generation and brainstorming tasks depend on the diversity of inputs in public NLP datasets?\\n\\n',\n",
              "  'answer': 'The performance of language models on open-ended generation and brainstorming tasks depends on the diversity of inputs in public NLP datasets. According to the context, public NLP datasets may not be diverse enough to capture the types of inputs that real-world users would be interested in using, which can limit the performance of language models on these tasks. Therefore, obtaining a high diversity of inputs is important for language models to perform well on open-ended generation and brainstorming tasks.',\n",
              "  'source_doc': 'Training language models to follow instructions with human feedback.pdf'},\n",
              " {'context': 'API. They push their PaLM results to 35.1% EM by applying an additional CoT step with self-\\nconsistency, which may resemble our ensemble approach in the sense of aggregating multiple an-\\nswers. Trivedi et al. (2022) reports 49% using a pipeline with code-davinci-002 LM on a sample\\nof 500 HotPotQA questions.\\n8 C ONCLUSION\\nThis paper introduced DSPy, a new programming model for designing AI systems using pipelines\\nof pretrained LMs and other tools. We presented three new concepts introduced in this abstraction\\n(DSPy signatures, modules, and teleprompters), and showed in two very different case studies that\\nit supports rapid development of highly effective systems that use relatively small LMs. We have\\nmaintained open-source versions of this framework for close to a year. In this period, we have seen\\nand created a large number of programs that were compiled to high-quality systems by DSPy, span-\\nning tasks from information extraction to low-resource synthetic data generation. In the interest of\\nspace and to maintain reasonable scope in this paper, we leave reporting on such tasks under con-\\ntrolled experimental conditions to future work. While in-context learning has proved transformative\\nover the past 2â€“3 years of LM research, we argue that the true expressive power in this emerging\\nparadigm is in building sophisticated text transformation graphs in which composable modules and\\noptimizers (teleprompters) come together to leverage LMs in more systematic and reliable ways.\\nACKNOWLEDGMENTS\\nWe thank Josh Purtell for suggesting the apt name â€œtext transformation graphâ€ for the computational\\ngraph abstraction of DSPy. We thank Rick Battle, Igor Kotenkov, Lisa Li, David Hall, Ashwin\\nParanjape, Chris Manning, Percy Liang, and many researchers, developers, and users for valuable\\n11Preprint\\ndiscussions and feedback. We thank Giuseppe Attanasio for his public L ATEX GitHub-style Python\\ncode formatting gist.6',\n",
              "  'question': 'In this paper, what is the difference between the ensemble approach and the self-consistency step used to improve the performance of the API?\\n\\n',\n",
              "  'answer': 'The ensemble approach and the self-consistency step used to improve the performance of the API are related in the sense that they both involve aggregating multiple answers. However, the ensemble approach involves combining the outputs of multiple models, while the self-consistency step involves training a single model to be more consistent with itself. The self-consistency step may resemble the ensemble approach in that it involves training a single model to be more consistent with its own output, which can be seen as a form of aggregation.',\n",
              "  'source_doc': 'DSPy_  Compiling Declarative Language Model Calls into Self-Improving Pipelines.pdf'},\n",
              " {'context': 'Irina Raicu, Subbu Vincent, Varoon Mathur, Kate Crawford, Su Lin Blodgett, Bertie Vidgen, and Paul\\nRÃ¶ttger for discussions and feedback on our approach. Finally, we thank Sam Bowman, Matthew\\nRahtz, Ben Mann, Liam Fedus, Helen Ngo, Josh Achiam, Leo Gao, Jared Kaplan, Cathy Yeh, Miles\\nBrundage, Gillian Hadï¬eld, Cooper Raterink, Gretchen Krueger, Tyna Eloundou, Rafal Jakubanis,\\nand Steven Adler for providing feedback on this paper. Weâ€™d also like to thank Owain Evans and\\nStephanie Lin for pointing out the fact that the automatic TruthfulQA metrics were overstating the\\ngains of our PPO models.\\nThanks to those who contributed in various ways to the infrastructure used to train and deploy our\\nmodels, including: Daniel Ziegler, William Saunders, Brooke Chan, Dave Cummings, Chris Hesse,\\nShantanu Jain, Michael Petrov, Greg Brockman, Felipe Such, Alethea Power, and the entire OpenAI\\nsupercomputing team. Weâ€™d also like to thank Suchir Balaji for help with recalibration, to Alper\\nErcetin and Justin Wang for designing the main diagram in this paper, and to the OpenAI Comms\\nteam for helping with the release, including: Steve Dowling, Hannah Wong, Natalie Summers, and\\nElie Georges.\\nFinally, we want to thank our labelers, without whom this work would not have been possible:\\nMeave Fryer, Sara Tirmizi, James Carroll, Jian Ouyang, Michelle Brothers, Conor Agnew, Joe\\nKwon, John Morton, Emma Duncan, Delia Randolph, Kaylee Weeks, Alexej Savreux, Siam Ahsan,\\nRashed Sorwar, Atresha Singh, Muhaiminul Rukshat, Caroline Oliveira, Juan Pablo CastaÃ±o RendÃ³n,\\nAtqiya Abida Anjum, Tinashe Mapolisa, Celeste Fejzo, Caio Oleskovicz, Salahuddin Ahmed, Elena\\nGreen, Ben Harmelin, Vladan Djordjevic, Victoria Ebbets, Melissa Mejia, Emill Jayson Caypuno,\\nRachelle Froyalde, Russell M. Bernandez, Jennifer Brillo, Jacob Bryan, Carla Rodriguez, Evgeniya\\nRabinovich, Morris Stuttard, Rachelle Froyalde, Roxanne Addison, Sarah Nogly, Chait Singh.\\nR',\n",
              "  'question': 'In your paper, what was the purpose of the discussions and feedback sessions with Irina Raicu, Subbu Vincent, Varoon Mathur, Kate Crawford, Su Lin Blodgett, Bertie Vidgen, and Paul?\\n\\n',\n",
              "  'answer': 'The purpose of the discussions and feedback sessions with Irina Raicu, Subbu Vincent, Varoon Mathur, Kate Crawford, Su Lin Blodgett, Bertie Vidgen, and Paul was to discuss and provide feedback on the approach taken in the paper.',\n",
              "  'source_doc': 'Training language models to follow instructions with human feedback.pdf'},\n",
              " {'context': 'likely to identify instruction-relevant products and options by reasoning to bridge the gap between\\nnoisy observations and actions (e.g. â€œFor â€˜space-saving ottoman bench for living roomâ€™, the item\\nhas options â€˜39x18x18inchâ€™ and â€˜blueâ€™ and seems good to buy.â€). However, existing methods are\\nstill far from the performance of expert humans (Table 4), who perform signiï¬cantly more product\\nexplorations and query re-formulations that are still challenging for prompting-based methods.\\nOn the value of internal reasoning vs. external feedback To our knowledge, ReAct is the ï¬rst\\ndemonstration of combined reasoning and action using an LLM applied to an interactive environment\\nwithin a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang\\net al. (2022b), in which actions from an embodied agent are motivated by an eponymous â€œinner\\nmonologueâ€. However, IMâ€™s â€œinner monologueâ€ is limited to observations of the environment\\nstate and what needs to be completed by the agent for the goal to be satisï¬ed. In contrast, the\\nreasoning traces in ReAct for decision making is ï¬‚exible and sparse, allowing diverse reasoning\\ntypes (see Section 2) to be induced for different tasks.\\nTo demonstrate the differences between ReAct and IM, and to highlight the importance of internal\\nreasoning vs. simple reactions to external feedback, we ran an ablation experiment using a thought\\npattern composed of IM-like dense external feedback. As can be seen in Table 3, ReAct substantially\\noutperforms IM-style prompting ( ReAct-IM ) (71 vs. 53 overall success rate), with consistent\\nadvantages on ï¬ve out of six tasks. Qualitatively, we observed that ReAct-IM often made mistakes\\nin identifying when subgoals were ï¬nished, or what the next subgoal should be, due to a lack of high-\\nlevel goal decomposition. Additionally, many ReAct-IM trajectories struggled to determine where\\nan item would likely be within the ALFWorld environment, due to a lack of commonsense reasoning.',\n",
              "  'question': 'What is the difference between ReAct and Inner Monologue (IM) in terms of reasoning and decision making?\\n\\n',\n",
              "  'answer': \"ReAct and IM differ in their reasoning traces and decision making processes. ReAct allows for flexible and sparse reasoning traces that can induce diverse reasoning types for different tasks, while IM's reasoning is limited to observations of the environment state and what needs to be completed for the goal to be satisfied. In the ablation experiment, ReAct outperformed IM-style prompting with a 71 vs. 53 overall success rate, and ReAct-IM often made mistakes in identifying when subgoals were finished or what the next subgoal should be due to a lack of high-level goal decomposition. Additionally, many ReAct-IM trajectories struggled to determine where an item would likely be within the ALFWorld environment due to a lack of commonsense reasoning.\",\n",
              "  'source_doc': 'ReAct_ Synergizing Reasoning and Acting in Language Models.pdf'},\n",
              " {'context': 'Slack API) and 43 seconds for Vicuna-33B V1.3 (a 33B LLaMA-based model, running locally on\\none NVIDIA A100 GPU) to answer the question in Fig. 1.\\nWe conclude three major causes of LLMsâ€™ slow inference: (1) A large model size requires a large\\namount of memory, memory access, and computation. For example, the FP16 weights of 175B GPT-\\n3 take 350GB memory, which means at least 5 Ã—80GB A100 GPUs are needed to keep the model\\nin GPU memory. Even with enough GPUs, the heavy memory access and computation slow down\\nthe inference. (2) The attention operation in the prevailing transformer architecture is I/O bounded\\nand has a quadratic memory and computation complexity in sequence length. (3) The sequential\\ndecoding approach in inference generates tokens one by one. This approach introduces a significant\\nâˆ—Equal contribution.\\nâ€ The main updates in arXiv V2 are as follows: (1) Add the quality and efficiency evaluation of SoT on\\nGPT-4. (2) Use GPT-4 as the judge for answer quality evaluation. The old results with ChatGPT-3.5 as the\\njudge are moved to App. I.3. (3) Add the SoT with Router (SoT-R) method (Â§ 4) which adaptively triggers SoT\\non suitable questions. (4) Move detailed answer analysis to the appendices.\\n1arXiv:2307.15337v2  [cs.CL]  8 Oct 2023Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding\\nAnswer1.Active listening involves fully concentrating on â€¦2.Identify issues. Look into the root causes of â€¦3.Compromise. Look for a middle ground â€¦What are the most effective strategies for conflict resolution in the workplace?QuestionSkeleton-of-Thought Decoding\\nGeneratesanswerssequentially â”SlowerNormal Decoding1. Active listening2. Identify issues3. CompromiseGeneratesanswersinparallelâ”Faster(1)Skeletonstage(2)Point-expandingstage\\n1.0 1.2 1.4 1.6 1.8\\nSpeed-upâˆ’0.20.00.20.4Net win ratesVicuna-13B V1.3StableVicuna-13B\\nUltraLM-13B\\nVicuna-33B V1.3\\nLLaMA2-Chat-7B\\nLLaMA2-Chat-13BVicuna-7B V1.3ChatGPT-3.5\\nClaude\\nVicuna-7B V1.1OpenChat-13BGPT-4\\nBaseline',\n",
              "  'question': \"What are the main causes of LLMs' slow inference?\\n\\n\",\n",
              "  'answer': \"The main causes of LLMs' slow inference are a large model size requiring a large amount of memory, memory access, and computation, the attention operation in the transformer architecture being I/O bounded and having a quadratic memory and computation complexity in sequence length, and the sequential decoding approach in inference generating tokens one by one.\",\n",
              "  'source_doc': 'Skeleton-of-Thought_ Large Language Models Can Do Parallel Decoding.pdf'},\n",
              " {'context': 'where the conditional probabilities are modeled\\nby employing a causal self-attention mask ( Rad-\\nford et al. ,2018 ). Notably, leading LMs such\\nas GPT-2 ( Radford et al. ,2019 ), GPT-3 ( Brown\\net al.,2020 ), OPT ( Zhang et al. ,2022 ) or Jurassic-\\n1 (Lieber et al. ,2021 ) follow this simple parame-\\nterization.\\nRetrieval augmented language models (RALMs)\\nadd an operation that retrieves one or more docu-\\nments from an external corpus C, and condition the\\nabove LM predictions on these documents. Speci\\ue000-\\ncally, for predicting xi, the retrieval operation from\\nCdepends on its pre\\ue000x: RC(x<i), so the most\\ngeneral RALM decomposition is: p(x 1, ..., x n) =\\ue007n\\ni=1p(x i|x<i,RC(x<i)). In order to condition\\nthe LM generation on the retrieved document, pre-\\nvious RALM approaches used specialized architec-\\ntures or algorithms (see Â§ 2). Inspired by the suc-\\ncess of In-Context Learning ( Brown et al. ,2020 ;\\nDong et al. ,2023 ),In-Context RALMrefers to the\\nfollowing speci\\ue000c, simple method of concatenatingthe retrieved documents2within the Transformerâ€™s\\ninput prior to the pre\\ue000x (see Figure 2),which does\\nnot involve altering the LM weightsÎ¸:\\np(x 1, ..., x n) =\\nn\\ue009\\ni=1pÎ¸(xi|[R C(x<i);x <i]),(2)\\nwhere [a;b] denotes the concatenation of strings a\\nandb.\\nSince common Transformer-based LM imple-\\nmentations support limited length input sequences,\\nwhen the concatenation of the document and the\\ninput sequence exceed this limit we remove to-\\nkens from the beginning of xuntil the overall input\\nlength equals that allowed by the model. Because\\nour retrieved documents are passages of limited\\nlength, we always have enough context left from x\\n(see Â§ 4.3).\\n3.2 RALM Design Choices\\nWe detail below two practical design choices often\\nmade in RALM systems. In Â§ 5, we investigate the\\neffect of these in the setting of In-Context RALM.\\nRetrieval StrideWhile in the above formulation\\na retrieval operation can occur at each generation\\nstep, we might want to perform retrieval only once',\n",
              "  'question': 'In retrieval augmented language models (RALMs), how is the conditional probability of a sequence of words modeled?\\n\\n',\n",
              "  'answer': \"In RALMs, the conditional probability of a sequence of words is modeled by employing a causal self-attention mask and concatenating the retrieved documents within the Transformer's input prior to the preprocessing. The most general RALM decomposition is: p(x 1, ..., x n) = Î¸(x i|x <i, RC(x <i)), where [a;b] denotes the concatenation of strings a and b, and [R C(x <i);x <i] denotes the concatenation of the retrieved document and the input sequence. Common Transformer-based LM implementations support limited length input sequences, so when the concatenation of the document and the input sequence exceeds this limit, a retrieval stride design choice is often made to remove tensors from the beginning of x until the overall input length equals that allowed by the model.\",\n",
              "  'source_doc': 'In-Context Retrieval-Augmented Language Models.pdf'},\n",
              " {'context': '33B OASST1 16 1e-4 1875 - 512\\n33B HH-RLHF 32 1e-4 5000 - 768\\n33B Longform 32 1e-4 2343 512 1024\\n65B All 64 1e-4 2500 384 128\\n65B OASST1 16 1e-4 1875 - 512\\n65B HH-RLHF 64 1e-4 2500 - 768\\n65B Longform 32 1e-4 2343 512 1024\\nTable 9: Training hyperparameters for QL ORA finetuning on different datasets and across model sizes.\\nSelf-Instruct, Alpaca, Unnatural Instructions The Self-Instruct, Alpaca, and Unnatural Instruc-\\ntions datasets [ 59,55,26] are instruction tuning datasets collected with various approaches of model\\ndistillation from GPT-3 Instruct and ChatGPT. They rely on prompting, in-context learning, and\\nparaphrasing to come up with diverse sets of instructions and outputs. The datasets comprise of\\n82,612, 51,942, and 240,670 examples respectively. One advantage of such distilled datasets is that\\nthey contain a more diverse set of instruction styles compared to the FLAN v2 collection and similar\\ninstruction tuning collections.\\nLongform The LongForm dataset [ 30] is based on an English corpus augmented with instructions\\nand as such is a hybrid human-generated dataset. The underlying documents are human-written and\\ncome from C4 and Wikipedia while the instructions are generated visa LLMs. The dataset is extended\\nwith additional structured corpora examples such as Stack Exchange and WikiHow and task examples\\nsuch as question answering, email writing, grammar error correction, story/poem generation, and text\\nsummarization. The dataset contains 23,700 examples.\\nChip2 is part of the OIG Laion dataset. It contains Python code examples, natural instruction exam-\\nples, generic harmless instructions, instruction/responses with lists, follow-up questions, Wikipedia\\ntoxic adversarial questions, grade school math, reasoning instructions, and character and scene\\ndescriptions with a total of 210,289 examples.\\nB.2 Hyperparameters\\nWe provide the exact hyperparameters used in our QLORAfinetuning experiments. We find hyper-',\n",
              "  'question': 'What were the hyperparameters used in the QL ORA finetuning experiments on various instruction tuning datasets and model sizes?\\n\\n',\n",
              "  'answer': 'The hyperparameters used in the QL ORA finetuning experiments on various instruction tuning datasets and model sizes are provided in the table. The hyperparameters include the learning rate, batch size, number of epochs, number of hidden layers, number of units per layer, dropout rate, and regularization strength. The specific hyperparameters used for each dataset and model size are listed in the table.',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': 'considerable interest (Graves et al., 2014; Kurach et al., 2016; Kaiser & Sutskever, 2016; Graves\\net al., 2016; Reed & de Freitas, 2016; Veli Ë‡ckovi Â´c et al., 2020a;b). This work proposes new neural\\narchitectures, inspired by theoretical models of computation, whose inductive bias allows them to\\nmore easily learn algorithm induction tasks. Several methods for algorithm induction speciï¬cally\\nadd adaptive computation time to sequence models (Graves, 2016; Dehghani et al., 2018; Banino\\net al., 2021). In particular, universal transformers include adaptive computation time, and are eval-\\nuated both on algorithm induction and on learning to execute tasks (Dehghani et al., 2018). In\\ncontrast, a scratchpad is a simple way both to provide a transformer model with adaptive compu-\\ntation time, and also to provide supervision about how to use that additional computation, without\\nrequiring modiï¬cation to the underlying architecture.\\nAlgorithm induction has also been connected to pre-trained models. Lu et al. (2021) show that\\nTransformers can be used to some extent as universal computation engines, by pre-training on natu-\\nral language, and ï¬ne-tuning a small fraction of the weights on non-language tasks, including simple\\nalgorithm induction tasks. Finally, supervised approaches to semantic parsing (Zelle & Mooney,\\n1996; Zettlemoyer & Collins, 2005; Kwiatkowksi et al., 2010; Wong & Mooney, 2006) predict the\\ntext of a database query, which can then be executed to answer a natural language question.\\n7 L IMITATIONS AND FUTURE WORK\\nContext window size In this work, we limit our experiments to problems where the scratchpad\\ntext ï¬ts within the model generation window (512 tokens). However, many problems require very\\nlong scratchpad generations. Therefore, fully realizing the potential of the scratchpad technique\\nmay require further improvements in transformer generation window size. This is an active area',\n",
              "  'question': 'Can you explain how the scratchpad technique provides adaptive computation time to transformer models?\\n\\n',\n",
              "  'answer': 'The scratchpad technique provides adaptive computation time to transformer models by allowing them to generate text within a given window size. This allows the model to generate the necessary scratchpad text to execute a task before executing it. The scratchpad text is generated using the transformer model, and the resulting output is used to execute the task. The scratchpad technique does not require modification to the underlying architecture, making it a simple and effective way to provide adaptive computation time to transformer models.',\n",
              "  'source_doc': 'Show Your Work_  Scratchpads for Intermediate Computation with Language Models.pdf'},\n",
              " {'context': 'system prompt or instruction format used during training if publicly available. We also compare our\\nmethod to concurrent work, CoVE 65B(Dhuliawala et al., 2023), which introduces iterative prompt\\nengineering to improve the factuality of LLM generations.\\nBaselines with retrievals. We evaluate models augmented with retrieval at test time or during training.\\nThe first category includes standard RAG baselines, where an LM (Llama2, Alpaca) generates output\\ngiven the query prepended with the top retrieved documents using the same retriever as in our system.\\nIt also includes Llama2-FT, where Llama2 is fine-tuned on all training data we use without the\\nreflection tokens or retrieved passages. We also report the result of retrieval-augmented baselines\\nwith LMs trained with private data: Ret-ChatGPT and Ret-Llama2-chat, which deploy the same\\naugmentation technique above, as well as perplexity.ai, an InstructGPT-based production search\\nsystem. The second category includes concurrent methods that are trained with retrieved text\\npassages, i.e., SAIL (Luo et al., 2023) to instruction-tune an LM on the Alpaca instruction-tuning\\ndata with top retrieved documents inserted before instructions, and Toolformer (Schick et al., 2023)\\nto pre-train an LM with API calls (e.g., Wikipedia APIs).6\\n4.3 E XPERIMENTAL SETTINGS\\nTraining data and settings. Our training data consists of diverse instruction-following input-output\\npairs. In particular, we sample instances from Open-Instruct processed data (Wang et al., 2023) and\\nknowledge-intensive datasets (Petroni et al., 2021; Stelmakh et al., 2022; Mihaylov et al., 2018). In\\ntotal, we use 150k instruction-output pairs. We use Llama2 7B and 13B (Touvron et al., 2023) as\\nour generator base LM, and we use Llama2 7B as our base critic LM. For the retriever model R, we\\nuse off-the-shelf Contriever-MS MARCO (Izacard et al., 2022a) by default and retrieve up to ten\\ndocuments for each input. More training details are in the Appendix Section B.1.',\n",
              "  'question': 'In this context, what is the difference between the first and second categories of baselines with retrievals?\\n\\n',\n",
              "  'answer': 'The first category of baselines with retrievals includes standard RAG baselines where an LM generates output given the query prepended with the top retrieved documents using the same retriever as in the system. It also includes Llama2-FT, where Llama2 is fine-tuned on all training data without the reflection tokens or retrieved passages. The second category includes concurrent methods that are trained with retrieved text passages, such as SAIL and Toolformer.',\n",
              "  'source_doc': 'Self-Rag_ Learning to Retrieve, Generate, and Critique Through Self-Reflection.pdf'},\n",
              " {'context': 'enable the LLMs to generate high-quality reason-ing steps leading to fewer calculation errors.\\nCorrelation Analysis of Generated Reasoning\\nand Error Types. To obtain deeper insight into\\nthe impact of PS+ prompting on error types, we\\nexamine the correlation between the sub-parts of\\nthe generated reasoning and error types. Specifi-\\ncally, we analyze the existence of variable defini-\\ntion, reasoning plan, and solution in the generated\\nreasoning text and correlate them with the three\\nerror types. The set of problems used for this anal-\\nysis study is the same as that used in the earlier\\nerror type analysis. Figure 5 shows the correla-\\ntion matrix among the existence of variable defi-\\nnitions, plans, solutions and three different types\\nof errors. It is observed that both variable defini-\\ntion and plan existences have a negative correlation\\nwith calculation errors and missing-reasoning-step\\nerrors. The Zero-shot-PS+ prompt can further im-\\nprove the performance of LLMs on mathematical\\nreasoning problems by reducing calculation errors\\nand missing-reasoning-step errors.\\nExploring the Presence of Plans in PS Predic-\\ntions. To ascertain the presence of a plan in each\\nprediction made by PS, we conducted a random\\nsampling of 100 data examples and examined their\\ncorresponding predictions. Our analysis reveals\\nthat 90 of the 100 predictions indeed incorporated\\na plan. This observation indicates the emergenceCalculation Step Missing SemanticVariables Plan Solution-0.41 -0.56 0.76\\n-0.02 -0.83 0.7\\n-0.42 0.076 0.24\\n0.8\\n0.6\\n0.4\\n0.2\\n0.00.20.40.6Figure 5: Correlation analysis of generated reasoning\\nand error types of randomly sampled 100 data examples\\nfrom GSM8K for Zero-shot-PS+.\\nof strong planning abilities in recent LLMs such as\\nGPT-3.5 and GPT-4.\\n5 Related Work\\n5.1 Reasoning in NLP\\nIt is well known that complex reasoning prob-\\nlems are challenging for NLP models, and such\\nproblems include mathematical reasoning (Cobbe\\net al., 2021; Patel et al., 2021; Ling et al., 2017;',\n",
              "  'question': 'What is the correlation between the sub-parts of the generated reasoning and error types in complex mathematical reasoning problems?\\n\\n',\n",
              "  'answer': 'The correlation analysis revealed that both variable definition and reasoning plan existences have a negative correlation with calculation errors and missing-reasoning-step errors. Furthermore, the presence of plans in PS predictions was found to be strongly correlated with improved performance on mathematical reasoning problems.',\n",
              "  'source_doc': 'Plan-and-Solve Prompting_  Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models.pdf'},\n",
              " {'context': 'speciï¬c downstream tasks that were learned but not emphasized in the general pre-training model .\\n8 C ONCLUSION AND FUTURE WORK\\nFine-tuning enormous language models is prohibitively expensive in terms of the hardware required\\nand the storage/switching cost for hosting independent instances for different tasks. We propose\\nLoRA, an efï¬cient adaptation strategy that neither introduces inference latency nor reduces input\\nsequence length while retaining high model quality. Importantly, it allows for quick task-switching\\nwhen deployed as a service by sharing the vast majority of the model parameters. While we focused\\non Transformer language models, the proposed principles are generally applicable to any neural\\nnetworks with dense layers.\\nThere are many directions for future works. 1) LoRA can be combined with other efï¬cient adapta-\\ntion methods, potentially providing orthogonal improvement. 2) The mechanism behind ï¬ne-tuning\\nor LoRA is far from clear â€“ how are features learned during pre-training transformed to do well\\non downstream tasks? We believe that LoRA makes it more tractable to answer this than full ï¬ne-\\n12tuning. 3) We mostly depend on heuristics to select the weight matrices to apply LoRA to. Are\\nthere more principled ways to do it? 4) Finally, the rank-deï¬ciency of \\x01Wsuggests that Wcould\\nbe rank-deï¬cient as well, which can also be a source of inspiration for future works.\\nR',\n",
              "  'question': 'Can LoRA be combined with other efficient adaptation methods to provide orthogonal improvement?\\n',\n",
              "  'answer': 'Yes, LoRA can be combined with other efficient adaptation methods to potentially provide orthogonal improvement.',\n",
              "  'source_doc': 'LoRA_ Low-Rank Adaptation of Large Language Models.pdf'},\n",
              " {'context': 'scores are returned, i.e., v1, ..., v h=R(G, pÎ¸, h).\\nSpecific forms of EandRdepend on a use case. We dis-\\ncuss the details in Section 5. For example, the score (or rank)\\nfor sorting corresponds to the count of elements correctly\\nsorted (or incorrectly, when obtaining the error as a score).\\n4 System Architecture & Extensibility\\nThe GoT architecture consists of a set of interacting mod-\\nules, see Figure 3 (the blue part). These modules are the\\nPrompter (prepares the messages for the LLM), the Parser\\n(extracts information from LLMsâ€™ replies), the Scoring\\nmodule (verifies and scores the LLM replies), and the Con-\\ntroller (coordinates the entire reasoning process, and decides\\non how to progress it). The Controller contains two further\\nimportant elements: the Graph of Operations (GoO) and the\\nGraph Reasoning State (GRS). GoO is a static structure thatspecifies the graph decomposition of a given task , i.e., it pre-\\nscribes transformations to be applied to LLM thoughts, to-\\ngether with their order & dependencies. GRS is a dynamic\\nstructure that maintains the state of the ongoing LLM rea-\\nsoning process (the history of its thoughts and their states).\\n4.1 Prompter\\nThe Prompter prepares the prompt to be sent to the LLM.\\nThis module is responsible for the specifics of encoding the\\ngraph structure within the prompt. The GoT architecture en-\\nables the user to implement use-case specific graph encod-\\nings by providing full access to the graph structure.\\n4.2 Parser\\nThe Parser extracts information from LLMâ€™s thoughts. For\\neach such thought, the Parser constructs the thought state ,\\nwhich contains this extracted information. The thought state\\nis then used to update GRS accordingly.\\n4.3 Scoring & Validation\\nHere, we verify whether a given LLMâ€™s thought satisfies po-\\ntential correctness conditions, and then we assign it a score.\\nDepending on how the score is derived, the module may\\nconsult the LLM. Moreover, depending on the use case, the',\n",
              "  'question': 'Given that scores are returned as v1, ..., vh=R(G, pÎ¸, h), what is the specific form of E in the context of GoT architecture?\\n\\n',\n",
              "  'answer': 'The specific form of E in the context of GoT architecture depends on the use case. However, one example mentioned is the score (or rank) for sorting, which corresponds to the count of elements correctly sorted (or incorrectly, when obtaining the error as a score).',\n",
              "  'source_doc': 'Graph of Thoughts_ Solving Elaborate Problems with Large Language Models.pdf'},\n",
              " {'context': 'search (BFS) or depth-ï¬rst search (DFS), which allow systematic exploration of the tree of thoughts\\nwith lookahead and backtracking.\\nEmpirically, we propose three new problems that challenge existing LM inference methods even with\\nthe state-of-the-art language model, GPT-4 [ 20]: Game of 24, Creative Writing, and Crosswords\\n(Table 1). These tasks require deductive, mathematical, commonsense, lexical reasoning abilities,\\nand a way to incorporate systematic planning or search. We show ToT obtains superior results on\\nall three tasks by being general and ï¬‚exible enough to support different levels of thoughts, different\\nways to generate and evaluate thoughts, and different search algorithms that adapt to the nature of\\ndifferent problems. We also analyze how such choices affect model performances via systematic\\nablations and discuss future directions to better train and use LMs.\\n2 Background\\nWe ï¬rst formalize some existing methods that use large language models for problem-solving,\\nwhich our approach is inspired by and later compared with. We use p\\x12to denote a pre-trained LM\\nwith parameters \\x12, and lowercase letters x;y;z;s;\\x01\\x01\\x01to denote a language sequence , i.e.x=\\n(x[1];\\x01\\x01\\x01;x[n])where eachx[i]is a token, so that p\\x12(x) =Qn\\ni=1p\\x12(x[i]jx[1:::i]). We use uppercase\\nlettersS;\\x01\\x01\\x01to denote a collection of language sequences.\\nInput-output (IO) prompting is the most common way to turn a problem input xinto outputywith\\nLM:y\\x18p\\x12(yjpromptIO(x)), where promptIO(x)wraps inputxwith task instructions and/or few-\\nshot input-output examples. For simplicity, let us denote pprompt\\n\\x12(outputjinput ) =p\\x12(outputj\\nprompt (input )), so that IO prompting can be formulated as y\\x18pIO\\n\\x12(yjx).\\n2Chain-of-thought (CoT) prompting [35] was proposed to address cases where the mapping of\\ninputxto outputyis non-trivial (e.g. when xis a math question and yis the ï¬nal numerical answer).\\nThe key idea is to introduce a chain of thoughtsz1;\\x01\\x01\\x01;znto bridgexandy, where each ziis a',\n",
              "  'question': 'In this context, what is the difference between input-output (IO) prompting and chain-of-thought (CoT) prompting for using large language models for problem-solving?\\n\\n',\n",
              "  'answer': 'In IO prompting, the input is transformed into output by directly applying the pre-trained language model with parameters \\x12, while in CoT prompting, a chain of thought is introduced to bridge the gap between the input and output. The chain of thought consists of multiple intermediate steps that are generated and evaluated by the language model to arrive at the final output.',\n",
              "  'source_doc': 'Tree of Thoughts_  Deliberate Problem Solving with Large Language Models.pdf'},\n",
              " {'context': 'are the â€™deplorablesâ€™ and who are the â€™parallel struggles?â€™\" after the jihad. Clements\\nalso said he found Bansaris and Jolyon Friesen guilty of helping incite hatred and hate.\\nZean Korah, a Supreme Court of Canada judge and former face of hate speech at the Toronto\\ndetention centre, will hold a preliminary hearing at Wednesdayâ€™s district court, Clements\\nsaid. Article Continued Below Zean Joeberg will be heading up an \"anti-fascist group\";\\nas Ziya Janette Sharma will be in charge of the prosecution, Clements agreed. He said\\nhe didnâ€™t anticipate Clements making such a verdict in the light of events in Ferguson,\\nMo., and that findings \"are not to be taken lightly,\" Clements said. \"Itâ€™s the judgeâ€™s\\njob to look like weâ€™ve done well in addressing this issue,\" Clements said. The judge\\nsentenced Brazau to nine months in prison, awarding him $500,000 for fighting the charges.\\nThe sentence came after a two-day trial in Toronto where he stood with fellow protesters\\nin front of Chief of Staff at the Toronto Police Service, then endorsed the suspect\\nin the flagrant breach of secrecy. He was scheduled to appear in St. Johnâ€™s Superior\\nCourt on Monday. During the trial, meanwhile, some of his accusers sat in on the most\\ncontentious and contested event of his life, the lawyer arguing that the judge should\\nhave subjected Brazau to such an intense chilling procedure that could have been triggered\\nby his admission of hate speech. He also called the reality of being falsely accused by\\nthe St. Johnâ€™s Residents Association of mistreating his client a \"victory\" instead of\\n\"a blow to his reputation.\" Clements said the judge was aware of so many anti-jihadary\\ndevelopments as he had been studying the case on a trip to the New York area last month.\\nBut he rejected allegations of political bias, saying that Brazauâ€™s remarks were \"symbolic\\nand ironic.\" Clements took issue with allegations of the judge condoning violence, saying:\\n\"Thereâ€™s some type!',\n",
              "  'question': 'Given the context, what were the \"parallel struggles\" mentioned by Clements after the jihad?\\n\\n',\n",
              "  'answer': 'The \"parallel struggles\" mentioned by Clements after the jihad were not explicitly stated in the provided context.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'We evaluate the impact of using FlashAttention to train Transformer models. We validate two claims\\nabout training time and model accuracy, and report attention runtime and memory benchmarks.\\nâ€¢Training Speed. FlashAttention outperforms the MLPerf 1.1 [ 58] speed record for BERT by 15%, and\\nspeeds up GPT-2 up to 3 \\x02over HuggingFace [ 87] and 1\\x958\\x02over Megatron [ 77] over standard Transformers.\\nFlashAttention speeds up the long-range arena (LRA) benchmark 2.4 \\x02.\\nâ€¢Quality. FlashAttention scales Transformers to longer sequences, yielding higher quality. FlashAt-\\ntention trains GPT-2 with context length 4K faster than Megatron trains GPT-2 with context length\\n1K, while achieving 0.7 better perplexity. Modeling longer sequences yields 6.4 points of lift on two long-\\ndocument classiï¬cation tasks. Finally, FlashAttention yields the ï¬rst Transformer that can achieve\\nbetter-than-random performance on the challenging Path-X task (sequence length 16K), and block-sparse\\nFlashAttention yields the ï¬rst sequence model that we know of that can achieve better-than-random\\nperformance on Path-256 (sequence length 64K).\\nâ€¢Benchmarking Attention. We measure the runtime and memory performance of FlashAttention\\nand block-sparse FlashAttention based on sequence length. We conï¬rm that the memory footprint\\nofFlashAttention scales linearly with seq. length and is up to 3 \\x02faster than standard attention for\\ncommon seq. lengths (up to 2K). We conï¬rm that runtime of block-sparse FlashAttention scales linearly\\nin seq. length and is faster than all existing approximate attention baselines.\\nAdditional experiment details are in Appendix E.\\n4.1 Faster Models with FlashAttention\\nBERT. FlashAttention yields the fastest single-node BERT training speed that we know of. We train a\\nBERT-large [ 22] model with FlashAttention on Wikipedia. Table 1 compares our training time to the\\nimplementation from Nvidia that set the training speed record for MLPerf 1.1 [ 58]. Our implementation is\\n15% faster.',\n",
              "  'question': 'Does using FlashAttention improve the training speed and accuracy of Transformer models?\\n\\n',\n",
              "  'answer': 'Yes, using FlashAttention improves the training speed and accuracy of Transformer models. FlashAttention outperforms the MLPerf 1.1 speed record for BERT by 15%, speeds up GPT-2 up to 3 times over standard Transformers, and achieves higher quality with lower perplexity and better performance on long-document classification tasks. Additionally, FlashAttention scales linearly with sequence length and has a faster runtime than existing approximate attention baselines.',\n",
              "  'source_doc': 'FlashAttention_  Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf'},\n",
              " {'context': 'ranking function is based on term and inverse doc-\\nument frequencies. We use the implementation\\nfrom Apache Lucene1with default parameters, and\\ntokenize questions and passages with SpaCy.2In\\nDPR, passages and questions are represented as\\ndense vector representations, computed using two\\nBERT networks. The ranking function is the dot\\nproduct between the query and passage represen-\\ntations. Retrieval is performed using approximate\\nnearest neighbors with the FAISS library.3\\nReading. Our generative model for open domain\\nQA is based on a sequence-to-sequence network,\\npretrained on unsupervised data, such as T5 or\\nBART (Raffel et al., 2020; Lewis et al., 2020a).\\nThe model takes as input the question, as well as\\nthe support passages, and generates the answer.\\nMore precisely, each retrieved passage and its title\\nare concatenated with the question, and processed\\nindependently from other passages by the encoder.\\nWe add special tokens question: ,title: and\\ncontext: before the question, title and text of\\neach passage. Finally, the decoder performs atten-\\n1lucene.apache.org\\n2spacy.io\\n3github.com/facebookresearch/faisstion over the concatenation of the resulting repre-\\nsentations of all the retrieved passages. The model\\nthus performs evidence fusion in the decoder only,\\nand we refer to it as Fusion-in-Decoder .\\nBy processing passages independently in the en-\\ncoder, but jointly in the decoder, this method differs\\nfrom Min et al. (2020) and Lewis et al. (2020b).\\nProcessing passages independently in the encoder\\nallows to scale to large number of contexts, as it\\nonly performs self attention over one context at a\\ntime. This means that the computation time of the\\nmodel grows linearly with the number of passages,\\ninstead of quadratically. On the other hand, pro-\\ncessing passages jointly in the decoder allows to\\nbetter aggregate evidence from multiple passages.\\n4 Experiments\\nIn this section, we report empirical evaluations of\\nFusion-in-Decoder for open domain QA.',\n",
              "  'question': 'In Fusion-in-Decoder, how does the generative model process passages?\\n\\n',\n",
              "  'answer': 'In Fusion-in-Decoder, the generative model processes passages independently in the encoder but jointly in the decoder. By processing passages independently in the encoder, the computation time of the model grows linearly with the number of passages, allowing for scalability to large numbers of contexts. On the other hand, processing passages jointly in the decoder allows for better aggregation of evidence from multiple passages.',\n",
              "  'source_doc': 'Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering.pdf'},\n",
              " {'context': 'ÆœÄË¤Æ›ÄÃ²Ë¤Æ›ÄÃ²Ä­Ã²Ë¤ÄµÆ™Ë¤ÅÃ²Ä¦Æ™ËŠÄ‘Ä­Å”Å—ÄµÅ¿Ã²Ä­Ã²Ä®ÆœË¤ÃŠÄ®Ã­Ë¤Ã²Ä­Ã¦Å—ÃŠÃ§Ä‘Ä®ÄˆË¤Ã§ÄÃŠÄ¦Ä¦Ã²Ä®ÄˆÃ²ÅÊœË¤Ä­ÃŠÄ£Ä‘Ä®ÄˆË¤Æ—ÄµÅ—Ë¤ÃŠË¤Ã§ÄµÄÃ²Å—Ã²Ä®ÆœË¤Å”ÃŠÅÅÃŠÄˆÃ²Ê›Ë¤Ë¤Ê³Ê›Ê›Ê›Ê´Ë¤\\x9aÄÃ²Ë¤Ã¦Ã²ÅÆœË¤Ã§ÄÄµÆ“Ã§Ã²Ë¤Æ“ÅË¤É¿Ê›\\x01Ä®ÃŠÄ¦Æ†ÆÄ‘Ä®ÄˆË¤Ã²ÃŠÃ§ÄË¤Ã§ÄÄµÆ“Ã§Ã²Ë¤Ä‘Ä®Ë¤Ã­Ã²Å¤ÃŠÆ“Ä¦ÊË¤Ë¤\\x1dÄÄµÆ“Ã§Ã²Ë¤É¾ÊœË¤Æ€ÄÆ“Ä¦Ã²Ë¤Ä‘Ä®Ã§ÄµÅ—Å”ÄµÅ—ÃŠÆœÄ‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤Å—Ã²Å–Å©Ä‘Å—Ã²Ã­Ë¤Ã²Ä®Ã­Ë¤ÅÃ²Ä®Å¤Ã²Ä®Ã§Ã²ÅÊœË¤ÅÃ²Ã²Ä­ÅË¤Æ˜ÄµË¤Ä¦ÃŠÃ§Ä£Ë¤ÃŠË¤Ã§Ä¦Ã²ÃŠÅ—Ë¤Ã§ÄµÄ®Ä®Ã²Ã§ÆœÆ“ÄµÄ®Ë¤Ã¦Ã²Å¤Æ€Ã²Ã²Ä®Ë¤Æ›ÄÃ²Ë¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄÅÊŸË¤\\x1dÄÄµÆ“Ã§Ã²Ë¤É¿Ë¤ÄµÆ™Ä‡Ã²Å—ÅË¤ÃŠÄ®Ë¤Ä‘Ä®Å¤Ã²Å—Ã²ÅÆœÄ‘Ä®ÄˆË¤Å”Ã²Å—ÅÅ”Ã²Ã§ÆœÆ“Å¿Ã²Ë¤Ã¦Æ†Ë¤Å©ÅÄ‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤Å—Ã²Å–Å©Ä‘Å—Ã²Ã­Ë¤Ã²Ä®Ã­Ë¤ÅÃ²Ä®Å¤Ã²Ä®Ã§Ã²ÅË¤Æ˜ÄµË¤Å”Å—Ã²ÅÃ²Ä®ÆœË¤ÃŠË¤ÅÃ²Ä¦Æ™ËŠÄÃ²Ä¦Å”Ë¤Ã¦ÄµÄµÄ£Ë™ÅË¤Ã§ÄµÄ®Å¤Ã²Ä®ÆœÊ›Ë¤GÆœË¤Ã§ÄµÄ®Ä®Ã²Ã§Å¤ÅË¤Æ›ÄÃ²Ë¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄÅË¤Æ€Æ“ÆœÄË¤Æ›ÄÃ²Ë¤Æ›ÄÃ²Ä­Ã²Ë¤ÄµÆ™Ë¤ÅÃ²Ä¦Æ™ËŠÄ‘Ä­Å”Å—ÄµÅ¿Ã²Ä­Ã²Ä®ÆœË¤ÃŠÄ®Ã­Ë¤Ã²Ä­Ã¦Å—ÃŠÃ§Ä‘Ä®ÄˆË¤Ã§ÄÃŠÄ¦Ä¦Ã²Ä®ÄˆÃ²ÅÊœË¤Ä­ÃŠÄ£Ä‘Ä®ÄˆË¤Æ—ÄµÅ—Ë¤ÃŠË¤Ã§ÄµÄÃ²Å—Ã²Ä®ÆœË¤Å”ÃŠÅÅÃŠÄˆÃ²Ê›Ë¤Ë¤Ê³Ê›Ê›Ê›Ê´Ë¤\\x9aÄÃ²Ë¤Ã¦Ã²ÅÆœË¤Ã§ÄÄµÆ“Ã§Ã²Ë¤Æ“ÅË¤É¿Ê›\\x01Ä®ÃŠÄ¦Æ†ÆÄ‘Ä®ÄˆË¤Ã²ÃŠÃ§ÄË¤Ã§ÄÄµÆ“Ã§Ã²Ë¤Ä‘Ä®Ë¤Ã­Ã²Å¤ÃŠÆ“Ä¦ÊË¤Ë¤\\x1dÄÄµÆ“Ã§Ã²Ë¤É¾ÊœË¤Æ€ÄÆ“Ä¦Ã²Ë¤Ä‘Ä®Ã§ÄµÅ—Å”ÄµÅ—ÃŠÆœÄ‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤Å—Ã²Å–Å©Ä‘Å—Ã²Ã­Ë¤Ã²Ä®Ã­Ë¤ÅÃ²Ä®Å¤Ã²Ä®Ã§Ã²ÅÊœË¤ÅÃ²Ã²Ä­ÅË¤Æ˜ÄµË¤Ä¦ÃŠÃ§Ä£Ë¤ÃŠË¤Ã§Ä¦Ã²ÃŠÅ—Ë¤Ã§ÄµÄ®Ä®Ã²Ã§ÆœÆ“ÄµÄ®Ë¤Ã¦Ã²Å¤Æ€Ã²Ã²Ä®Ë¤Æ›ÄÃ²Ë¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄÅÊŸË¤\\x1dÄÄµÆ“Ã§Ã²Ë¤É¿Ë¤ÄµÆ™Ä‡Ã²Å—ÅË¤ÃŠÄ®Ë¤Ä‘Ä®Å¤Ã²Å—Ã²ÅÆœÄ‘Ä®ÄˆË¤Å”Ã²Å—ÅÅ”Ã²Ã§ÆœÆ“Å¿Ã²Ë¤Ã¦Æ†Ë¤Å©ÅÄ‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤Å—Ã²Å–Å©Ä‘Å—Ã²Ã­Ë¤Ã²Ä®Ã­Ë¤ÅÃ²Ä®Å¤Ã²Ä®Ã§Ã²ÅË¤Æ˜ÄµË¤Å”Å—Ã²ÅÃ²Ä®ÆœË¤ÃŠË¤ÅÃ²Ä¦Æ™ËŠÄÃ²Ä¦Å”Ë¤Ã¦ÄµÄµÄ£Ë™ÅË¤Ã§ÄµÄ®Å¤Ã²Ä®ÆœÊ›Ë¤GÆœË¤Ã§ÄµÄ®Ä®Ã²Ã§Å¤ÅË¤Æ›ÄÃ²Ë¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄÅË¤Æ€Æ“ÆœÄË¤Æ›ÄÃ²Ë¤Æ›ÄÃ²Ä­Ã²Ë¤ÄµÆ™Ë¤ÅÃ²Ä¦Æ™ËŠÄ‘Ä­Å”Å—ÄµÅ¿Ã²Ä­Ã²Ä®ÆœË¤ÃŠÄ®Ã­Ë¤Ã²Ä­Ã¦Å—ÃŠÃ§Ä‘Ä®ÄˆË¤Ã§ÄÃŠÄ¦Ä¦Ã²Ä®ÄˆÃ²ÅÊœË¤Ä­ÃŠÄ£Ä‘Ä®ÄˆË¤Æ—ÄµÅ—Ë¤ÃŠË¤Ã§ÄµÄÃ²Å—Ã²Ä®ÆœË¤Å”ÃŠÅÅÃŠÄˆÃ²Ê›Ë¤Ë¤Ê³Ê›Ê›Ê›Ê´Ë¤\\x9aÄÃ²Ë¤Ã¦Ã²ÅÆœË¤Ã§ÄÄµÆ“Ã§Ã²Ë¤Æ“ÅË¤É¿Ê›\\x01Ä®ÃŠÄ¦Æ†ÆÄ‘Ä®ÄˆË¤Ã²ÃŠÃ§ÄË¤Ã§ÄÄµÆ“Ã§Ã²Ë¤Ä‘Ä®Ë¤Ã­Ã²Å¤ÃŠÆ“Ä¦ÊË¤Ë¤\\x1dÄÄµÆ“Ã§Ã²Ë¤É¾ÊœË¤Æ€ÄÆ“Ä¦Ã²Ë¤Ä‘Ä®Ã§ÄµÅ—Å”ÄµÅ—ÃŠÆœÄ‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤Å—Ã²Å–Å©Ä‘Å—Ã²Ã­Ë¤Ã²Ä®Ã­Ë¤ÅÃ²Ä®Å¤Ã²Ä®Ã§Ã²ÅÊœË¤ÅÃ²Ã²Ä­ÅË¤Æ˜ÄµË¤Ä¦ÃŠÃ§Ä£Ë¤ÃŠË¤Ã§Ä¦Ã²ÃŠÅ—Ë¤Ã§ÄµÄ®Ä®Ã²Ã§ÆœÆ“ÄµÄ®Ë¤Ã¦Ã²Å¤Æ€Ã²Ã²Ä®Ë¤Æ›ÄÃ²Ë¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄÅË¤Ê³Ê›Ê›Ê›Ê´Ë¤\\x1dÄÄµÆ“Ã§Ã²Ë¤É¿Ë¤ÄµÆ™Ä‡Ã²Å—ÅË¤ÃŠÄ®Ë¤Ä‘Ä®Å¤Ã²Å—Ã²ÅÆœÄ‘Ä®ÄˆË¤Å”Ã²Å—ÅÅ”Ã²Ã§ÆœÆ“Å¿Ã²Ë¤Ã¦Æ†Ë¤Å©ÅÄ‘Ä®ÄˆË¤Æ›ÄÃ²Ë¤Å—Ã²Å–Å©Ä‘Å—Ã²Ã­Ë¤Ã²Ä®Ã­Ë¤ÅÃ²Ä®Å¤Ã²Ä®Ã§Ã²ÅË¤Æ˜ÄµË¤Å”Å—Ã²ÅÃ²Ä®ÆœË¤ÃŠË¤ÅÃ²Ä¦Æ™ËŠÄÃ²Ä¦Å”Ë¤Ã¦ÄµÄµÄ£Ë™ÅË¤Ã§ÄµÄ®Å¤Ã²Ä®ÆœÊ›Ë¤GÆœË¤Ã§ÄµÄ®Ä®Ã²Ã§Å¤ÅË¤Æ›ÄÃ²Ë¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄÅË¤Æ€Æ“ÆœÄË¤Æ›ÄÃ²Ë¤Æ›ÄÃ²Ä­Ã²Ë¤ÄµÆ™Ë¤ÅÃ²Ä¦Æ™ËŠÄ‘Ä­Å”Å—ÄµÅ¿Ã²Ä­Ã²Ä®ÆœË¤ÃŠÄ®Ã­Ë¤Ã²Ä­Ã¦Å—ÃŠÃ§Ä‘Ä®ÄˆË¤Ã§ÄÃŠÄ¦Ä¦Ã²Ä®ÄˆÃ²ÅÊœË¤Ä­ÃŠÄ£Ä‘Ä®ÄˆË¤Æ—ÄµÅ—Ë¤ÃŠË¤Ã§ÄµÄÃ²Å—Ã²Ä®ÆœË¤Å”ÃŠÅÅÃŠÄˆÃ²Ê›Ë¤Ë¤Ê³Ê›Ê›Ê›Ê´Ë¤\\x9aÄÃ²Ë¤Ã¦Ã²ÅÆœË¤Ã§ÄÄµÆ“Ã§Ã²Ë¤Æ“ÅË¤É¿Ê›Ê±Ê€Ê«Ê‚Ë¤Å¿ÄµÅ¤Ã²ÅÊ²ÊŸÉ¾É¿GÄ®Å”Å©Æœ\\x89Ä¦ÃŠÄ®Ë¤É¾\\x89Ä¦ÃŠÄ®Ë¤É¿ÊŸÊŸ\\x89ÃŠÅÅÃŠÄˆÃ²É¾\\x89ÃŠÅÅÃŠÄˆÃ²É¿ÊŸÊŸ)L[\\x03FRORU\\x03\\x0bE\\\\\\x03<XTLDQ\\x0cÂµÅ—Æ“Å¤Ã²Ë¤ÃŠË¤Ã§ÄµÄÃ²Å—Ã²Ä®ÆœË¤Å”ÃŠÅÅÃŠÄˆÃ²Ë¤ÄµÆ™Ë¤ÊË¤ÅÄÄµÅ—ÆœË¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄÅÊ›Ë¤\\x9aÄÃ²Ë¤Ã²Ä®Ã­Ë¤ÅÃ²Ä®Å¤Ã²Ä®Ã§Ã²Ë¤ÄµÆ™Ë¤Ã²ÃŠÃ§ÄË¤Å”ÃŠÅ—ÃŠÄˆÅ—ÃŠÅ”ÄË¤Ä­Å©ÅÆœË¤Ã¦Ã²ÊË¤É¾Ê›Ë¤GÆœË¤Æ“ÅÄ®Ë™Æ›Ë¤Ã­Ä‘Æ™Æ™Æ“Ã§Å©Ä¦ÆœË¤Æ˜ÄµË¤Ã­ÄµË¤ÃŠË¤ÄÃŠÄ®Ã­ÅÅ¤ÃŠÄ®Ã­Ë¤Ä‘Æ™Ë¤Æ†ÄµÅ©Ë¤Ä Å©ÅÆœË¤ÅÅ¤ÃŠÄ®Ã­Ë¤ÄµÄ®Ë¤Æ†ÄµÅ©Å—Ë¤ÄÃŠÄ®Ã­ÅÊ›Ë¤É¿Ê›Ë¤GÆœË¤Ã§ÃŠÅ©ÄˆÄÆœË¤ÄÄ‘Ä­Ë¤ÄµÆ™Æ™Ë¤ÄˆÅ©ÃŠÅ—Ã­Ë¤Æ›ÄÃŠÆœË¤ÅÅ”ÃŠÃ§Ã²Ë¤ÅÄ­Ã²Ä¦Ä¦Ã²Ã­Ë¤ÄµÆ™Ë¤ÅÃ²ÃŠÅ—Ã²Ã­Ë¤ÅÅ¤Ã²ÃŠÄ£Ê›Ë¤Ê€Ê›Ë¤ÂµÄÃ²Ä®Ë¤ÅÄÃ²Ë¤Ã­Æ“Ã­Ä®Ë’Æ›Ë¤Ä¦Ä‘Ä£Ã²Ë¤ÃŠË¤ÄˆÅ©Æ†Ë¤Æ€ÄÄµË¤Æ€ÃŠÅË¤Æ›Å—Æ†Ä‘Ä®ÄˆË¤Æ˜ÄµË¤Å”Æ“Ã§Ä£Ë¤ÄÃ²Å—Ë¤Å©Å”ÊœË¤ÅÄÃ²Ë¤ÅÅ¤ÃŠÅ—Å¤Ã²Ã­Ë¤Å©ÅÄ‘Ä®ÄˆË¤ÅÆ“ÄˆÄ®Ë¤Ä¦ÃŠÄ®ÄˆÅ©ÃŠÄˆÃ²Ê›Ë¤ÊÊ›Ë¤)ÃŠÃ§ÄË¤Å”Ã²Å—ÅÄµÄ®Ë¤Æ€ÄÄµË¤Ä£Ä®ÄµÆ€ÅË¤Æ†ÄµÅ©Ë¤ÄÃŠÅË¤ÃŠË¤Ã­Ä‘Æ™Ä‡Ã²Å—Ã²Ä®ÆœË¤Å”Ã²Å—Ã§Ã²Å”ÆœÆ“ÄµÄ®Ë¤ÄµÆ™Ë¤Æ€ÄÄµË¤Æ†ÄµÅ©Ë¤ÃŠÅ—Ã²Ê›Ë¤Ë¤É¾Ê›Ë¤GÄ®ÆœÅ—ÄµÃ­Å©Ã§Ã²Ë¤ÃŠÄ®Ã­Ë¤Ã²Æ…Å”Ä¦ÃŠÄ‘Ä®Ë¤Æ›ÄÃ²Ë¤Æ˜Ã²Ã§ÄÄ®Æ“Å–Å©Ã²Ë¤ÄµÆ™Ë¤Ã­ÄµÄ‘Ä®ÄˆË¤ÃŠË¤ÄÃŠÄ®Ã­ÅÅ¤ÃŠÄ®Ã­Ë¤É¿Ê›Ë¤\\x92Æ€Æ“Å¤Ã§ÄË¤Æ˜ÄµË¤ÃŠË¤ÅÅ¤ÄµÅ—Æ†Ë¤ÃŠÃ¦ÄµÅ©ÆœË¤ÃŠÄ®Ë¤ÃŠÅÆœÅ—ÄµÄ®ÃŠÅ©ÆœË™ÅË¤ÆšÄ‘Å—ÅÆœË¤Æ›Ä‘Ä­Ã²Ë¤Ä‘Ä®Ë¤ÅÅ”ÃŠÃ§Ã²Ë¤Ê€Ê›Ë¤#Ã²ÅÃ§Å—Ä‘Ã¦Ã²Ë¤ÃŠË¤ÅÆ“ÆœÅ©ÃŠÆœÆ“ÄµÄ®Ë¤Æ€ÄÃ²Å—Ã²Ë¤ÃŠË¤Æ€ÄµÄ­ÃŠÄ®Ë¤Å©ÅÃ²ÅË¤ÅÆ“Äˆ',\n",
              "  'question': 'What is the definition of \"deep factual or conceptual question\"?\\n\\n',\n",
              "  'answer': 'A deep factual or conceptual question is a question that requires a thorough understanding of a subject or concept, often involving complex concepts, and cannot be answered with a simple answer based on surface-level knowledge. The question must be answerable from the context provided, and must not mention \"according to the passage\" or \"context\".',\n",
              "  'source_doc': 'Tree of Thoughts_  Deliberate Problem Solving with Large Language Models.pdf'},\n",
              " {'context': 'corresponding to the query encoder. Thus, the embeddings of documents are ï¬xed, and we do not need to\\nrefresh the index, and thus there is no computational overhead. As we will see in practice, the impact of\\nï¬xing the documents encoder varies greatly for diï¬€erent tasks when a large training dataset is available. For\\nmost of the few-shot settings that we consider, query-side ï¬netuning does not have large performance impact,\\nand sometimes even slightly improves performance.\\n3 Related work\\n3.1 Retrieval in natural language processing\\nRetrieval for knowledge intensive tasks. Previous work has shown that retrieval improves performance\\nacross a variety of tasks such as question answering (Voorhees et al., 1999; Chen et al., 2017; Kwiatkowski et al.,\\n2019), fact checking (Thorne et al., 2018), dialogue (Dinan et al., 2019) or citation recommendation (Petroni\\net al., 2022). Historically, this information retrieval step was implemented using term-matching methods, such\\nas TF-IDF or BM25 (Jones, 1972; Robertson et al., 1995). For open-domain question answering (Voorhees\\net al., 1999), documents are often retrieved from Wikipedia (Chen et al., 2017). Recently, dense retrievers\\nbased on neural networks have become popular. These usually follow a dual-encoder architecture (Yih et al.,\\n2011; Huang et al., 2013; Shen et al., 2014), where queries and passages are encoded independently as vectors,\\nand relevance is computed using the inner product or Euclidean distance. Popular supervised retrievers\\ninclude DPR (Karpukhin et al., 2020), which is trained to discriminate the relevant passage among negative\\npassages, and extensions such as ANCE (Xiong et al., 2020) which improved the hard negatives mining\\nprocess. We refer the reader to Yates et al. (2021) for a survey of dense retrieval techniques.\\nAfter retrieval, the relevant documents are processed to produce the ï¬nal output. In open-domain QA, models',\n",
              "  'question': 'In natural language processing, what is the role of the query encoder in information retrieval?\\n\\n',\n",
              "  'answer': 'The query encoder in natural language processing is responsible for encoding queries and passages independently as vectors, and relevance is computed using the inner product or Euclidean distance.',\n",
              "  'source_doc': 'Atlas_  Few-shot Learning with Retrieval Augmented Language Models.pdf'},\n",
              " {'context': '\\x15D\\x0c\\x03$FW\\x102QO\\\\\\x03$FW\\x03\\x14\\x1d\\x03*R\\x03WR\\x03GUDZHU\\x03\\x14\\x032EV\\x03\\x14\\x1d\\x037KH\\x03GUDZHU\\x03\\x14\\x03LV\\x03FORVHG\\x11\\x03$FW\\x03\\x15\\x1d\\x032SHQ\\x03GUDZHU\\x03\\x14\\x032EV\\x03\\x15\\x1d\\x03<RX\\x03RSHQ\\x03WKH\\x03GUDZHU\\x03\\x14\\x11\\x037KH\\x03GUDZHU\\x03\\x14\\x03LV\\x03RSHQ\\x11\\x03,Q\\x03LW\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03GLVKVSRQJH\\x03\\x15\\x0f\\x03DQG\\x03D\\x03VSRRQ\\x03\\x14\\x11\\x03$FW\\x03\\x16\\x1d\\x03*R\\x03WR\\x03VLQNEDVLQ\\x03\\x14\\x032EV\\x03\\x16\\x1d\\x032Q\\x03WKH\\x03VLQNEDVLQ\\x03\\x14\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03GLVKVSRQJH\\x03\\x16\\x0f\\x03D\\x03VSDWXOD\\x03\\x14\\x0f\\x03DQG\\x03D\\x03VSRRQ\\x03\\x15\\x11\\x03$FW\\x03\\x17\\x1d\\x037DNH\\x03SHSSHUVKDNHU\\x03\\x14\\x03IURP\\x03VLQNEDVLQ\\x03\\x14\\x032EV\\x03\\x17\\x1d\\x031RWKLQJ\\x03KDSSHQV\\x11\\x03$FW\\x03\\x18\\x1d\\x037DNH\\x03SHSSHUVKDNHU\\x03\\x14\\x03IURP\\x03VLQNEDVLQ\\x03\\x14\\x032EV\\x03\\x18\\x1d\\x031RWKLQJ\\x03KDSSHQV\\x11\\x03\\x0b\\x15E\\x0c\\x035H$FW\\x03\\x0b5HDVRQ\\x03\\x0e\\x03$FW\\x0c\\nÒ¼<RX\\x03DUH\\x03LQ\\x03WKH\\x03PLGGOH\\x03RI\\x03D\\x03URRP\\x11\\x03/RRNLQJ\\x03TXLFNO\\\\\\x03DURXQG\\x03\\\\RX\\x0f\\x03\\\\RX\\x03VHH\\x03D\\x03FDELQHW\\x03\\x19\\x0f\\x03D\\x03FDELQHW\\x03\\x14\\x0f\\x03D\\x03FRIIHH\\x03PDFKLQH\\x03\\x14\\x0f\\x03D\\x03FRXQWHUWRS\\x03\\x16\\x0f\\x03\\x03D\\x03VWRYH\\x03EXUQHU\\x03\\x14\\x0f\\x03DQG\\x03D\\x03WRDVWHU\\x03\\x14\\x11\\x03<RXU\\x03WDVN\\x03LV\\x03WR\\x1d\\x033XW\\x03VRPH\\x03SHSSHU\\x03VKDNHU\\x03RQ\\x03D\\x03GUDZHU\\x11\\x0b\\x15\\x0c\\x03$OI:RUOG\\x0b\\x14\\x0c\\x03+RWVSRW\\x034$\\nFigure 1: (1) Comparison of 4 prompting methods, (a) Standard , (b) Chain-of-thought ( CoT,\\nReason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018)\\nquestion. (2) Comparison of (a) Act-only and (b) ReAct prompting to solve an AlfWorld (Shridhar\\net al., 2020b) game. In both domains, we omit in-context examples in the prompt, and only show task\\nsolving trajectories generated by the model (Act, Thought) and the environment (Obs).\\nanswers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al.,\\n2022). However, this â€œchain-of-thoughtâ€ reasoning is a static black box, in that the model uses\\nits own internal representations to generate thoughts and is not grounded in the external world,\\nwhich limits its ability to reason reactively or update its knowledge. This can lead to issues like fact\\nhallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand,\\nrecent work has explored the use of pre-trained language models for planning and acting in interactive\\nenvironments (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with\\na focus on predicting actions via language priors. These approaches usually convert multi-modal',\n",
              "  'question': 'How can pre-trained language models be used for planning and acting in interactive environments?\\n\\n',\n",
              "  'answer': 'Pre-trained language models can be used for planning and acting in interactive environments by predicting actions via language priors.',\n",
              "  'source_doc': 'ReAct_ Synergizing Reasoning and Acting in Language Models.pdf'},\n",
              " {'context': 'arXiv:2211.05102 , 2022.\\n[48] G. Qin and J. Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv\\npreprint arXiv:2104.06599 , 2021.\\n[49] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu.\\nExploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn.\\nRes., 21(1), jan 2020. ISSN 1532-4435.\\n[50] V . Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler,\\nT. L. Scao, A. Raja, et al. Multitask prompted training enables zero-shot task generalization.\\narXiv preprint arXiv:2110.08207 , 2021.\\n[51] M. Sap, R. LeBras, D. Fried, and Y . Choi. Neural theory-of-mind? on the limits of social\\nintelligence in large lms. arXiv preprint arXiv:2210.13312 , 2022.\\n[52] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili Â´c, D. Hesslow, R. CastagnÃ©, A. S. Luccioni,\\nF. Yvon, M. GallÃ©, et al. Bloom: A 176b-parameter open-access multilingual language model.\\narXiv preprint arXiv:2211.05100 , 2022.\\n[53] S. Shaphiro and M. Wilk. An analysis of variance test for normality. Biometrika , 52(3):591â€“611,\\n1965.\\n[54] Y .-L. Sung, V . Nair, and C. A. Raffel. Training neural networks with fixed sparse masks.\\nAdvances in Neural Information Processing Systems , 34:24193â€“24205, 2021.\\n19[55] R. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto.\\nStanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/\\nstanford_alpaca , 2023.\\n[56] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos,\\nL. Baker, Y . Du, et al. Lamda: Language models for dialog applications. arXiv preprint\\narXiv:2201.08239 , 2022.\\n[57] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. RoziÃ¨re, N. Goyal,\\nE. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv\\npreprint arXiv:2302.13971 , 2023.',\n",
              "  'question': 'How have recent advancements in language modeling techniques impacted the ability of LLMs to learn and perform tasks beyond their initial training?\\n\\n',\n",
              "  'answer': 'Recent advancements in language modeling techniques, such as querying LLMs with mixtures of soft prompts, exploring the limits of transfer learning with a unified text-to-text transformer, and training neural networks with fixed sparse masks, have enabled LLMs to learn and perform tasks beyond their initial training. These techniques have allowed LLMs to achieve zero-shot task generalization, learn social intelligence, and improve their ability to follow instructions in dialog applications. Additionally, the development of open and efficient foundation language models has further expanded the capabilities of LLMs.',\n",
              "  'source_doc': 'QLORA_ Efficient Finetuning of Quantized LLMs.pdf'},\n",
              " {'context': '!â€œnoâ€ ).\\nAs the construction of these symbolic reasoning tasks is\\nwell-deï¬ned, for each task we consider an in-domain test\\nset for which examples had the same number of steps as\\nthe training/few-shot exemplars, as well as an out-of-domain (OOD) test set, for which evaluation\\nexamples had more steps than those in the exemplars. For last letter concatenation, the model only\\nsees exemplars of names with two words, and then performs last letter concatenation on names with 3\\nand 4 words.4We do the same for the number of potential ï¬‚ips in the coin ï¬‚ip task. Our experimental\\nsetup uses the same methods and models as in the prior two sections. We again manually compose\\nchains of thought for the few-shot exemplars for each task, which are given in Figure 3.\\nResults. The results of these in-domain and OOD evaluations are shown in Figure 8 for PaLM,\\nwith results for LaMDA shown in Appendix Table 5. With PaLM 540B, chain-of-thought prompting\\nleads to almost 100% solve rates (note that standard prompting already solves coin ï¬‚ip with PaLM\\n540, though not for LaMDA 137B). Note that these in-domain evaluations are â€œtoy tasksâ€ in the\\nsense that perfect solution structures are already provided by the chains of thought in the few-shot\\nexemplars; all the model has to do is repeat the same steps with the new symbols in the test-time\\nexample. And yet, small models still failâ€”the ability to perform abstract manipulations on unseen\\nsymbols for these three tasks only arises at the scale of 100B model parameters.\\nAs for the OOD evaluations, standard prompting fails for both tasks. With chain-of-thought prompting,\\nlanguage models achieve upward scaling curves (though performance is lower than in the in-domain\\nsetting). Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of\\nthought for language models of sufï¬cient scale.\\n6 Discussion\\nWe have explored chain-of-thought prompting as a simple mechanism for eliciting multi-step rea-',\n",
              "  'question': 'How does chain-of-thought prompting facilitate length generalization beyond seen chains of thought for language models of sufficient scale?\\n\\n',\n",
              "  'answer': 'Chain-of-thought prompting facilitates length generalization beyond seen chains of thought for language models of sufficient scale by allowing the model to learn abstract manipulations on unseen symbols. This is demonstrated through the results of the experimental setup, which showed that small models still failed to perform abstract manipulations on unseen symbols for the toy tasks, while larger models were able to achieve upward scaling curves with chain-of-thought prompting. The ability to perform abstract manipulations on unseen symbols arises at the scale of 100B model parameters, indicating that the scale of the model is a crucial factor in facilitating length generalization.',\n",
              "  'source_doc': 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.pdf'},\n",
              " {'context': 'Adapter Layers Introduce Inference Latency There are many variants of adapters. We focus\\non the original design by Houlsby et al. (2019) which has two adapter layers per Transformer block\\nand a more recent one by Lin et al. (2020) which has only one per block but with an additional\\nLayerNorm (Ba et al., 2016). While one can reduce the overall latency by pruning layers or exploit-\\ning multi-task settings (R Â¨ucklÂ´e et al., 2020; Pfeiffer et al., 2021), there is no direct ways to bypass\\nthe extra compute in adapter layers. This seems like a non-issue since adapter layers are designed\\nto have few parameters (sometimes <1% of the original model) by having a small bottleneck di-\\nmension, which limits the FLOPs they can add. However, large neural networks rely on hardware\\nparallelism to keep the latency low, and adapter layers have to be processed sequentially. This makes\\na difference in the online inference setting where the batch size is typically as small as one. In a\\ngeneric scenario without model parallelism, such as running inference on GPT-2 (Radford et al., b)\\nmedium on a single GPU, we see a noticeable increase in latency when using adapters, even with a\\nvery small bottleneck dimension (Table 1).\\nThis problem gets worse when we need to shard the model as done in Shoeybi et al. (2020); Lep-\\nikhin et al. (2020), because the additional depth requires more synchronous GPU operations such as\\nAllReduce andBroadcast , unless we store the adapter parameters redundantly many times.\\nDirectly Optimizing the Prompt is Hard The other direction, as exempliï¬ed by preï¬x tuning (Li\\n& Liang, 2021), faces a different challenge. We observe that preï¬x tuning is difï¬cult to optimize\\nand that its performance changes non-monotonically in trainable parameters, conï¬rming similar\\nobservations in the original paper. More fundamentally, reserving a part of the sequence length for\\nadaptation necessarily reduces the sequence length available to process a downstream task, which',\n",
              "  'question': 'What problem does the use of adapter layers introduce in online inference settings?\\n\\n',\n",
              "  'answer': 'The use of adapter layers introduces an increase in latency in online inference settings, even with a very small bottleneck dimension. This is because adapter layers have to be processed sequentially, which makes a difference in the online inference setting where the batch size is typically as small as one.',\n",
              "  'source_doc': 'LoRA_ Low-Rank Adaptation of Large Language Models.pdf'},\n",
              " {'context': 'What other prompting methods might expand the range of tasks that language models can solve?\\nAs for limitations, we ï¬rst qualify that although chain of thought emulates the thought processes of\\nhuman reasoners, this does not answer whether the neural network is actually â€œreasoning,â€ which\\nwe leave as an open question. Second, although the cost of manually augmenting exemplars with\\nchains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for\\nï¬netuning (though this could potentially be surmounted with synthetic data generation, or zero-shot\\ngeneralization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct\\nand incorrect answers; improving factual generations of language models is an open direction for\\nfuture work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, inter alia ). Finally,\\nthe emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in\\nreal-world applications; further research could explore how to induce reasoning in smaller models.\\n7 Related Work\\nThis work is inspired by many research areas, which we detail in an extended related work section\\n(Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.\\nThe ï¬rst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017)\\npioneer the idea of using natural language rationales to solve math word problems through a series\\nof intermediate steps. Their work is a remarkable contrast to the literature using formal languages\\nto reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe\\net al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to ï¬netune a pretrained\\nlanguage model rather than training a model from scratch. In the domain of program synthesis,\\nNye et al. (2021) leverage language models to predict the ï¬nal outputs of Python programs via',\n",
              "  'question': 'Can intermediate steps be used to solve reasoning problems in language models?\\n',\n",
              "  'answer': 'Yes, intermediate steps can be used to solve reasoning problems in language models. This approach has been demonstrated in the work of Ling et al. (2017) and Cobbe et al. (2021), which use natural language rationales to solve math word problems and program synthesis, respectively. In these studies, language models are trained on a series of intermediate steps to reason through problems, rather than relying solely on formal languages. Additionally, Nye et al. (2021) used language models to predict the final outputs of Python programs via intermediate steps.',\n",
              "  'source_doc': 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.pdf'},\n",
              " {'context': 'Parameters Acc. (%) Acc. (%) R1/R2/RL\\nGPT-3 (FT) 175,255.8M 73.8 89.5 52.0/28.0/44.5\\nGPT-3 (BitFit) 14.2M 71.3 91.0 51.3/27.4/43.5\\nGPT-3 (PreEmbed) 3.2M 63.1 88.6 48.3/24.2/40.5\\nGPT-3 (PreLayer) 20.2M 70.1 89.5 50.8/27.3/43.5\\nGPT-3 (AdapterH) 7.1M 71.9 89.8 53.0/28.9/44.8\\nGPT-3 (AdapterH) 40.1M 73.2 91.5 53.2/29.0/45.1\\nGPT-3 (LoRA) 4.7M 73.4 91.7 53.8/29.8/45.9\\nGPT-3 (LoRA) 37.7M 74.0 91.6 53.4/29.2/45.1\\nTable 4: Performance of different adaptation methods on GPT-3 175B. We report the logical form\\nvalidation accuracy on WikiSQL, validation accuracy on MultiNLI-matched, and Rouge-1/2/L on\\nSAMSum. LoRA performs better than prior approaches, including full ï¬ne-tuning. The results\\non WikiSQL have a ï¬‚uctuation around \\x060:5%, MNLI-m around \\x060:1%, and SAMSum around\\n\\x060:2/\\x060:2/\\x060:1for the three metrics.\\n5.5 S CALING UP TO GPT-3 175B\\nAs a ï¬nal stress test for LoRA, we scale up to GPT-3 with 175 billion parameters. Due to the high\\ntraining cost, we only report the typical standard deviation for a given task over random seeds, as\\nopposed to providing one for every entry. See Section D.4 for details on the hyperparameters used.\\nAs shown in Table 4, LoRA matches or exceeds the ï¬ne-tuning baseline on all three datasets. Note\\nthat not all methods beneï¬t monotonically from having more trainable parameters, as shown in Fig-\\nure 2. We observe a signiï¬cant performance drop when we use more than 256 special tokens for\\npreï¬x-embedding tuning or more than 32 special tokens for preï¬x-layer tuning. This corroborates\\nsimilar observations in Li & Liang (2021). While a thorough investigation into this phenomenon\\nis out-of-scope for this work, we suspect that having more special tokens causes the input distri-\\nbution to shift further away from the pre-training data distribution. Separately, we investigate the\\nperformance of different adaptation approaches in the low-data regime in Section F.3.\\n6 7 8 9 10 11\\nlog10 # Trainable Parameters0.550.600.650.700.75Validation Accuracy\\nWikiSQL\\nMethod',\n",
              "  'question': 'Given the table and figure provided, what is the effect of increasing the number of special tokens used for pre-embedding and pre-layer tuning on the validation accuracy of LoRA on WikiSQL?\\n\\n',\n",
              "  'answer': 'Increasing the number of special tokens used for pre-embedding and pre-layer tuning beyond 256 and 32, respectively, causes a significant performance drop in the validation accuracy of LoRA on WikiSQL. This suggests that having more special tokens causes the input distribution to shift further away from the pre-training data distribution, which negatively impacts the performance of the adaptation approach.',\n",
              "  'source_doc': 'LoRA_ Low-Rank Adaptation of Large Language Models.pdf'},\n",
              " {'context': 'is the least preferred in our rankings, which could indicate that the model generates very short or\\ndegenerate responses.\\nTo evaluate the modelâ€™s propensity to generate biased speech (see Appendix E), we also evaluated\\nInstructGPT on modiï¬ed versions of the Winogender (Rudinger et al., 2018) and CrowS-Pairs (Nangia\\net al., 2020) datasets. These datasets consists of pairs of sentences which can highlight potential bias.\\nWe calculate the relative probabilities of producing the sentences in each pair and the entropy (in bits)\\nof the associated binary probability distributions. Perfectly unbiased models will have no preference\\nbetween the sentences in each pair and will therefore have maximum entropy. By this metric, our\\nmodels are not less biased than GPT-3. The PPO-ptx model shows similar bias to GPT-3, but when\\ninstructed to act respectfully it exhibits lower entropy and thus higher bias. The pattern of the bias is\\nnot clear; it appears that the instructed models are more certain of their outputs regardless of whether\\nor not their outputs exhibit stereotypical behavior.\\nWe can minimize performance regressions on public NLP datasets by modifying our RLHF\\nï¬ne-tuning procedure. By default, when we train a PPO model on our API distribution, it suffers\\nfrom an â€œalignment taxâ€, as its performance on several public NLP datasets decreases. We want an\\nalignment procedure that avoids an alignment tax, because it incentivizes the use of models that are\\nunaligned but more capable on these tasks.\\n14Prompt:\\nÃ‰crivez une courte histoire sur une grenouille qui voyage dans le temps jusquâ€™Ã  la GrÃ¨ce antique en franÃ§ais.\\nGPT-3 175B completion: InstructGPT 175B completion:\\nÃ‰crivez une histoire au sujet dâ€™un enfant qui voudrait\\ntout savoir sur les jeux des dieux et qui se retrouve\\ndans lâ€™une de leurs histoires.\\nÃ‰crivez une histoire sur un jeune homme qui a une\\naventure dans une Ã©poque lointaine avec une ï¬lle de\\nlâ€™Ã©poque.\\nÃ‰crivez une histoire sur un enfant qui est envoyÃ© Ã ',\n",
              "  'question': 'How does the PPO-ptx model exhibit bias in its outputs?\\n\\n',\n",
              "  'answer': 'The PPO-ptx model exhibits bias in its outputs by having a higher entropy and being more certain of its outputs regardless of whether or not their outputs exhibit stereotypical behavior. This suggests that the instructed models are more biased than GPT-3, but when instructed to act respectfully, they exhibit lower entropy and thus higher bias.',\n",
              "  'source_doc': 'Training language models to follow instructions with human feedback.pdf'},\n",
              " {'context': 'environments (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with\\na focus on predicting actions via language priors. These approaches usually convert multi-modal\\nobservations into text, use a language model to generate domain-speciï¬c actions or plans, and then\\nuse a controller to choose or execute them. However, they do not employ language models to reason\\nabstractly about high-level goals or maintain a working memory to support acting, barring Huang\\net al. (2022b) who perform a limited form of verbal reasoning to reiterate spatial facts about the\\ncurrent state. Beyond such simple embodied tasks to interact with a few blocks, there have not been\\nstudies on how reasoning and acting can be combined in a synergistic manner for general task solving,\\nand if such a combination can bring systematic beneï¬ts compared to reasoning or acting alone.\\nIn this work, we present ReAct , a general paradigm to combine reasoning and acting with language\\nmodels for solving diverse language reasoning and decision making tasks (Figure 1). ReAct\\nprompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an\\ninterleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and\\nadjust high-level plans for acting (reason to act), while also interact with the external environments\\n(e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).\\n2Published as a conference paper at ICLR 2023\\nWe conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks:\\nquestion answering (HotPotQA, Yang et al., 2018), fact veriï¬cation (Fever, Thorne et al., 2018),\\ntext-based game (ALFWorld, Shridhar et al., 2020b), and webpage navigation (WebShop, Yao\\net al., 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact\\nwith, ReAct outperforms vanilla action generation models while being competitive with chain-of-',\n",
              "  'question': 'How does ReAct combine reasoning and acting with language models for solving diverse language reasoning and decision making tasks?\\n\\n',\n",
              "  'answer': 'ReAct prompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an interleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and adjust high-level plans for acting (reason to act), while also interact with the external environments (e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).',\n",
              "  'source_doc': 'ReAct_ Synergizing Reasoning and Acting in Language Models.pdf'},\n",
              " {'context': 'in only a few thousand examples of useful calls\\nto the calculator API. A potential solution to this\\nproblem might be to iteratively apply our approach,\\nsimilar to how this is done in related bootstrapping\\napproaches (Schick and SchÃ¼tze, 2021a; Izacard\\nand Grave, 2021; Parisi et al., 2022). Finally, when\\ndeciding whether or not to make an API call, Tool-\\nformer currently does not take into account the\\ntool-dependent, computational cost incurred from\\nmaking an API call.\\n8 Conclusion\\nWe have introduced Toolformer, a language model\\nthat learns in a self-supervised way how to use\\ndifferent tools such as search engines, calculators,\\nand translation systems via simple API calls. This\\nis done by ï¬netuning on a large number of sampled\\nAPI calls that are ï¬ltered based on whether theyreduce perplexity on future tokens. Toolformer\\nconsiderably improves zero-shot performance of a\\n6.7B parameter GPT-J model, enabling it to even\\noutperform a much larger GPT-3 model on a range\\nof different downstream tasks.\\nRefere',\n",
              "  'question': 'How does Toolformer improve zero-shot performance of a GPT-J model?\\n',\n",
              "  'answer': 'Toolformer improves zero-shot performance of a GPT-J model by ï¬netuning on a large number of sampled API calls that are ï¬ltered based on whether they reduce perplexity on future tokens.',\n",
              "  'source_doc': 'Toolformer_ Language Models Can Teach Themselves to Use Tools.pdf'},\n",
              " {'context': 'apart from inserted API calls the augmented dataset\\nC\\x03contains the exact same texts as C, the original\\ndataset. As a consequence, ï¬netuning MonC\\x03\\nexposes it to the same content as ï¬netuning on C.\\nMoreover, as API calls are inserted in exactly those\\npositions and with exactly those inputs that help\\nMpredict future tokens, ï¬netuning on C\\x03enables\\nthe language model to decide when and how to use\\nwhich tool, based purely on its own feedback.\\nInference When generating text with Mafter\\nï¬netuning with our approach, we perform regular\\ndecoding until Mproduces the â€œ!â€ token, indicat-\\ning that it next expects the response for an API call.\\nAt this point, we interrupt the decoding process,\\ncall the appropriate API to get a response, and con-\\ntinue the decoding process after inserting both the\\nresponse and the </API> token.\\n3 Tools\\nWe explore a variety of tools to address different\\nshortcomings of regular LMs. The only constraints\\nwe impose on these tools is that (i) both their inputs\\nand outputs can be represented as text sequences,\\nand (ii) we can obtain a few demonstrations of\\ntheir intended use. Concretely, we explore the fol-\\nlowing ï¬ve tools: a question answering system, a\\nWikipedia search engine, a calculator, a calendar,\\nand a machine translation system. Some examples\\nof potential calls and return strings for the APIs\\nassociated with each of these tools are shown in\\nTable 1. We brieï¬‚y discuss all tools below; further\\ndetails can be found in Appendix A.\\nQuestion Answering Our ï¬rst tool is a question\\nanswering system based on another LM that can an-\\nswer simple factoid questions. Speciï¬cally, we use\\nAtlas (Izacard et al., 2022), a retrieval-augmented\\nLM ï¬netuned on Natural Questions (Kwiatkowski\\net al., 2019).\\nCalculator As a second tool, we use a calculator\\nthat can perform simple numeric calculations; we\\nonly support the four basic arithmetic operations.\\nResults are always rounded to two decimal places.\\nWikipedia Search Our third tool is a search en-',\n",
              "  'question': 'Can a language model be trained to use tools based on its own feedback during inference?\\n\\n',\n",
              "  'answer': 'Yes, a language model can be trained to use tools based on its own feedback during inference. This approach allows the language model to decide when and how to use each tool, based on its own feedback, enabling it to generate more accurate and relevant text.',\n",
              "  'source_doc': 'Toolformer_ Language Models Can Teach Themselves to Use Tools.pdf'},\n",
              " {'context': 'task, including self-supervised pre-training. As shown in the experimental section, pre-training is critical for\\nobtaining models that exhibit few-shot learning abilities.\\nAttention Distillation (ADist). The ï¬rst loss that we consider is based on the attention scores of the\\nlanguage model, and is heavily inspired by Izacard & Grave (2021). The main idea is that the cross-attention\\nscores between the input documents and the output, can be used as a proxy of the importance of each input\\ndocument when generating the output. In particular, Izacard & Grave (2021) showed that these scores can\\nbe aggregated across attention heads, layers and tokens for a given document to obtain a single score for each\\ndocument. Then, these scores can be distilled into the retriever by minimizing the KL-divergence with the\\nprobability distribution pretrover the top-K documents {dk}1,...,Kobtained from the retriever:\\npretr(d|q) =exp(s(d,q)/Î¸)/summationtextK\\nk=1exp(s(dk,q)/Î¸), (1)\\nwheresis the dot-product between the query and documents vectors and Î¸is a temperature hyper-parameter.\\nIn the original paper, it was proposed to use the pre-softmax scores from the decoder cross-attentions, and\\naverage across heads, layers and tokens. Here, we propose an alternative which gives slightly stronger results,\\nwhich relies on the following observation. In the attention mechanism, as deï¬ned by\\ny=N/summationdisplay\\nn=1Î±nvn,\\nthe contribution to the output yof a particular token ncannot be evaluated from the attention score Î±n\\nalone, but should also take the norm of the value vninto account. Hence, we use the quantity Î±n/bardblvn/bardbl2as the\\nmeasure of relevance for token n. Following Izacard & Grave (2021), we average these scores over all attention\\nheads, layers, and tokens to obtain a score for each document. We apply the Softmax operator over the\\nresulting scores, to obtain a distribution pattn(dk)over the top-K retrieved documents. We then minimize',\n",
              "  'question': 'What is the loss function used in Attention Distillation (ADist) for few-shot learning abilities in pre-trained language models?\\n\\n',\n",
              "  'answer': 'The loss function used in Attention Distillation (ADist) for few-shot learning abilities in pre-trained language models is based on the attention scores of the language model. It heavily inspires Izacard & Grave (2021) and involves minimizing the KL-divergence with the probability distribution of the top-K documents obtained from the retriever. The loss function relies on the dot-product between the query and documents vectors and a temperature hyper-parameter Î¸. It also takes into account the norm of the value vn into account for each token n in the attention mechanism. The resulting scores are averaged over all attention heads, layers, and tokens to obtain a score for each document. The Softmax operator is then applied over the scores to obtain a distribution pattn(dk) over the top-K retrieved documents.',\n",
              "  'source_doc': 'Atlas_  Few-shot Learning with Retrieval Augmented Language Models.pdf'},\n",
              " {'context': 'The first Generate\\nsplits the 64-element\\ninput array into four\\n16-element chunks.\\nSorting is implemented within\\nthe Generate operation. Here,\\nN=3 means that, for each 16\\nelement chunk, we generate\\nthree different sortings. \\nHere, N=1 means that we\\nmaintain a single best\\nsorting outcome out of\\nthe three input ones.\\nHere, N=10 means that we try 10 different\\naggregations of the two input 16-element subarrays.To obtain the score, for every\\nnumber 0 - 9, we get the\\ndifference between the input\\nand the sorted list, and we sum\\nall 10 values. Zero indicates\\ncorrectly sorted. To show\\n\"the higher the better\", we do\\nmax(input_length - score, 0)Note that this is an example graph decomposition. The structure\\nof connections between all operations can be arbitrarily modified.\\nSort\\nKeepBest\\nAggregateFigure 4: An example graph decomposition of the sorting\\nuse case in GoT. All the used operations (Generate, Aggre-\\ngate, Score, KeepBest) are described in Figure 3.intersection sets are aggregated for the final results. For the\\nevaluation we use different set sizes of 32, 64 and 128 el-\\nements and we vary the number of elements found in both\\nsets to be between 25% and 75%.\\nOur score indicates the total number of missing or in-\\ncorrectly included elements in the final intersection. Specif-\\nically, denote two input sets with A= [a1, a2, ..., a n]\\nandB= [b1, b2, ..., b n], and the output set with C=\\n[c1, c2, ..., c m]. Then,\\nerror-scope =X1+X2+Xd\\nwhere X1=|C\\\\(Aâˆ©B)|are the number of elements in C\\nthat are not supposed to be there, X2=|(Aâˆ©B)\\\\C|are the\\nnumber of elements missing from C, and Xdis the number\\nof duplicates in C(because the LLM expresses the set as a\\nlist in natural language). Finally, to use a â€œpositive scoreâ€\\ndescribing â€œthe scope of correctly computedâ€ elements, one\\ncan use the value max( nâˆ’error-scope ,0).\\n5.3 Keyword Counting\\nKeyword counting finds the frequency of keywords in a\\ngiven category (countries in our example implementation)',\n",
              "  'question': 'How does the keyword counting operation in GoT use a graph decomposition to find the frequency of keywords in a given category?\\n\\n',\n",
              "  'answer': 'The keyword counting operation in GoT uses a graph decomposition to find the frequency of keywords in a given category by first splitting the input data into smaller chunks. These chunks are then sorted and aggregated to obtain the final results. The error-scope of the operation is calculated as the total number of missing or incorrectly included elements in the final intersection set of the two input sets. Finally, the scope of correctly computed elements is used to calculate a positive score, which describes the scope of correctly computed elements.',\n",
              "  'source_doc': 'Graph of Thoughts_ Solving Elaborate Problems with Large Language Models.pdf'},\n",
              " {'context': 'challenge. arXiv preprint arXiv:1803.05457 , 2018.\\n[10] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\\nsolve math word problems. arXiv preprint arXiv:2110.14168 , 2021.\\n[11] Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. FlashAttention: Fast\\nand memory-efficient exact attention with IO-awareness. In Advances in Neural Information\\nProcessing Systems , 2022.\\n[12] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\\nJacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint\\narXiv:2009.03300 , 2020.\\n[13] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn\\nSong, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.\\narXiv preprint arXiv:2103.03874 , 2021.\\n[14] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Thomas\\nHennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia\\nGuy, Simon Osindero, KarÃ©n Simonyan, Erich Elsen, Oriol Vinyals, Jack Rae, and Laurent\\nSifre. An empirical analysis of compute-optimal large language model training. In Advances in\\nNeural Information Processing Systems , volume 35, 2022.\\n[15] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large\\nscale distantly supervised challenge dataset for reading comprehension. arXiv preprint\\narXiv:1705.03551 , 2017.\\n[16] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\\nbenchmark for question answering research. Transactions of the Association for Computational\\nLinguistics , 7:453â€“466, 2019.',\n",
              "  'question': \"Given that the authors of [10] and [11] have proposed methods for training verifiers to solve math word problems, and the authors of [13] have proposed a method for measuring mathematical problem solving, what is the relationship between these methods and the authors' overall goal of improving mathematical problem-solving ability?\\n\\n\",\n",
              "  'answer': 'The methods proposed by the authors of [10] and [11] aim to train verifiers to solve math word problems, while the method proposed by the authors of [13] aims to measure mathematical problem-solving ability. These methods are related in that they both aim to improve mathematical problem-solving ability, but they use different approaches to achieve this goal. The authors of [10] and [11] focus on training verifiers to solve math word problems, while the authors of [13] focus on measuring mathematical problem-solving ability.',\n",
              "  'source_doc': 'Mistral7B.pdf'},\n",
              " {'context': 'we assume availability of training data from the\\ntarget corpus. Our reranker is a classi\\ue000er that gets\\na pre\\ue000x xâ‰¤sÂ·jand a document di(foriâˆˆ[k] ), and\\nproduces a scalar f(x â‰¤sÂ·j, di)that should resemble\\nthe relevance ofd iforthe continuationofx â‰¤sÂ·j.\\nWe then normalize these relevance scores:\\nprank(di|xâ‰¤sÂ·j) =exp(f(x â‰¤sÂ·j, di))\\ue006k\\niâ€²=1exp(f(x â‰¤sÂ·j, diâ€²)),(7)\\nand choose the documentd Ë†isuch that\\nË†i= arg max\\niâˆˆ[k]prank(di|xâ‰¤sÂ·j).(8)\\nCollecting Training ExamplesTo train our pre-\\ndictive reranker, we collected training examples\\nas follows. Let xâ‰¤sÂ·jbe a pre\\ue000x we sample from\\nthe training data, and y:=x sÂ·j+1 , ..., x sÂ·j+s be the\\ntext for generation upcoming in its next stride. We\\nrun BM25 on the query qs,â„“\\njderived from xâ‰¤sÂ·j\\n(see Â§ 3.2) and get kdocuments {d1, ..., d k}. For\\neach document di, we then run the LM to compute\\npÎ¸(y|[d i;xâ‰¤sÂ·j])similar to Eq. ( 4).Figure 8: Zero-shot performance of In-Context\\nRALM on the development set of Natural Ques-\\ntions and TriviaQA, when varying the number of\\ndocuments (retrieved by DPR) shown in-context.\\nTrainingOur reranker was a\\ue000ne-tuned\\nRoBERTa-base ( Liu et al. ,2019 ) that trained for\\n10,000 steps with a peak learning rate of 10âˆ’5and\\na batch size of 32. Overall, we created 300,000\\nexamples from the training set of WikiText-103 as\\nexplained above. The loss function we use to train\\nthe reranker follows previous work ( Guu et al. ,\\n2020 ;Lewis et al. ,2020 ):\\nâˆ’logk\\ue008\\ni=1prank(di|xâ‰¤sÂ·j)Â·p Î¸(y|[d i;xâ‰¤sÂ·j]).(9)\\nNote that unlike those works, we train only the\\nreranker (p rank), keeping the LM weightsÎ¸frozen.\\nResultsTable 1shows the result of our predictive\\nreranker, trained on WikiText-103. Speci\\ue000cally, we\\ntrained it with data produced by GPT-2 110M (S),\\nand tested its effectiveness for all GPT-2 models.\\nWe observed signi\\ue000cant gains obtained from Predic-\\ntive Reranking. For example, the perplexity of GPT-\\n2 110M (S) improved from 29.6 to 26.8, and that of\\nGPT-2 1.5B (XL) improved from 16.6 to 15.4. This',\n",
              "  'question': 'What is the training process of the predictive reranker in the given context?\\n\\n',\n",
              "  'answer': 'The predictive reranker in the given context is trained using a RoBERTa-base model. The training process involves sampling pre-x from the training data and generating text for generation upcoming in its next stride. Then, BM25 is run on the query qs,lljderived from xâ‰¤sÂ·j to retrieve kdocuments {d1, ..., d k}. For each document di, the LM is run to compute pÎ¸(y|[d i;xâ‰¤sÂ·j]). The loss function used to train the reranker is -logk&#37;i=1prank(di|xâ‰¤sÂ·j)Â·p Î¸(y|[d i;xâ‰¤sÂ·j]). The model is trained for 10,000 steps with a peak learning rate of 10âˆ’5and a batch size of 32. The LM weights are frozen during training. The effectiveness of the predictive reranker is tested on WikiText-103 using data produced by GPT-2 110M (S). The perplexity of GPT-2 110M (S) improved from 29.6 to 26.8, and that of GPT-2 1.5B (XL) improved from 16.6 to 15.4 from the training process.',\n",
              "  'source_doc': 'In-Context Retrieval-Augmented Language Models.pdf'},\n",
              " {'context': 'models (Wang et al., 2019). Min et al. (2019a)\\nintroduced a method based on hard expectation-\\nmaximization to tackle noisy supervision from this\\nsetting. Wang et al. (2018b) described a technique\\nto aggregate answers from different paragraphs,\\nusing conï¬dence and coverage scores.\\nPassage retrieval is an important step in open\\ndomain question answering, and is an active area of\\nresearch to improve QA systems. Initially, sparse\\nrepresentations based on TF/IDF were used to\\nretrieve support documents (Chen et al., 2017).\\nLee et al. (2018) introduced a supervised learningmethod to rerank paragraphs based on BiLSTM,\\nwhile Wang et al. (2018a) trained a ranking system\\nwith reinforcement learning. A second approach\\nto improve the retrieval step of QA systems is to\\nused additional information such as the Wikipedia\\nor Wikidata graphs (Min et al., 2019b; Asai et al.,\\n2020). Recently, multiple works show that retrieval\\nsystems entirely based on dense representation\\nand approximate nearest neighbors were competi-\\ntive with traditional approaches. Such models can\\nbe trained using weak supervision in the form of\\nquestion-answer pairs (Karpukhin et al., 2020), or\\npretrained using a cloze task and ï¬netuned end-to-\\nend (Guu et al., 2020; Lee et al., 2019).\\nGenerative question answering was mostly\\nconsidered in previous work for datasets requiring\\nto generate answers, such as NarrativeQA (Ko Ë‡cisk`y\\net al., 2018), CoQA (Reddy et al., 2019) or\\nELI5 (Fan et al., 2019). These datasets were gen-\\nerated in a way that answers do not correspond\\nto spans in support documents, thus requiring ab-\\nstractive models. Raffel et al. (2020) showed that\\ngenerative models are competitive for reading com-\\nprehension tasks such as SQuAD (Rajpurkar et al.,\\n2016), where answers are spans. Roberts et al.\\n(2020) proposed to use large pretrained generative\\nmodels, without using additional knowledge, for\\nopen domain question answering. Closest to our\\nwork, Min et al. (2020) and Lewis et al. (2020b) in-',\n",
              "  'question': 'In light of the context, what are some recent advances in models for open domain question answering?\\n\\n',\n",
              "  'answer': 'Recent advances in models for open domain question answering include the use of dense representation and approximate nearest neighbors (Karpukhin et al., 2020; Guu et al., 2020; Lee et al., 2019), generative models (Raffel et al., 2020; Roberts et al., 2020), and the incorporation of additional information such as Wikipedia or Wikidata graphs (Min et al., 2019b; Asai et al., 2020). These models can be trained using weak supervision in the form of question-answer pairs or pretrained using a cloze task and fine-tuned end-to-end.',\n",
              "  'source_doc': 'Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering.pdf'},\n",
              " {'context': 'to both a larger computational budget, enabling more complex reasoning, and the ability to memorize more\\ninformation related to downstream tasks from the larger training data. While it is intuitive to assume that\\nincreased reasoning abilities lead to better generalisation, and hence few-shot learning, the same is not true\\nfor in-parameter memorisation. Speciï¬cally, it is unclear to what extent eï¬€ective few-shot learning requires\\nvast knowledge in the parameters of the model.\\nIn this paper, we investigate whether few-shot learning requires models to store a large amount of information\\nin their parameters, and if memorisation can be decoupled from generalisation. To do so, we leverage the fact\\nthat memory can be outsourced and replaced by an external non-parametric knowledge source by employing\\naretrieval-augmented architecture. These models employ a non-parametric memory, e.g. a neural retriever\\nover a large, external, potentially non-static knowledge source to enhance a parametric language model. In\\naddition to their memorisation abilities, such architectures are attractive due to a number of other established\\nadvantages in terms of adaptability, interpretability and eï¬ƒciency (Guu et al., 2020; Lewis et al., 2020;\\nYogatama et al., 2021; Borgeaud et al., 2021, inter alia). However, retrieval-augmented models have yet to\\nâˆ—equal contribution\\n1arXiv:2208.03299v3  [cs.CL]  16 Nov 2022Fact checking:Bermuda Triangle is in the western part of the Himalayas.AtlasFalseMasked Language Modelling:Bermuda Triangle is in the <MASK> of the Atlantic Ocean.\\nTheBermuda Triangle is anurban legendfocused on a loosely-defined region in the western part of the NorthAtlantic Ocean.western partPretrainingFew-shotQuestion answering:Where is the Bermuda Triangle?Western part of the North Atlantic Oceanâ€¦â€¦Figure 1: We introduce Atlas, a retrieval-augmented language model that exhibits strong few-shot perfor-\\nmance on knowledge tasks, and uses retrieval during both pre-training and ï¬ne-tuning.',\n",
              "  'question': 'How does the ability to memorize more information related to downstream tasks affect few-shot learning in language models?\\n\\n',\n",
              "  'answer': 'It is unclear to what extent effective few-shot learning requires vast knowledge in the parameters of the model. While it is intuitive to assume that increased reasoning abilities lead to better generalisation, the same is not true for in-parameter memorisation. Retrieval-augmented architectures, which employ a non-parametric memory to enhance a parametric language model, have yet to be proven to equal the contribution of vast knowledge in the parameters of the model to few-shot learning.',\n",
              "  'source_doc': 'Atlas_  Few-shot Learning with Retrieval Augmented Language Models.pdf'},\n",
              " {'context': 'destruction, nor hobbyists, are all having it the way Washington does, so any possible\\nmoves must have been made based on failed missile maintenance. \"You know what theyâ€™re\\ndoing,\" McFarland told Fox, \"weâ€™re not going to send the whole team along.\" Which is\\na strange thing for a reclusive state, not just because of North Koreaâ€™s weight, but\\nalso because its government boasted that it didnâ€™t last long. These days, it appears\\nto be a formula for luck and luck for the elites who once thought maybe they were doing\\nsomething about the North Korean missile crisis. Weâ€™re supposed to give our president\\nsome credit for doing what he did and getting some information, but every year, late\\nchanges happen to stifle the executive in power. Madhya Pradesh, India and Bangladesh\\nwill always have to tighten security structures to ensure the integrity of North Koreaâ€™s\\ndefense, foreign affairs, and militaries. The Obama administration, concerned with\\nother threatened nuclear war with China or the U.S., has been getting pretty wild about\\nKim Jong Il. President Obama only added to the tensions by denouncing Kim Jong Unâ€™s\\nactivities as \"evil.\" The latest episode in North Koreaâ€™s long downward spiral began\\nwith the sinking of a fifth submarine into the Korean Sea late last year, U.S. Pacific\\nCommand announced on its blog. In response to such concern, Obama pledged \"recovery\\nactivities\" against North Korea. In the same post, National Intelligence Director James\\nClapper said the administration will fight the Northâ€™s nuclear threats and leverage the\\nmeans to get countries to do more. Meanwhile, Washington has recently raised concerns\\nabout Russiaâ€™s involvement in the crisis, and says sanctions imposed against Russia for\\nits role in the conflict arenâ€™t working.The president also told The Associated Press, \"We\\nurge President Putin to ensure that Russia does its business in international law and\\naccountability.\"Listen to the full report from Business Insider, obtained by CNBC via a',\n",
              "  'question': 'Based on the context, what are the factors contributing to the ongoing North Korean missile crisis?\\n\\n',\n",
              "  'answer': 'The ongoing North Korean missile crisis is influenced by several factors, including the reclusive state\\'s weight, lack of transparency, and the government\\'s boasting about its defense capabilities. Additionally, the crisis is compounded by the Obama administration\\'s denunciation of Kim Jong Un\\'s activities as \"evil\" and the sinking of a fifth submarine into the Korean Sea. Furthermore, the administration\\'s plans for recovery activities and its concerns about Russia\\'s involvement in the crisis are further complicating the situation.',\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': '540B parameters model, which required 50x more pre-training compute. We also provided detailed ablations\\nand analyses for what factors are important when training such retrieval-augmented models, and demonstrated\\nAtlasâ€™s updateability, interpretability and controlability capabilities. Lastly, we demonstrated that Atlasis\\nalso powerful in full-dataset settings obtaining a new state-of-the-art results on NaturalQuestions, TriviaQA,\\nFEVER, and 5 KILT tasks.',\n",
              "  'question': 'Given the provided context, what are the important factors that contribute to the successful training of retrieval-augmented models, and how does Atlas demonstrate its updateability, interpretability, and controlability capabilities?\\n\\n',\n",
              "  'answer': 'The important factors that contribute to the successful training of retrieval-augmented models include compute resources, detailed ablations and analyses, and demonstration of updateability, interpretability, and controlability capabilities. Atlas demonstrates its updateability, interpretability, and controlability capabilities by providing detailed ablations and analyses, and by demonstrating its performance on various benchmark datasets, including NaturalQuestions, TriviaQA, FEVER, and 5 KILT tasks.',\n",
              "  'source_doc': 'Atlas_  Few-shot Learning with Retrieval Augmented Language Models.pdf'},\n",
              " {'context': 'shows what happens for the third chunk (â€œthe dog go toâ€): it attends itself using a causal mask (rightmost block),\\nattends the cache using a sliding window (center block), and does not attend to past tokens as they are outside of\\nthe sliding window (left block).\\n3 Results\\nWe compare Mistral 7B to Llama, and re-run all benchmarks with our own evaluation pipeline for\\nfair comparison. We measure performance on a wide variety of tasks categorized as follow:\\nâ€¢Commonsense Reasoning (0-shot): Hellaswag [ 28], Winogrande [ 21], PIQA [ 4], SIQA [ 22],\\nOpenbookQA [19], ARC-Easy, ARC-Challenge [9], CommonsenseQA [24]\\nâ€¢World Knowledge (5-shot): NaturalQuestions [16], TriviaQA [15]\\nâ€¢Reading Comprehension (0-shot): BoolQ [8], QuAC [7]\\nâ€¢Math: GSM8K [10] (8-shot) with maj@8 and MATH [13] (4-shot) with maj@4\\nâ€¢Code: Humaneval [5] (0-shot) and MBPP [2] (3-shot)\\nâ€¢Popular aggregated results: MMLU [ 12] (5-shot), BBH [ 23] (3-shot), and AGI Eval [ 29]\\n(3-5-shot, English multiple-choice questions only)\\nDetailed results for Mistral 7B, Llama 2 7B/13B, and Code-Llama 7B are reported in Table 2. Figure 4\\ncompares the performance of Mistral 7B with Llama 2 7B/13B, and Llama 1 34B4in different\\ncategories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on\\nmost benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics,\\nand reasoning benchmarks.\\n4Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\\n3Figure 4: Performance of Mistral 7B and different Llama models on a wide range of benchmarks . All\\nmodels were re-evaluated on all metrics with our evaluation pipeline for accurate comparison. Mistral 7B\\nsignificantly outperforms Llama 2 7B and Llama 2 13B on all benchmarks. It is also vastly superior to Llama 1\\n34B in mathematics, code generation, and reasoning benchmarks.\\nModel Modality MMLU HellaSwag WinoG PIQA Arc-e Arc-c NQ TriviaQA HumanEval MBPP MATH GSM8K',\n",
              "  'question': 'Can you explain the difference between the causal mask and sliding window in the context of attention mechanisms in natural language processing?\\n\\n',\n",
              "  'answer': 'The causal mask and sliding window are both attention mechanisms used in natural language processing. The causal mask is used to attend to tokens in a sequence based on their causal relationship with previous tokens, while the sliding window is used to attend to tokens in a sequence based on their proximity to the current token. The causal mask is typically used when the context is important for understanding the current token, while the sliding window is used when the context is less important.',\n",
              "  'source_doc': 'Mistral7B.pdf'},\n",
              " {'context': 'ofSachan et al. (2022 ) for open-domain question\\nanswering, where yâ€²(i.e., the last pre\\ue000x tokens) can\\nbe thought of as their â€œquestionâ€.\\nNote that our zero-shot reranking does not re-\\nquire that the LM used for reranking is the same\\nmodel as the LM used for generation (i.e., the LM\\nin Eq. (6), parameterized by Ï•, does not need to be\\nthe LM in Eq. (2), parameterized by Î¸). This ob-\\nservation unlocks the possibility of reranking with\\nsmaller (and thus faster) models, which is impor-\\ntant for two main reasons: (i) Reranking kdocu-\\nments requires kforward passes; and (ii) it allows\\nour methods to be used in cases where the actual\\nLMâ€™s log probabilities are not available (for exam-\\nple, when the LM is accessed through an API).6\\nResultsA minimal hyper-parameter search on\\nthe development set of WikiText-103 revealed that\\nthe optimal query length is sâ€²= 16 ,7so we proceed\\nwith this value going forward. Table 1shows the\\nresults of letting the LM perform zero-shot rerank-\\ning on the top-16 documents retrieved by BM25\\n(third row for each of the models). It is evident\\nthat reranking yielded consistently better results\\nthan simply taking the\\ue000rst result returned by the\\nretriever.\\n6Note we do not require that the two models share the\\nsame vocabulary.\\n7We experimented withsâ€²âˆˆ{4,8,16,32}.Table 3shows that a small LM (GPT-2 117M)\\ncan be used to rerank the documents for all larger\\nGPT-2 models, with roughly the same performance\\nas having each LM perform reranking for itself,\\nsupporting the applicability of this method for LMs\\nthat are only accessible via an API.\\n6.2 Training LM-dedicated Rerankers\\nNext, wetraineda reranker to choose one of the\\ntop-kdocuments retrieved by BM25. We refer to\\nthis approach asPredictive Reranking, since the\\nreranker learns to choose which document will help\\nin â€œpredictingâ€ the upcoming text. For this process,\\nwe assume availability of training data from the\\ntarget corpus. Our reranker is a classi\\ue000er that gets\\na pre\\ue000x xâ‰¤sÂ·jand a document di(foriâˆˆ[k] ), and',\n",
              "  'question': 'What is the optimal query length for zero-shot reranking in open-domain question answering using the LM in Eq. (6)?\\n\\n',\n",
              "  'answer': 'The optimal query length for zero-shot reranking in open-domain question answering using the LM in Eq. (6) is sâ€²= 16.',\n",
              "  'source_doc': 'In-Context Retrieval-Augmented Language Models.pdf'},\n",
              " {'context': 'answer quality, SoT is a first attempt at exploiting the power of prompting to improve efficiency .\\n6 L IMITATIONS , FUTURE WORK,AND OPEN QUESTIONS\\nAnswer quality evaluation. Our answer quality evaluation is far from perfect due to the limited\\nprompt set, the potential bias of GPT-4 judges, and the inherent difficulty of evaluating LLM gener-\\nations. Currently, we did not conduct human evaluation since it is easy for a human to tell whether\\nan answer is generated with SoT due to its distinctive pattern, which might cause evaluation bias.\\nWe leave a more thorough evaluation of answer quality to future work.\\nEliciting or improving LLMsâ€™ ability. Â§ 3.2.4 demonstrates SoTâ€™s potential of enhancing answer\\nquality. It is part of a broader trend in recent research, exemplified by work including CoT (Kojima\\net al., 2022; Wei et al., 2022), ToT (Yao et al., 2023), and ReAct (Yao et al., 2022), which collectively\\naffirm the notion that explicitly articulating the thought process in language can elicit high-quality\\nanswers from LLMs . These findings resemble human thinking: rather than relying solely on the\\nfirst intuition or purely sequential thinking, we often document step-by-step reasoning or thought\\norganization to attain high-quality answers. This intriguing parallel prompts us to explore further\\nhow we can draw from the human thinking process to facilitate more effective and efficient AI.\\nFor instance, SoT currently ignores the dependencies between points. A conceptually better way is\\nto organize the points as Graph-of-Thoughts , where the edges represent the dependencies, and each\\npoint is decoded conditioned on the contents of its ancestor points. In addition, instead of complying\\nwith a static graph, we expect the need of having dynamic Graph-of-Thoughts , where the high-level\\nthought structure is adjusted dynamically by LLMs themselves. This could potentially combine the',\n",
              "  'question': 'Can explicit articulation of the thought process in language elicit high-quality answers from LLMs?\\n\\n',\n",
              "  'answer': 'Yes, recent research has shown that explicitly articulating the thought process in language can elicit high-quality answers from LLMs. Examples include CoT (Kojima et al., 2022; Wei et al., 2022), ToT (Yao et al., 2023), and ReAct (Yao et al., 2022). These findings resemble human thinking and suggest that LLMs can benefit from drawing from the human thinking process to facilitate more effective and efficient AI.',\n",
              "  'source_doc': 'Skeleton-of-Thought_ Large Language Models Can Do Parallel Decoding.pdf'},\n",
              " {'context': \"facts (Maynez et al., 2020; Ji et al., 2022), difï¬cul-\\nties in understanding low-resource languages (Lin\\net al., 2021), a lack of mathematical skills to per-\\nform precise calculations (Patel et al., 2021) and an\\nunawareness of the progression of time (Dhingra\\net al., 2022).\\nThe New England Journal of Medicine is a registered \\ntrademark of  [QA(â€œWho is the publisher of The New  \\nEngland Journal of Medicine?â€) â†’ Massachusetts  \\nMedical Society]  the MMS. \\nOut of 1400 participants, 400 (or [Calculator(400 / 1400)  \\nâ†’ 0.29]  29%) passed the test. \\nThe name derives from â€œla tortugaâ€, the Spanish word for \\n[MT(â€œtortugaâ€) â†’ turtle]  turtle. \\nThe Brown Act is Californiaâ€™s law  [WikiSearch(â€œBrown  \\nActâ€) â†’ The Ralph M. Brown Act is an act of the  \\nCalifornia State Legislature that guarantees the public's  \\nright to attend and participate in meetings of local  \\nlegislative bodies.]  that requires legislative bodies, like \\ncity councils, to hold their meetings open to the public. Figure 1: Exemplary predictions of Toolformer. The\\nmodel autonomously decides to call different APIs\\n(from top to bottom: a question answering system,\\na calculator, a machine translation system, and a\\nWikipedia search engine) to obtain information that is\\nuseful for completing a piece of text.\\nA simple way to overcome these limitations of\\ntodayâ€™s language models is to give them the abil-\\nity to use external tools such as search engines,\\ncalculators, or calendars. However, existing ap-\\nproaches either rely on large amounts of human\\nannotations (Komeili et al., 2022; Thoppilan et al.,\\n2022) or limit tool use to task-speciï¬c settings only\\n(e.g., Gao et al., 2022; Parisi et al., 2022), hinder-\\ning a more widespread adoption of tool use in LMs.\\nTherefore, we propose Toolformer , a model that\\nlearns to use tools in a novel way, which fulï¬lls the\\nfollowing desiderata:\\nâ€¢The use of tools should be learned in a\\nself-supervised way without requiring large\",\n",
              "  'question': 'How does Toolformer learn to use tools in a self-supervised way without requiring large amounts of human annotations or limiting tool use to task-specific settings?\\n\\n',\n",
              "  'answer': 'Toolformer learns to use tools in a self-supervised way by utilizing unsupervised learning techniques such as masked language modeling and next-sentence prediction. This allows the model to learn how to use tools without the need for large amounts of human annotations. Additionally, Toolformer is designed to be flexible and adaptable, allowing it to use tools in a variety of settings and tasks. This makes it more widely applicable and useful in a variety of contexts.',\n",
              "  'source_doc': 'Toolformer_ Language Models Can Teach Themselves to Use Tools.pdf'},\n",
              " {'context': '2015; Roy & Roth, 2015; Ling et al., 2017; Roy & Roth, 2018). Recently, more challenging datasets (Dua\\netal.,2019;Saxtonetal.,2019;Miaoetal.,2020;Aminietal.,2019;Hendrycksetal.,2021;Pateletal.,2021)\\nhave been proposed to increase the difficulty, diversity or even adversarial robustness. LiLA (Mishra et al.,\\n2022) proposes to assemble a large set of mathematical datasets into a unified dataset. LiLA also annotates\\nPython programs as the generation target for solving mathematical problems. However, LiLA (Mishra et al.,\\n2022) is mostly focused on dataset unification. Our work aims to understand how to generate â€˜thoughtful\\nprogramsâ€™ to best elicit LLMâ€™s reasoning capability. Besides, we also investigate how to solve math problems\\nwithout any exemplars. Austin et al. (2021) propose to evaluate LLMsâ€™ capabilities to synthesize code on\\ntwo curated datasets MBPP and MathQA-Python.\\n10Published in Transactions on Machine Learning Research (10/2023)\\n4.2 In-context Learning with LLMs\\nGPT-3 (Brown et al., 2020) demonstrated a strong capability to perform few-shot predictions, where the\\nmodel is given a description of the task in natural language with few examples. Scaling model size, data,\\nand computing are crucial to enable this learning ability. Recently, Rae et al. (2021); Smith et al. (2022);\\nChowdhery et al. (2022); Du et al. (2022) have proposed to train different types of LLMs with different\\ntraining recipes. The capability to follow few-shot exemplars to solve unseen tasks is not existent on smaller\\nLMs, but only emerge as the model scales up (Kaplan et al., 2020). Recently, there have been several\\nworks (Xie et al., 2021; Min et al., 2022) aiming to understand how and why in-context learning works.\\nAnother concurrent work similar to ours is BINDER (Cheng et al., 2022), which applies Codex to synthesize\\nâ€˜softâ€™ SQL queries to answer questions from tables.\\n4.3 Chain of Reasoning with LLMs',\n",
              "  'question': 'Can large language models (LLMs) perform in-context learning and generate thoughtful programs to solve mathematical problems without any exemplars?\\n\\n',\n",
              "  'answer': 'Yes, recent studies have shown that scaling the size of LLMs, data, and computing resources is crucial to enable their in-context learning ability to follow few-shot exemplars and solve unseen tasks. Additionally, there have been several works that aim to understand how and why in-context learning works, such as Xie et al. (2021) and Min et al. (2022). Furthermore, LLMs can also generate thoughtful programs to solve mathematical problems without any exemplars, as demonstrated by LiLA (Mishra et al., 2022).',\n",
              "  'source_doc': 'Program of Thoughts Prompting_  Disentangling Computation from Reasoning for Numerical Reasoning Tasks.pdf'},\n",
              " {'context': 'CodeBERT (Feng et al., 2020) and PyMT5 (Clement et al.,\\n2020).\\nSimilarly, our early investigation of GPT-3 (Brown et al.,\\n2020) revealed that it could generate simple programs from\\nPython docstrings. While rudimentary, this capability was\\nexciting because GPT-3 was not explicitly trained for code\\ngeneration. Given the considerable success of large lan-\\nguage models in other modalities and the abundance of\\npublicly available code, we hypothesized that a specialized\\nGPT model, called Codex, could excel at a variety of coding\\ntasks. This paper describes several early Codex models,\\nwhose descendants power GitHub Copilot and the Codex\\nmodels in the OpenAI API.arXiv:2107.03374v2  [cs.LG]  14 Jul 2021Evaluating Large Language Models Trained on Code\\nFigure 1. Pass rates of our models on the HumanEval dataset as a\\nfunction of model size. When a single sample is generated for each\\nproblem, GPT-12B solves no problems, but Codex (ï¬ne-tuned\\non code) solves 28.8% of the problems, and Codex-S (further\\nï¬ne-tuned on correctly implemented standalone functions) solves\\n37.7% of the problems. From here, further gains can be realized by\\ngenerating 100 samples per problem and selecting the sample with\\nthe highest mean log-probability (44.5% solved) or by selecting\\nthe sample that passes the unit tests (77.5% solved). All samples\\nare generated with temperature 0.8.\\nIn this work, we focus on the task of generating stan-\\ndalone Python functions from docstrings, and evaluate the\\ncorrectness of code samples automatically through unit\\ntests. This is in contrast to natural language generation,\\nwhere samples are typically evaluated by heuristics or by\\nhuman evaluators. To accurately benchmark our model,\\nwe create a dataset of 164 original programming problems\\nwith unit tests. These problems assess language compre-\\nhension, algorithms, and simple mathematics, with some\\ncomparable to simple software interview questions. We\\nrelease this data along with an evaluation framework at',\n",
              "  'question': 'What is the difference between GPT-12B and Codex in terms of their performance on the HumanEval dataset?\\n\\n',\n",
              "  'answer': 'Codex outperforms GPT-12B in generating standalone Python functions from docstrings on the HumanEval dataset. When a single sample is generated for each problem, Codex solves 28.8% of the problems, while GPT-12B solves no problems. However, when generating 100 samples per problem and selecting the sample with the highest mean log-probability, Codex solves 44.5% of the problems, while GPT-12B solves no problems. This suggests that Codex is better suited for generating code than GPT-12B.',\n",
              "  'source_doc': 'Evaluating Large Language Models Trained on Code.pdf'},\n",
              " {'context': 'In future work, this stage can easily accommodate techniques for more dynamic (i.e., test-time)\\nbootstrapping as well as automatic backtracking-like logic.\\n5 G OALS OF EVALUATION\\nProgramming frameworks can be evaluated along many dimensions: computational efficiency, de-\\nveloper efficiency, intuitiveness of the code and concepts, and so forth. In this paper, we focus on\\nperhaps the most pressing issue for current LM pipelines: the role of hand-written, task-specific\\nprompts in achieving performant systems. Our evaluations seek to test the following hypotheses:\\nH1With DSPy, we can replace hand-crafted prompt strings with concise and well-defined\\nmodules, without reducing quality or expressive power.\\nH2Parameterizing the modules and treating prompting as an optimization problem makes\\nDSPy better at adapting to different LMs, and it may outperform expert-written prompts.\\nH3The resulting modularity makes it possible to more thoroughly explore complex pipelines\\nthat have useful performance characteristics or that fit nuanced metrics.\\nOur evaluation will explore these hypotheses using diverse taskâ€“program pairs. We hope this begins\\na shift from underspecified questions like â€œhow do different LMs compare on GSM8Kâ€ toward â€œhow\\nthey compare on GSM8K with program P when compiled with strategy Sâ€, which is a well-defined\\nand reproducible run. Ultimately, our goal is to reduce the role of artful prompt construction in\\nmodern AI in favor of the development of new modular, composable programs and optimizers.\\n7Preprint\\nTable 1: Results with in-context learning on GSM8K math word problems. Each row represents\\na separate pipeline: the module in the Program column is compiled against the examples in the\\nTraining set. The programs, compilers, and (small) training sets are defined in Section 6. Rows with\\nensemble build on the immediately preceding row. Notably, all programs in this table are expressed',\n",
              "  'question': 'Can a programming framework like DSPy achieve performant systems by replacing hand-crafted prompt strings with concise and well-defined modules, without reducing quality or expressive power?\\n\\n',\n",
              "  'answer': 'Yes, DSPy can achieve performant systems by replacing hand-crafted prompt strings with concise and well-defined modules, without reducing quality or expressive power. This is supported by the results in Table 1 of the preprint, which shows that all programs in the table are expressed in DSPy and achieve high performance on GSM8K math word problems. Additionally, the preprint states that DSPy makes it possible to more thoroughly explore complex pipelines and may outperform expert-written prompts, indicating that it can achieve similar or better performance with less artful prompt construction.',\n",
              "  'source_doc': 'DSPy_  Compiling Declarative Language Model Calls into Self-Improving Pipelines.pdf'},\n",
              " {'context': 'Published as a conference paper at ICLR 2023\\nLEAST -TO-MOST PROMPTING ENABLES COMPLEX\\nREASONING IN LARGE LANGUAGE MODELS\\nDenny Zhouy\\x03Nathanael Sch Â¨arliyLe HouyJason WeiyNathan ScalesyXuezhi Wangy\\nDale SchuurmansyClaire CuiyOlivier BousquetyQuoc LeyEd Chiy\\nyGoogle Research, Brain Team\\nABSTRACT\\nChain-of-thought prompting has demonstrated remarkable performance on vari-\\nous natural language reasoning tasks. However, it tends to perform poorly on\\ntasks which requires solving problems harder than the exemplars shown in the\\nprompts. To overcome this challenge of easy-to-hard generalization, we propose\\na novel prompting strategy, least-to-most prompting . The key idea in this strat-\\negy is to break down a complex problem into a series of simpler subproblems\\nand then solve them in sequence. Solving each subproblem is facilitated by the\\nanswers to previously solved subproblems. Our experimental results on tasks re-\\nlated to symbolic manipulation, compositional generalization, and math reason-\\ning reveal that least-to-most prompting is capable of generalizing to more difï¬cult\\nproblems than those seen in the prompts. A notable ï¬nding is that when the GPT-3\\ncode-davinci-002 model is used with least-to-most prompting, it can solve\\nthe compositional generalization benchmark SCAN in any split (including length\\nsplit) with an accuracy of at least 99% using just 14 exemplars, compared to only\\n16% accuracy with chain-of-thought prompting. This is particularly noteworthy\\nbecause neural-symbolic models in the literature that specialize in solving SCAN\\nare trained on the entire training set containing over 15,000 examples. We have\\nincluded prompts for all the tasks in the Appendix.\\n1 I NTRODUCTION\\nDespite the great success of deep learning in the past decade, there still remain huge differences\\nbetween human intelligence and machine learning: (1) Given a new task, humans usually can learn\\nto accomplish it from only a few demonstration examples, while machine learning requires a large',\n",
              "  'question': 'Given a complex problem, how can we facilitate its solution by breaking it down into a series of simpler subproblems and solving them in sequence?\\n\\n',\n",
              "  'answer': 'Least-to-most prompting is a novel prompting strategy that breaks down a complex problem into a series of simpler subproblems and solves them in sequence. The key idea in this strategy is to use the answers to previously solved subproblems to facilitate the solution of each subsequent subproblem. Experimental results have shown that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts, and can achieve high accuracy on tasks related to symbolic manipulation, compositional generalization, and math reasoning with just a small number of exemplars.',\n",
              "  'source_doc': 'Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.pdf'},\n",
              " {'context': 'in the early stages of this work. We thank Sewon Min, Joongwon (Daniel) Kim, and Sandy Kaplan\\nfor valuable feedback on the paper, and Tianyu Gao and Weijia Shi for their help on evaluations.\\nAkari Asai is supported by the IBM Fellowship. We thank Stability AI for providing computing\\nto train and evaluate the LMs in this work, and Microsoft Accelerate Foundation Models Research\\nProgram for the access to OpenAI APIs. This work was funded in part by the DARPA MCS program\\nthrough NIWC Pacific (N66001-19-2-4031), NSF IIS-2044660, and gifts from AI2.',\n",
              "  'question': 'How do the LMs in this work handle the input sequence?\\n',\n",
              "  'answer': 'The LMs in this work handle the input sequence by using a transformer architecture with positional encoding. The transformer architecture allows the LMs to capture long-range dependencies in the input sequence, while the positional encoding provides information about the position of each token in the sequence. The LMs are trained on large amounts of text data and can generate text that is coherent and contextually relevant.',\n",
              "  'source_doc': 'Self-Rag_ Learning to Retrieve, Generate, and Critique Through Self-Reflection.pdf'},\n",
              " {'context': '(Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting\\nby a large margin and achieves new state-of-the-art performance. A prompting only approach is\\nimportant because it does not require a large training dataset and because a single model checkpoint\\ncan perform many tasks without loss of generality. This work underscores how large language models\\ncan learn via a few examples with natural language data about the task (c.f. automatically learning\\nthe patterns underlying inputs and outputs via a large training dataset).\\n2 Chain-of-Thought Prompting\\nConsider oneâ€™s own thought process when solving a complicated reasoning task such as a multi-step\\nmath word problem. It is typical to decompose the problem into intermediate steps and solve each\\nbefore giving the ï¬nal answer: â€œAfter Jane gives 2 ï¬‚owers to her mom she has 10 :::then after she\\ngives 3 to her dad she will have 7 :::so the answer is 7. â€ The goal of this paper is to endow language\\nmodels with the ability to generate a similar chain of thought â€”a coherent series of intermediate\\nreasoning steps that lead to the ï¬nal answer for a problem. We will show that sufï¬ciently large\\n2language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\\nprovided in the exemplars for few-shot prompting.\\nFigure 1 shows an example of a model producing a chain of thought to solve a math word problem\\nthat it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution\\nand can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it\\nmimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations\\ntypically come after the ï¬nal answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al.,\\n2022, inter alia )).\\nChain-of-thought prompting has several attractive properties as an approach for facilitating reasoning\\nin language models.',\n",
              "  'question': 'What is the difference between traditional prompting and chain-of-thought prompting in language models?\\n\\n',\n",
              "  'answer': 'Chain-of-thought prompting involves providing demonstrations of intermediate reasoning steps to language models, which allows them to generate a coherent series of steps leading to the final answer for a problem. This approach contrasts with traditional prompting, which does not provide such demonstrations and instead relies on a large training dataset and a single model checkpoint.',\n",
              "  'source_doc': 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.pdf'},\n",
              " {'context': 'other must-have treats on the Ice Cream Parlorâ€™s menu, including the house-made ice cream\\nsandwich (preferably ordered with a drizzled sauce!), the \"kidsâ€™ cone\" (itâ€™s totally OK to\\norder this as a grown ColdOnion Roll, since kids eat cake too!), and good olâ€™ Oldmanâ€™s Sweet Ice Cream based Mini\\nSix Peanut Butter Cups, as well as brand new Oreos Made from Oakwood. Anyway, we digress.\\nSo basically, we did a little code-changing this afternoon, and it turns out that weâ€™ll\\nbe doing the full-on \"YIMBA\" package once again. Nope, nobody added New Master Cake\\n(weâ€™ll have to share your images), and youâ€™re walking back your welcome to Polynesian\\nmacen as much as possible! Also, just to reiterate, all 10 of our Frozen Shorts! You\\nsee, we received one gift from Disney with a date of March 20. No luck deciding to use\\nanother date. No silly excuse, just a sense of nostalgia. See ya all next time! (This\\npattern has been gifted to us by Original Couture Forestes, which are mostly adorable\\nbecause they totally turned out their own thing!) Please feel free to tell us what you\\nthink in the comments below...weâ€™ll see you all on. But if you have any other ideas, let\\nus know and weâ€™ll try to make them a reality in the future, too! (C) Original Couture\\nForestes & Haircutters, YAMAHA Haircutters, and Innovative Mesh, all of whom deserve the\\naward for this awesome Wild Forest creativity. For more casual fan-inspired pixel-art,\\ncheck out these indie-design throwback images from the Splendour Of Sound booth! Look\\naround at this awesome hidden gem. Youâ€™re never going to be alone with these work done\\nby YOU! Other [And This Has When Youâ€™re on a Mission?!] samples from above or click the\\nINVERSELEads button below to explore [Colourful Imagery]. [Colourful Imagery samples] by\\nChloe Yee. Quite a handful of amazing Red Solo was shown at the Smithsonian in June 2009.\\nHave some of your favorite characters and locations be inspired by their design and art!',\n",
              "  'question': \"What types of ice cream treats are available on the Ice Cream Parlor's menu?\\n\",\n",
              "  'answer': \"The Ice Cream Parlor's menu includes house-made ice cream sandwiches, kids' cones, Oldman's Sweet Ice Cream based Mini Six Peanut Butter Cups, and brand new Oreos Made from Oakwood.\",\n",
              "  'source_doc': 'SequenceMatch_ Imitation Learning for_Autoregressive Sequence Modelling with Backtracking.pdf'},\n",
              " {'context': 'CHAIN -OF-VERIFICATION REDUCES HALLUCINATION\\nINLARGE LANGUAGE MODELS\\nShehzaad Dhuliawala\\nMeta AI & ETH Z Â¨urichMojtaba Komeili\\nMeta AIJing Xu\\nMeta AIRoberta Raileanu\\nMeta AI\\nXian Li\\nMeta AIAsli Celikyilmaz\\nMeta AIJason Weston\\nMeta AI\\nABSTRACT\\nGeneration of plausible yet incorrect factual information, termed hallucination,\\nis an unsolved issue in large language models. We study the ability of language\\nmodels to deliberate on the responses they give in order to correct their mistakes.\\nWe develop the Chain-of-Verification (C OVE) method whereby the model first (i)\\ndrafts an initial response; then (ii) plans verification questions to fact-check its\\ndraft; (iii) answers those questions independently so the answers are not biased\\nby other responses; and (iv) generates its final verified response. In experiments,\\nwe show COVEdecreases hallucinations across a variety of tasks, from list-based\\nquestions from Wikidata, closed book MultiSpanQA and longform text generation.\\n1 I NTRODUCTION\\nLarge Language Models (LLMs) are trained on huge corpora of text documents with billions of\\ntokens of text. It has been shown that as the number of model parameters is increased, performance\\nat tasks such as closed book QA improve in accuracy, and larger models can generate more correct\\nfactual statements (Radford et al., 2019; Petroni et al., 2019). However, even the largest models can\\nstill fail, particularly on lesser known torso and tail distribution facts (Sun et al., 2023a), i.e. those\\nthat occur relatively rarely in the training corpora. In those cases where the model is incorrect, they\\ninstead generate an alternative response which is typically plausible looking (e.g., a similar entity, but\\nan incorrect one). These factually incorrect generations are referred to as hallucinations (Maynez\\net al., 2020). Further, in longform tasks consisting of generating multiple sentences or paragraphs, the',\n",
              "  'question': 'Can the Chain-of-Verification (C OVE) method reduce hallucinations in large language models?\\n\\n',\n",
              "  'answer': 'Yes, the Chain-of-Verification (C OVE) method has been shown to decrease hallucinations across a variety of tasks, including list-based questions from Wikidata, closed book MultiSpanQA, and longform text generation.',\n",
              "  'source_doc': 'Chain-of-Verification Reduces Hallucination in Large Language Models.pdf'},\n",
              " {'context': 'Graph of Thoughts: Solving Elaborate Problems with Large Language Models\\nMaciej Besta1*, Nils Blach1*, Ales Kubicek1, Robert Gerstenberger1,\\nLukas Gianinazzi1, Joanna Gajda2, Tomasz Lehmann2, MichaÅ‚ Podstawski3,\\nHubert Niewiadomski2, Piotr Nyczyk2, Torsten Hoefler1\\n1ETH Zurich,2Cledar,3Warsaw University of Technology\\nbestam@inf.ethz.ch, nils.blach@inf.ethz.ch, htor@inf.ethz.ch\\nAbstract\\nWe introduce Graph of Thoughts (GoT): a framework that\\nadvances prompting capabilities in large language models\\n(LLMs) beyond those offered by paradigms such as Chain-of-\\nThought or Tree of Thoughts (ToT). The key idea and primary\\nadvantage of GoT is the ability to model the information gen-\\nerated by an LLM as an arbitrary graph , where units of infor-\\nmation (â€œLLM thoughtsâ€) are vertices, and edges correspond\\nto dependencies between these vertices. This approach en-\\nables combining arbitrary LLM thoughts into synergistic out-\\ncomes, distilling the essence of whole networks of thoughts,\\nor enhancing thoughts using feedback loops. We illustrate\\nthat GoT offers advantages over state of the art on different\\ntasks, for example increasing the quality of sorting by 62%\\nover ToT, while simultaneously reducing costs by >31%.\\nWe ensure that GoT is extensible with new thought transfor-\\nmations and thus can be used to spearhead new prompting\\nschemes. This work brings the LLM reasoning closer to hu-\\nman thinking or brain mechanisms such as recurrence, both\\nof which form complex networks.\\nWebsite & code: https://github.com/spcl/graph-of-thoughts\\n1 Introduction\\nLarge language models (LLMs) are taking over the world\\nof AI. Recent years saw a rapid development of models pri-\\nmarily based on the decoder-only Transformer variant [64],\\nsuch as GPT [52, 51, 14, 13], PaLM [19], or LLaMA [62].\\nPrompt engineering is a resource-efficient approach for\\nsolving different LLM tasks. In brief, one includes the task\\ndescription within the input sent to an LLM. If this descrip-',\n",
              "  'question': 'In Graph of Thoughts, how does the ability to model information generated by an LLM as an arbitrary graph enable synergistic outcomes and enhance thoughts?\\n\\n',\n",
              "  'answer': 'The ability to model information generated by an LLM as an arbitrary graph in Graph of Thoughts enables synergistic outcomes and enhances thoughts by allowing units of information, or LLM thoughts, to be combined in arbitrary ways. This enables the distillation of the essence of whole networks of thoughts and the use of feedback loops to improve the quality of the output. This approach is more flexible and powerful than other paradigms such as Chain-of-Thought or Tree of Thoughts (ToT), which limit the way in which LLM thoughts can be combined. By enabling more complex and nuanced combinations of LLM thoughts, Graph of Thoughts brings LLM reasoning closer to human thinking or brain mechanisms such as recurrence.',\n",
              "  'source_doc': 'Graph of Thoughts_ Solving Elaborate Problems with Large Language Models.pdf'},\n",
              " {'context': 'framework. We begin by deï¬ning the pass@kmetric, and\\nexplain its advantages over standard match-based metrics.\\nNext, we describe the dataset of hand-written problems,\\ncalled â€œHumanEval,â€ which we created in order to bench-\\nmark our models. Finally, we discuss the sandbox environ-\\nment we used to safely execute model-generated code.\\n2.1. Functional Correctness\\nGenerative models for code are predominantly benchmarked\\nby matching samples against a reference solution, where\\nthe match can be exact or fuzzy (as in BLEU score). How-\\never, recent work has surfaced deï¬ciencies in match-based\\nmetrics for code. For instance, Ren et al. (2020) ï¬nds that\\nBLEU has problems capturing semantic features speciï¬c\\nto code, and suggests several semantic modiï¬cations to the\\nscore.\\nMore fundamentally, match-based metrics are unable to ac-\\ncount for the large and complex space of programs function-\\nally equivalent to a reference solution. As a consequence,\\nrecent works in unsupervised code translation (Lachaux\\net al., 2020) and pseudocode-to-code translation (Kulal et al.,\\n2019) have turned to functional correctness instead, where\\na sample is considered correct if it passes a set of unit tests.\\nWe argue that this metric should be applied to docstring-\\nconditional code generation as well.\\nPerhaps the most convincing reason to evaluate functional\\ncorrectness is that it is used by human developers to judge\\ncode. A framework known as test-driven development dic-\\ntates that software requirements be converted into test cases\\nbefore any implementation begins, and success is deï¬ned\\nby a program that passes these tests. While few organiza-\\ntions employ full test-driven development, integration of\\nnew code is usually dependent on creating and passing unit\\ntests.\\nKulal et al. (2019) evaluate functional correctness using\\nthe pass@kmetric, where kcode samples are generated\\nper problem, a problem is considered solved if any sampleEvaluating Large Language Models Trained on Code',\n",
              "  'question': 'In code generation, what is the advantage of using functional correctness over match-based metrics?\\n\\n',\n",
              "  'answer': 'Functional correctness is advantageous in code generation because it accounts for the large and complex space of programs functionally equivalent to a reference solution, whereas match-based metrics are unable to do so. Additionally, functional correctness is used by human developers to judge code, making it a more convincing metric to evaluate code generation.',\n",
              "  'source_doc': 'Evaluating Large Language Models Trained on Code.pdf'}]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "aUlOUDv59jVN",
        "outputId": "c9634fdb-2a7f-43a6-c4eb-e60b166b8238"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth',800)\n",
        "display(pd.DataFrame(outputs).head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KG4dNtg9jVN"
      },
      "source": [
        "### 1.3. Setup critique agents\n",
        "\n",
        "The questions generated by the previous agent can have many flaws: we should do a quality check before validating these questions.\n",
        "\n",
        "We thus build critique agents that will rate each question on several criteria, given in [this paper](https://huggingface.co/papers/2312.10003):\n",
        "- **Groundedness:** can the question be answered from the given context?\n",
        "- **Relevance:** is the question relevant to users? For instance, `\"What is the date when transformers 4.29.1 was released?\"` is not relevant for ML practicioners.\n",
        "\n",
        "One last failure case we've noticed is when a function is tailored for the particular setting where the question was generated, but undecipherable by itself, like `\"What is the name of the function used in this guide?\"`.\n",
        "We also build a critique agent for this criteria:\n",
        "- **Stand-alone**: is the question understandable free of any context, for someone with domain knowledge/Internet access? The opposite of this would be `What is the function used in this article?` for a question generated from a specific blog article.\n",
        "\n",
        "We systematically score functions with all these agents, and whenever the score is too low for any one of the agents, we eliminate the question from our eval dataset.\n",
        "\n",
        "ğŸ’¡ ___When asking the agents to output a score, we first ask them to produce its rationale. This will help us verify scores, but most importantly, asking it to first output rationale gives the model more tokens to think and elaborate an answer before summarizing it into a single score token.___\n",
        "\n",
        "We now build and run these critique agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05aSgTGs9jVO"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "### Semi-working backup\n",
        "# question_groundedness_critique_prompt = \"\"\"\n",
        "# You will be given a context and a question.\n",
        "# Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
        "# Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
        "\n",
        "# Provide your answer as follows:\n",
        "\n",
        "# Answer:::\n",
        "# Evaluation: (your rationale for the rating, as a text)\n",
        "# Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "# Now here are the question and context.\n",
        "\n",
        "# Question: {question}\\n\n",
        "# Context: {context}\\n\n",
        "# Answer::: \"\"\"\n",
        "\n",
        "# question_relevance_critique_prompt = \"\"\"\n",
        "# You will be given a question.\n",
        "# Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
        "# Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
        "\n",
        "# Provide your answer as follows:\n",
        "\n",
        "# Answer:::\n",
        "# Evaluation: (your rationale for the rating, as a text)\n",
        "# Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "# Now here is the question.\n",
        "\n",
        "# Question: {question}\\n\n",
        "# Answer::: \"\"\"\n",
        "\n",
        "# question_standalone_critique_prompt = \"\"\"\n",
        "# You will be given a question.\n",
        "# Your task is to provide a 'total rating' representing how context-independent this question is.\n",
        "# Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
        "# For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
        "# The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
        "\n",
        "# For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\n",
        "\n",
        "# Provide your answer as follows:\n",
        "\n",
        "# Answer:::\n",
        "# Evaluation: (your rationale for the rating, as a text)\n",
        "# Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "# Now here is the question.\n",
        "\n",
        "# Question: {question}\\n\n",
        "# Answer::: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "## Semi-working backup\n",
        "question_groundedness_critique_prompt = \"\"\"\n",
        "You will be given a context and a question.\n",
        "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: !!!(your rating, as a number between 1 and 5)!!!\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.  'Total rating:' should be encolsed in !!! and !!! as in '!!!4.5!!!'.\n",
        "\n",
        "\n",
        "Now here are the question and context.\n",
        "\n",
        "Question: {question}\\n\n",
        "Context: {context}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_relevance_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: !!!(your rating, as a number between 1 and 5)!!!\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.  'Total rating:' should be encolsed in !!! and !!! as in '!!!4.5!!!'.\n",
        "\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_standalone_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how context-independent this question is.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
        "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
        "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
        "\n",
        "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: !!!(your rating, as a number between 1 and 5)!!!\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.  'Total rating:' should be encolsed in !!! and !!! as in '!!!4.5!!!'.\n",
        "\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ## Semi-working backup\n",
        "# question_groundedness_critique_prompt = \"\"\"\n",
        "# You will be given a context and a question.\n",
        "# Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
        "# Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
        "\n",
        "# Provide your answer as follows:\n",
        "\n",
        "# Answer:::\n",
        "# Evaluation: (your rationale for the rating, as a text)\n",
        "# Total rating: !!!(your rating, as a number between 1 and 5)!!!\n",
        "\n",
        "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.  'Total rating:' should be preceded and followd by '!!!' as in 'Total rating: !!!4.5!!!'\n",
        "\n",
        "# Now here are the question and context.\n",
        "\n",
        "# Question: {question}\\n\n",
        "# Context: {context}\\n\n",
        "# Answer::: \"\"\"\n",
        "\n",
        "# question_relevance_critique_prompt = \"\"\"\n",
        "# You will be given a question.\n",
        "# Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
        "# Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
        "\n",
        "# Provide your answer as follows:\n",
        "\n",
        "# Answer:::\n",
        "# Evaluation: (your rationale for the rating, as a text)\n",
        "# Total rating: !!!(your rating, as a number between 1 and 5)!!!\n",
        "\n",
        "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.  'Total rating:' should be preceded and followd by '!!!' as in 'Total rating: !!!4.5!!!'\n",
        "\n",
        "# Now here is the question.\n",
        "\n",
        "# Question: {question}\\n\n",
        "# Answer::: \"\"\"\n",
        "\n",
        "# question_standalone_critique_prompt = \"\"\"\n",
        "# You will be given a question.\n",
        "# Your task is to provide a 'total rating' representing how context-independent this question is.\n",
        "# Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
        "# For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
        "# The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
        "\n",
        "# For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\n",
        "\n",
        "# Provide your answer as follows:\n",
        "\n",
        "# Answer:::\n",
        "# Evaluation: (your rationale for the rating, as a text)\n",
        "# Total rating: !!!(your rating, as a number between 1 and 5)!!!\n",
        "\n",
        "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.  'Total rating:' should be preceded and followd by '!!!' as in 'Total rating: !!!4.5!!!'\n",
        "\n",
        "# Now here is the question.\n",
        "\n",
        "# Question: {question}\\n\n",
        "# Answer::: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "b9tbk7ME9jVO"
      },
      "outputs": [],
      "source": [
        "# import re\n",
        "# print(\"Generating critique for each QA couple...\")\n",
        "# for output in tqdm(outputs[:1]):\n",
        "#     evaluations = {\n",
        "#         \"groundedness\": call_llm(question=question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]), \n",
        "#                                 generator=generator_llm,\n",
        "#                                 tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "#                                 max_new_tokens=1024),\n",
        "#         \"relevance\": call_llm(question=question_relevance_critique_prompt.format(question=output[\"question\"]), \n",
        "#                                 generator=generator_llm,\n",
        "#                                 tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "#                                 max_new_tokens=1024),\n",
        "                    \n",
        "#         \"standalone\": call_llm(question=question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
        "#                                 generator=generator_llm,\n",
        "#                                 tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "#                                 max_new_tokens=1024)\n",
        "#     }\n",
        "#     try:\n",
        "#         # for criterion, evaluation in evaluations.items():\n",
        "#         #     # score, eval = (\n",
        "#         #     #     (evaluation.split(\"Total rating: \")[-1].strip()),\n",
        "#         #     #     evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
        "#         #     # )\n",
        "#         #     score, eval = (\n",
        "#         #         float(evaluation.split(\"!!!\")[1]),\n",
        "#         #         evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
        "#         #     )\n",
        "#         #     output.update(\n",
        "#         #         {\n",
        "#         #             f\"{criterion}_score\": score,\n",
        "#         #             f\"{criterion}_eval\": eval,\n",
        "#         #         }\n",
        "#         #     )\n",
        "#         for criterion, evaluation in evaluations.items():\n",
        "#             score_match = re.search(r'Total rating: !!!([\\d\\.]+)!!!', evaluation)\n",
        "#             #re.search(r'Total rating: !!!([\\d\\.]+)!!!.*', evaluation)\n",
        "#             eval_match = re.search(r'Evaluation: (.*?)Total rating:', evaluation)\n",
        "            \n",
        "#             if score_match and eval_match:\n",
        "#                 score = float(score_match.group(1))\n",
        "#                 eval = eval_match.group(1).strip()\n",
        "#                 output.update(\n",
        "#                     {\n",
        "#                         f\"{criterion}_score\": score,\n",
        "#                         f\"{criterion}_eval\": eval,\n",
        "#                     }\n",
        "#                 )\n",
        "#     except Exception as e:\n",
        "#         print(\"\\033[91m\" + f\"EVALUATION:\" + \"\\033[0m\")\n",
        "#         print(evaluations)\n",
        "#         print(\"\\033[91m\" + f\"EXCEPTION: {e}\" + \"\\033[0m\")\n",
        "#         break\n",
        "#         #continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "question_groundedness_critique_prompt = \"\"\"\n",
        "You will be given a context and a question.\n",
        "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here are the question and context.\n",
        "\n",
        "Question: {question}\\n\n",
        "Context: {context}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_relevance_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_standalone_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
        "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
        "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
        "\n",
        "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Generating critique for each QA couple...\")\n",
        "for output in tqdm(outputs):\n",
        "    evaluations = {\n",
        "        \"groundedness\": call_llm(question=question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]), \n",
        "                                generator=generator_llm,\n",
        "                                tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "                                max_new_tokens=1024),\n",
        "        \"relevance\": call_llm(question=question_relevance_critique_prompt.format(question=output[\"question\"]), \n",
        "                                generator=generator_llm,\n",
        "                                tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "                                max_new_tokens=1024),\n",
        "                    \n",
        "        \"standalone\": call_llm(question=question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
        "                                generator=generator_llm,\n",
        "                                tokenizer=generator_tokenizer,settings=generator_settings,\n",
        "                                max_new_tokens=1024)\n",
        "    }\n",
        "    try:\n",
        "        for criterion, evaluation in evaluations.items():\n",
        "            score, eval = (\n",
        "                # int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
        "                (evaluation.split(\"Total rating: \")[-1].strip()),\n",
        "                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
        "            )\n",
        "            output.update(\n",
        "                {\n",
        "                    f\"{criterion}_score\": score,\n",
        "                    f\"{criterion}_eval\": eval,\n",
        "                }\n",
        "            )\n",
        "    except Exception as e:\n",
        "        #print(\"\\033[91m\" + f\"EVALUATION:\" + \"\\033[0m\")\n",
        "        #print(evaluations)\n",
        "        #print(\"\\033[91m\" + f\"EXCEPTION: {e}\" + \"\\033[0m\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'groundedness': \"<s>[INST] \\nYou will be given a context and a question.\\nYour task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\\nGive your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\\n\\nNow here are the question and context.\\n\\nQuestion: Can incorporating ReAct and CoT-SC improve the problem solving process in reasoning structure?\\n\\n\\n\\nContext: and reinforcement learning instead of prompting.\\nCombining Internal and External Knowledge As will be detail in Section 3.3, we observe that\\nthe problem solving process demonstrated by ReAct is more factual and grounded, whereas CoT\\nis more accurate in formulating reasoning structure but can easily suffer from hallucinated facts\\nor thoughts. We therefore propose to incorporate ReAct andCoT-SC , and let the model decide\\nwhen to switch to the other method based on the following heuristics: A) ReAct!CoT-SC : when\\nReAct fails to return an answer within given steps, back off to CoT-SC . We set 7 and 5 steps for\\nHotpotQA and FEVER respectively as we ï¬nd more steps will not improve ReAct performance3.\\nB)CoT-SC!ReAct : when the majority answer among nCoT-SC samples occurs less than n=2\\ntimes (i.e. internal knowledge might not support the task conï¬dently), back off to ReAct .\\nFinetuning Due to the challenge of manually annotating reasoning traces and actions at scale,\\nwe consider a bootstraping approach similar to Zelikman et al. (2022), using 3,000 trajectories\\nwith correct answers generated by ReAct (also for other baselines) to ï¬netune smaller language\\nmodels (PaLM-8/62B) to decode trajectories (all thoughts, actions, observations) conditioned on\\ninput questions/claims. More details are in Appendix B.1.\\n3.3 R ESULTS AND OBSERVATIONS\\nReAct outperforms Act consistently Table 1 shows HotpotQA and Fever results using PaLM-\\n540B as the base model with different prompting methods. We note that ReAct is better than Act\\non both tasks, demonstrating the value of reasoning to guide acting, especially for synthesizing the\\nï¬nal answer, as shown in Figure 1 (1c-d). Fine-tuning results 3 also conï¬rm the beneï¬t of reasoning\\ntraces for more informed acting.\\n3Of all trajectories with correct ï¬nal answers, those with 7 steps on HotpotQA and 5 steps on FEVER only\\ntake up 0.84% and 1.33% respectively.\\n5Published as a conference paper at ICLR 2023\\nType Deï¬nition ReAct CoT\\n\\nAnswer:::  [/INST] Evaluation:\\nThis question requires a clear understanding of the problem-solving process demonstrated by ReAct and CoT-SC, as well as the proposed method for incorporating these two methods. The context provides detailed information about the performance of ReAct and CoT-SC on reasoning structure, as well as the finetuning approach used to improve their performance.\\n\\nTotal rating: 4\",\n",
              " 'relevance': \"<s>[INST] \\nYou will be given a question.\\nYour task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\\nGive your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\\n\\nNow here is the question.\\n\\nQuestion: Can incorporating ReAct and CoT-SC improve the problem solving process in reasoning structure?\\n\\n\\n\\nAnswer:::  [/INST] Evaluation: The question explores the potential benefits of incorporating ReAct and CoT-SC in reasoning structure. ReAct is a model for reasoning about actions that takes into account the context of the action, while CoT-SC is a model for reasoning about events that considers the temporal and spatial relationships between events. Both models have shown promising results in the field of natural language processing. However, the question does not provide a clear context for how these models can improve the problem solving process in reasoning structure. Without more information about the specific problem domain and the methods used to incorporate the models, it is difficult to evaluate the usefulness of this question.\\n\\nTotal rating: 2\",\n",
              " 'standalone': '<s>[INST] \\nYou will be given a question.\\nYour task is to provide a \\'total rating\\' representing how context-independant this question is.\\nGive your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\\nFor instance, if the question refers to a particular setting, like \\'in the context\\' or \\'in the document\\', the rating must be 1.\\nThe questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\\n\\nFor instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\\n\\nProvide your answer as follows:\\n\\nAnswer:::\\nEvaluation: (your rationale for the rating, as a text)\\nTotal rating: (your rating, as a number between 1 and 5)\\n\\nYou MUST provide values for \\'Evaluation:\\' and \\'Total rating:\\' in your answer.\\n\\nNow here is the question.\\n\\nQuestion: Can incorporating ReAct and CoT-SC improve the problem solving process in reasoning structure?\\n\\n\\n\\nAnswer:::  [/INST] Evaluation:\\n\\nThe question is not context-dependent and can be understood without any additional information. It refers to the use of ReAct and CoT-SC in improving problem-solving processes in reasoning structures. The question also implies the use of these techniques in a reasoning environment, which is a common context in the field of artificial intelligence and reasoning systems.\\n\\nTotal rating: 5'}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQv36Y_f9jVO"
      },
      "source": [
        "Now let us filter out bad questions based on our critique agent scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['context', 'question', 'answer', 'source_doc', 'groundedness_score', 'groundedness_eval'])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "# generated_questions = pd.DataFrame.from_dict(output)\n",
        "\n",
        "# print(\"Evaluation dataset before filtering:\")\n",
        "# display(\n",
        "#     generated_questions.reindex(\n",
        "#         [\n",
        "#             \"question\",\n",
        "#             \"answer\",\n",
        "#             \"groundedness_score\",\n",
        "#             \"relevance_score\",\n",
        "#             \"standalone_score\",\n",
        "#         ],\n",
        "#         axis=1)\n",
        "    \n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation dataset before filtering:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>groundedness_score</th>\n",
              "      <th>relevance_score</th>\n",
              "      <th>standalone_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Can a large language model with parallel decoding be designed to think more like a human for answer quality?\\n\\n</td>\n",
              "      <td>Yes, the proposed Skeleton-of-Thought (SoT) method in the context is an attempt to optimize the inference efficiency of large language models (LLMs) for data-centric optimization, which can potentially improve the answer quality on several question categories. This is achieved by guiding the LLMs to generate the skeleton of the answer first and then conducting parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. The method is an initial attempt at data-centric optimization for inference efficiency and underscores the potential of pushing LLMs to think more like a human for answer quality.</td>\n",
              "      <td>3\\n\\nTherefore, the total rating is 3 out of 5.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Does the dataset augmentation with samples from the model improve the performance of the scratchpad model, while it harms the direct execution model?\\n\\n</td>\n",
              "      <td>Yes, the dataset augmentation with samples from the model improves the performance of the scratchpad model, while it harms the direct execution model.</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In relation to the results presented in Table 3, what is the significance of the differences in word-level success rates between IO, CoT, Oracle, and ToT prompting methods, and how do these differences impact the performance of the models in solving crossword games?\\n\\n</td>\n",
              "      <td>The results in Table 3 show that IO and CoT prompting methods have a word-level success rate of less than 16%, while ToT significantly improves all metrics, achieving a word-level success rate of 60% and solving 4 out of 20 games. This indicates that ToT has mechanisms in place to try different clues, make changes to decisions, and backtrack, which are lacking in IO and CoT. Additionally, when outputting from the oracle best DFS state, ToT performance is even higher and actually solves 7/20 games, indicating that the simple output heuristics used can be improved. The importance of backtracking is also confirmed by an ablation that keeps filling the most promising clue for at most 20 steps, which performs poorly with a word-level success rate of only 20%.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Given the context, how does the Mini Crosswords algorithm propose and aggregate thoughts in a priority queue during a depth-first search (DFS) for solving crossword puzzles?\\n\\n</td>\n",
              "      <td>The Mini Crosswords algorithm proposes and aggregates thoughts in a priority queue during a depth-first search (DFS) for solving crossword puzzles by first generating all possible thoughts for each given clue. These thoughts are then evaluated based on the possibility of filling in each remaining word clue, and pruned if any remaining clue is deemed not possible to fill by the LM. The remaining thoughts are then sorted in a priority queue based on their evaluation scores, and the algorithm explores the next promising thought for clue in the priority queue. This process continues until the deepest explored state is rendered into the final output.</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4\\n\\nThe Mini Crosswords algorithm is a well-documented and widely used method for solving crossword puzzles, and its DFS approach is a common technique in NLP applications. Additionally, the use of a priority queue to aggregate thoughts is a common approach in NLP, and understanding how this process works can be useful for developers building NLP applications. However, the specifics of the Mini Crosswords algorithm may not be applicable to all NLP applications, and the question could benefit from more context and specificity.</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How can dialogue models be improved to better align with human judgements?\\n\\n</td>\n",
              "      <td>Dialogue models can be improved to better align with human judgements by incorporating targeted human judgements into the training process. This can be done through methods such as active learning and human-in-the-loop approaches. Additionally, improving the quality of the training data and using more advanced techniques such as transfer learning and low-rank hypercomplex adapter layers can also help improve alignment.</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>What is the difference between traditional prompting and chain-of-thought prompting in language models?\\n\\n</td>\n",
              "      <td>Chain-of-thought prompting involves providing demonstrations of intermediate reasoning steps to language models, which allows them to generate a coherent series of steps leading to the final answer for a problem. This approach contrasts with traditional prompting, which does not provide such demonstrations and instead relies on a large training dataset and a single model checkpoint.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>What types of ice cream treats are available on the Ice Cream Parlor's menu?\\n</td>\n",
              "      <td>The Ice Cream Parlor's menu includes house-made ice cream sandwiches, kids' cones, Oldman's Sweet Ice Cream based Mini Six Peanut Butter Cups, and brand new Oreos Made from Oakwood.</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>Can the Chain-of-Verification (C OVE) method reduce hallucinations in large language models?\\n\\n</td>\n",
              "      <td>Yes, the Chain-of-Verification (C OVE) method has been shown to decrease hallucinations across a variety of tasks, including list-based questions from Wikidata, closed book MultiSpanQA, and longform text generation.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>In Graph of Thoughts, how does the ability to model information generated by an LLM as an arbitrary graph enable synergistic outcomes and enhance thoughts?\\n\\n</td>\n",
              "      <td>The ability to model information generated by an LLM as an arbitrary graph in Graph of Thoughts enables synergistic outcomes and enhances thoughts by allowing units of information, or LLM thoughts, to be combined in arbitrary ways. This enables the distillation of the essence of whole networks of thoughts and the use of feedback loops to improve the quality of the output. This approach is more flexible and powerful than other paradigms such as Chain-of-Thought or Tree of Thoughts (ToT), which limit the way in which LLM thoughts can be combined. By enabling more complex and nuanced combinations of LLM thoughts, Graph of Thoughts brings LLM reasoning closer to human thinking or brain mechanisms such as recurrence.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>In code generation, what is the advantage of using functional correctness over match-based metrics?\\n\\n</td>\n",
              "      <td>Functional correctness is advantageous in code generation because it accounts for the large and complex space of programs functionally equivalent to a reference solution, whereas match-based metrics are unable to do so. Additionally, functional correctness is used by human developers to judge code, making it a more convincing metric to evaluate code generation.</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0. The functional correctness approach to code generation is useful for ensuring correctness, especially in critical applications. However, match-based metrics may not guarantee correctness and may lead to false positives or false negatives.</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>300 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                           question  \\\n",
              "0                                                                                                                                                                  Can a large language model with parallel decoding be designed to think more like a human for answer quality?\\n\\n   \n",
              "1                                                                                                                         Does the dataset augmentation with samples from the model improve the performance of the scratchpad model, while it harms the direct execution model?\\n\\n   \n",
              "2    In relation to the results presented in Table 3, what is the significance of the differences in word-level success rates between IO, CoT, Oracle, and ToT prompting methods, and how do these differences impact the performance of the models in solving crossword games?\\n\\n   \n",
              "3                                                                                                 Given the context, how does the Mini Crosswords algorithm propose and aggregate thoughts in a priority queue during a depth-first search (DFS) for solving crossword puzzles?\\n\\n   \n",
              "4                                                                                                                                                                                                    How can dialogue models be improved to better align with human judgements?\\n\\n   \n",
              "..                                                                                                                                                                                                                                                                              ...   \n",
              "295                                                                                                                                                                     What is the difference between traditional prompting and chain-of-thought prompting in language models?\\n\\n   \n",
              "296                                                                                                                                                                                                  What types of ice cream treats are available on the Ice Cream Parlor's menu?\\n   \n",
              "297                                                                                                                                                                                Can the Chain-of-Verification (C OVE) method reduce hallucinations in large language models?\\n\\n   \n",
              "298                                                                                                                 In Graph of Thoughts, how does the ability to model information generated by an LLM as an arbitrary graph enable synergistic outcomes and enhance thoughts?\\n\\n   \n",
              "299                                                                                                                                                                         In code generation, what is the advantage of using functional correctness over match-based metrics?\\n\\n   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           answer  \\\n",
              "0                                                                                                                              Yes, the proposed Skeleton-of-Thought (SoT) method in the context is an attempt to optimize the inference efficiency of large language models (LLMs) for data-centric optimization, which can potentially improve the answer quality on several question categories. This is achieved by guiding the LLMs to generate the skeleton of the answer first and then conducting parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. The method is an initial attempt at data-centric optimization for inference efficiency and underscores the potential of pushing LLMs to think more like a human for answer quality.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Yes, the dataset augmentation with samples from the model improves the performance of the scratchpad model, while it harms the direct execution model.   \n",
              "2    The results in Table 3 show that IO and CoT prompting methods have a word-level success rate of less than 16%, while ToT significantly improves all metrics, achieving a word-level success rate of 60% and solving 4 out of 20 games. This indicates that ToT has mechanisms in place to try different clues, make changes to decisions, and backtrack, which are lacking in IO and CoT. Additionally, when outputting from the oracle best DFS state, ToT performance is even higher and actually solves 7/20 games, indicating that the simple output heuristics used can be improved. The importance of backtracking is also confirmed by an ablation that keeps filling the most promising clue for at most 20 steps, which performs poorly with a word-level success rate of only 20%.   \n",
              "3                                                                                                                   The Mini Crosswords algorithm proposes and aggregates thoughts in a priority queue during a depth-first search (DFS) for solving crossword puzzles by first generating all possible thoughts for each given clue. These thoughts are then evaluated based on the possibility of filling in each remaining word clue, and pruned if any remaining clue is deemed not possible to fill by the LM. The remaining thoughts are then sorted in a priority queue based on their evaluation scores, and the algorithm explores the next promising thought for clue in the priority queue. This process continues until the deepest explored state is rendered into the final output.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                          Dialogue models can be improved to better align with human judgements by incorporating targeted human judgements into the training process. This can be done through methods such as active learning and human-in-the-loop approaches. Additionally, improving the quality of the training data and using more advanced techniques such as transfer learning and low-rank hypercomplex adapter layers can also help improve alignment.   \n",
              "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ...   \n",
              "295                                                                                                                                                                                                                                                                                                                                                                                             Chain-of-thought prompting involves providing demonstrations of intermediate reasoning steps to language models, which allows them to generate a coherent series of steps leading to the final answer for a problem. This approach contrasts with traditional prompting, which does not provide such demonstrations and instead relies on a large training dataset and a single model checkpoint.   \n",
              "296                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The Ice Cream Parlor's menu includes house-made ice cream sandwiches, kids' cones, Oldman's Sweet Ice Cream based Mini Six Peanut Butter Cups, and brand new Oreos Made from Oakwood.   \n",
              "297                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Yes, the Chain-of-Verification (C OVE) method has been shown to decrease hallucinations across a variety of tasks, including list-based questions from Wikidata, closed book MultiSpanQA, and longform text generation.   \n",
              "298                                             The ability to model information generated by an LLM as an arbitrary graph in Graph of Thoughts enables synergistic outcomes and enhances thoughts by allowing units of information, or LLM thoughts, to be combined in arbitrary ways. This enables the distillation of the essence of whole networks of thoughts and the use of feedback loops to improve the quality of the output. This approach is more flexible and powerful than other paradigms such as Chain-of-Thought or Tree of Thoughts (ToT), which limit the way in which LLM thoughts can be combined. By enabling more complex and nuanced combinations of LLM thoughts, Graph of Thoughts brings LLM reasoning closer to human thinking or brain mechanisms such as recurrence.   \n",
              "299                                                                                                                                                                                                                                                                                                                                                                                                                   Functional correctness is advantageous in code generation because it accounts for the large and complex space of programs functionally equivalent to a reference solution, whereas match-based metrics are unable to do so. Additionally, functional correctness is used by human developers to judge code, making it a more convincing metric to evaluate code generation.   \n",
              "\n",
              "                                  groundedness_score  \\\n",
              "0    3\\n\\nTherefore, the total rating is 3 out of 5.   \n",
              "1                                                  3   \n",
              "2                                                NaN   \n",
              "3                                                4.5   \n",
              "4                                                4.5   \n",
              "..                                               ...   \n",
              "295                                              NaN   \n",
              "296                                                3   \n",
              "297                                              NaN   \n",
              "298                                              NaN   \n",
              "299                                              4.5   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          relevance_score  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     NaN   \n",
              "3    4\\n\\nThe Mini Crosswords algorithm is a well-documented and widely used method for solving crossword puzzles, and its DFS approach is a common technique in NLP applications. Additionally, the use of a priority queue to aggregate thoughts is a common approach in NLP, and understanding how this process works can be useful for developers building NLP applications. However, the specifics of the Mini Crosswords algorithm may not be applicable to all NLP applications, and the question could benefit from more context and specificity.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       4   \n",
              "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ...   \n",
              "295                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "296                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     4   \n",
              "297                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "298                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NaN   \n",
              "299                                                                                                                                                                                                                                                                                                   4.0. The functional correctness approach to code generation is useful for ensuring correctness, especially in critical applications. However, match-based metrics may not guarantee correctness and may lead to false positives or false negatives.   \n",
              "\n",
              "    standalone_score  \n",
              "0                NaN  \n",
              "1                NaN  \n",
              "2                NaN  \n",
              "3                  5  \n",
              "4                NaN  \n",
              "..               ...  \n",
              "295              NaN  \n",
              "296                5  \n",
              "297              NaN  \n",
              "298              NaN  \n",
              "299                5  \n",
              "\n",
              "[300 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "generated_questions = pd.DataFrame.from_dict(outputs)\n",
        "\n",
        "print(\"Evaluation dataset before filtering:\")\n",
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "generated_questions['groundedness_score']=generated_questions['groundedness_score'].astype(str).str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
        "generated_questions['relevance_score']=generated_questions['relevance_score'].astype(str).str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
        "generated_questions['standalone_score']=generated_questions['groundedness_score'].astype(str).str.extract(r'(\\d+\\.?\\d*)').astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "generated_questions.to_csv(\"./data/generated_questions_raw.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>groundedness_score</th>\n",
              "      <th>relevance_score</th>\n",
              "      <th>standalone_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Can a large language model with parallel decoding be designed to think more like a human for answer quality?\\n\\n</td>\n",
              "      <td>Yes, the proposed Skeleton-of-Thought (SoT) method in the context is an attempt to optimize the inference efficiency of large language models (LLMs) for data-centric optimization, which can potentially improve the answer quality on several question categories. This is achieved by guiding the LLMs to generate the skeleton of the answer first and then conducting parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. The method is an initial attempt at data-centric optimization for inference efficiency and underscores the potential of pushing LLMs to think more like a human for answer quality.</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Does the dataset augmentation with samples from the model improve the performance of the scratchpad model, while it harms the direct execution model?\\n\\n</td>\n",
              "      <td>Yes, the dataset augmentation with samples from the model improves the performance of the scratchpad model, while it harms the direct execution model.</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In relation to the results presented in Table 3, what is the significance of the differences in word-level success rates between IO, CoT, Oracle, and ToT prompting methods, and how do these differences impact the performance of the models in solving crossword games?\\n\\n</td>\n",
              "      <td>The results in Table 3 show that IO and CoT prompting methods have a word-level success rate of less than 16%, while ToT significantly improves all metrics, achieving a word-level success rate of 60% and solving 4 out of 20 games. This indicates that ToT has mechanisms in place to try different clues, make changes to decisions, and backtrack, which are lacking in IO and CoT. Additionally, when outputting from the oracle best DFS state, ToT performance is even higher and actually solves 7/20 games, indicating that the simple output heuristics used can be improved. The importance of backtracking is also confirmed by an ablation that keeps filling the most promising clue for at most 20 steps, which performs poorly with a word-level success rate of only 20%.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Given the context, how does the Mini Crosswords algorithm propose and aggregate thoughts in a priority queue during a depth-first search (DFS) for solving crossword puzzles?\\n\\n</td>\n",
              "      <td>The Mini Crosswords algorithm proposes and aggregates thoughts in a priority queue during a depth-first search (DFS) for solving crossword puzzles by first generating all possible thoughts for each given clue. These thoughts are then evaluated based on the possibility of filling in each remaining word clue, and pruned if any remaining clue is deemed not possible to fill by the LM. The remaining thoughts are then sorted in a priority queue based on their evaluation scores, and the algorithm explores the next promising thought for clue in the priority queue. This process continues until the deepest explored state is rendered into the final output.</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How can dialogue models be improved to better align with human judgements?\\n\\n</td>\n",
              "      <td>Dialogue models can be improved to better align with human judgements by incorporating targeted human judgements into the training process. This can be done through methods such as active learning and human-in-the-loop approaches. Additionally, improving the quality of the training data and using more advanced techniques such as transfer learning and low-rank hypercomplex adapter layers can also help improve alignment.</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>What is the difference between traditional prompting and chain-of-thought prompting in language models?\\n\\n</td>\n",
              "      <td>Chain-of-thought prompting involves providing demonstrations of intermediate reasoning steps to language models, which allows them to generate a coherent series of steps leading to the final answer for a problem. This approach contrasts with traditional prompting, which does not provide such demonstrations and instead relies on a large training dataset and a single model checkpoint.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>What types of ice cream treats are available on the Ice Cream Parlor's menu?\\n</td>\n",
              "      <td>The Ice Cream Parlor's menu includes house-made ice cream sandwiches, kids' cones, Oldman's Sweet Ice Cream based Mini Six Peanut Butter Cups, and brand new Oreos Made from Oakwood.</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>Can the Chain-of-Verification (C OVE) method reduce hallucinations in large language models?\\n\\n</td>\n",
              "      <td>Yes, the Chain-of-Verification (C OVE) method has been shown to decrease hallucinations across a variety of tasks, including list-based questions from Wikidata, closed book MultiSpanQA, and longform text generation.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>In Graph of Thoughts, how does the ability to model information generated by an LLM as an arbitrary graph enable synergistic outcomes and enhance thoughts?\\n\\n</td>\n",
              "      <td>The ability to model information generated by an LLM as an arbitrary graph in Graph of Thoughts enables synergistic outcomes and enhances thoughts by allowing units of information, or LLM thoughts, to be combined in arbitrary ways. This enables the distillation of the essence of whole networks of thoughts and the use of feedback loops to improve the quality of the output. This approach is more flexible and powerful than other paradigms such as Chain-of-Thought or Tree of Thoughts (ToT), which limit the way in which LLM thoughts can be combined. By enabling more complex and nuanced combinations of LLM thoughts, Graph of Thoughts brings LLM reasoning closer to human thinking or brain mechanisms such as recurrence.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>In code generation, what is the advantage of using functional correctness over match-based metrics?\\n\\n</td>\n",
              "      <td>Functional correctness is advantageous in code generation because it accounts for the large and complex space of programs functionally equivalent to a reference solution, whereas match-based metrics are unable to do so. Additionally, functional correctness is used by human developers to judge code, making it a more convincing metric to evaluate code generation.</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>300 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                           question  \\\n",
              "0                                                                                                                                                                  Can a large language model with parallel decoding be designed to think more like a human for answer quality?\\n\\n   \n",
              "1                                                                                                                         Does the dataset augmentation with samples from the model improve the performance of the scratchpad model, while it harms the direct execution model?\\n\\n   \n",
              "2    In relation to the results presented in Table 3, what is the significance of the differences in word-level success rates between IO, CoT, Oracle, and ToT prompting methods, and how do these differences impact the performance of the models in solving crossword games?\\n\\n   \n",
              "3                                                                                                 Given the context, how does the Mini Crosswords algorithm propose and aggregate thoughts in a priority queue during a depth-first search (DFS) for solving crossword puzzles?\\n\\n   \n",
              "4                                                                                                                                                                                                    How can dialogue models be improved to better align with human judgements?\\n\\n   \n",
              "..                                                                                                                                                                                                                                                                              ...   \n",
              "295                                                                                                                                                                     What is the difference between traditional prompting and chain-of-thought prompting in language models?\\n\\n   \n",
              "296                                                                                                                                                                                                  What types of ice cream treats are available on the Ice Cream Parlor's menu?\\n   \n",
              "297                                                                                                                                                                                Can the Chain-of-Verification (C OVE) method reduce hallucinations in large language models?\\n\\n   \n",
              "298                                                                                                                 In Graph of Thoughts, how does the ability to model information generated by an LLM as an arbitrary graph enable synergistic outcomes and enhance thoughts?\\n\\n   \n",
              "299                                                                                                                                                                         In code generation, what is the advantage of using functional correctness over match-based metrics?\\n\\n   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           answer  \\\n",
              "0                                                                                                                              Yes, the proposed Skeleton-of-Thought (SoT) method in the context is an attempt to optimize the inference efficiency of large language models (LLMs) for data-centric optimization, which can potentially improve the answer quality on several question categories. This is achieved by guiding the LLMs to generate the skeleton of the answer first and then conducting parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. The method is an initial attempt at data-centric optimization for inference efficiency and underscores the potential of pushing LLMs to think more like a human for answer quality.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Yes, the dataset augmentation with samples from the model improves the performance of the scratchpad model, while it harms the direct execution model.   \n",
              "2    The results in Table 3 show that IO and CoT prompting methods have a word-level success rate of less than 16%, while ToT significantly improves all metrics, achieving a word-level success rate of 60% and solving 4 out of 20 games. This indicates that ToT has mechanisms in place to try different clues, make changes to decisions, and backtrack, which are lacking in IO and CoT. Additionally, when outputting from the oracle best DFS state, ToT performance is even higher and actually solves 7/20 games, indicating that the simple output heuristics used can be improved. The importance of backtracking is also confirmed by an ablation that keeps filling the most promising clue for at most 20 steps, which performs poorly with a word-level success rate of only 20%.   \n",
              "3                                                                                                                   The Mini Crosswords algorithm proposes and aggregates thoughts in a priority queue during a depth-first search (DFS) for solving crossword puzzles by first generating all possible thoughts for each given clue. These thoughts are then evaluated based on the possibility of filling in each remaining word clue, and pruned if any remaining clue is deemed not possible to fill by the LM. The remaining thoughts are then sorted in a priority queue based on their evaluation scores, and the algorithm explores the next promising thought for clue in the priority queue. This process continues until the deepest explored state is rendered into the final output.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                          Dialogue models can be improved to better align with human judgements by incorporating targeted human judgements into the training process. This can be done through methods such as active learning and human-in-the-loop approaches. Additionally, improving the quality of the training data and using more advanced techniques such as transfer learning and low-rank hypercomplex adapter layers can also help improve alignment.   \n",
              "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ...   \n",
              "295                                                                                                                                                                                                                                                                                                                                                                                             Chain-of-thought prompting involves providing demonstrations of intermediate reasoning steps to language models, which allows them to generate a coherent series of steps leading to the final answer for a problem. This approach contrasts with traditional prompting, which does not provide such demonstrations and instead relies on a large training dataset and a single model checkpoint.   \n",
              "296                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The Ice Cream Parlor's menu includes house-made ice cream sandwiches, kids' cones, Oldman's Sweet Ice Cream based Mini Six Peanut Butter Cups, and brand new Oreos Made from Oakwood.   \n",
              "297                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Yes, the Chain-of-Verification (C OVE) method has been shown to decrease hallucinations across a variety of tasks, including list-based questions from Wikidata, closed book MultiSpanQA, and longform text generation.   \n",
              "298                                             The ability to model information generated by an LLM as an arbitrary graph in Graph of Thoughts enables synergistic outcomes and enhances thoughts by allowing units of information, or LLM thoughts, to be combined in arbitrary ways. This enables the distillation of the essence of whole networks of thoughts and the use of feedback loops to improve the quality of the output. This approach is more flexible and powerful than other paradigms such as Chain-of-Thought or Tree of Thoughts (ToT), which limit the way in which LLM thoughts can be combined. By enabling more complex and nuanced combinations of LLM thoughts, Graph of Thoughts brings LLM reasoning closer to human thinking or brain mechanisms such as recurrence.   \n",
              "299                                                                                                                                                                                                                                                                                                                                                                                                                   Functional correctness is advantageous in code generation because it accounts for the large and complex space of programs functionally equivalent to a reference solution, whereas match-based metrics are unable to do so. Additionally, functional correctness is used by human developers to judge code, making it a more convincing metric to evaluate code generation.   \n",
              "\n",
              "     groundedness_score  relevance_score  standalone_score  \n",
              "0                   3.0              NaN               3.0  \n",
              "1                   3.0              NaN               3.0  \n",
              "2                   NaN              NaN               NaN  \n",
              "3                   4.5              4.0               4.5  \n",
              "4                   4.5              4.0               4.5  \n",
              "..                  ...              ...               ...  \n",
              "295                 NaN              NaN               NaN  \n",
              "296                 3.0              4.0               3.0  \n",
              "297                 NaN              NaN               NaN  \n",
              "298                 NaN              NaN               NaN  \n",
              "299                 4.5              4.0               4.5  \n",
              "\n",
              "[300 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "for col in [\"groundedness_score\", \"relevance_score\", \"standalone_score\"]:\n",
        "    generated_questions[col] = generated_questions[col].fillna(generated_questions[[\"groundedness_score\", \"relevance_score\", \"standalone_score\"]].min(axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "oBWuOu1b9jVO",
        "outputId": "b32bacea-52f8-486a-96fe-5c188605c5a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================\n",
            "Final evaluation dataset:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>groundedness_score</th>\n",
              "      <th>relevance_score</th>\n",
              "      <th>standalone_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Can a large language model with parallel decoding be designed to think more like a human for answer quality?\\n\\n</td>\n",
              "      <td>Yes, the proposed Skeleton-of-Thought (SoT) method in the context is an attempt to optimize the inference efficiency of large language models (LLMs) for data-centric optimization, which can potentially improve the answer quality on several question categories. This is achieved by guiding the LLMs to generate the skeleton of the answer first and then conducting parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. The method is an initial attempt at data-centric optimization for inference efficiency and underscores the potential of pushing LLMs to think more like a human for answer quality.</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Does the dataset augmentation with samples from the model improve the performance of the scratchpad model, while it harms the direct execution model?\\n\\n</td>\n",
              "      <td>Yes, the dataset augmentation with samples from the model improves the performance of the scratchpad model, while it harms the direct execution model.</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Given the context, how does the Mini Crosswords algorithm propose and aggregate thoughts in a priority queue during a depth-first search (DFS) for solving crossword puzzles?\\n\\n</td>\n",
              "      <td>The Mini Crosswords algorithm proposes and aggregates thoughts in a priority queue during a depth-first search (DFS) for solving crossword puzzles by first generating all possible thoughts for each given clue. These thoughts are then evaluated based on the possibility of filling in each remaining word clue, and pruned if any remaining clue is deemed not possible to fill by the LM. The remaining thoughts are then sorted in a priority queue based on their evaluation scores, and the algorithm explores the next promising thought for clue in the priority queue. This process continues until the deepest explored state is rendered into the final output.</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How can dialogue models be improved to better align with human judgements?\\n\\n</td>\n",
              "      <td>Dialogue models can be improved to better align with human judgements by incorporating targeted human judgements into the training process. This can be done through methods such as active learning and human-in-the-loop approaches. Additionally, improving the quality of the training data and using more advanced techniques such as transfer learning and low-rank hypercomplex adapter layers can also help improve alignment.</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>In the context of TeacherLM-7.1B, what are the characteristics and deficiencies of the generation content compared to human annotation and text-davinci-003?\\n\\n</td>\n",
              "      <td>TeacherLM-7.1B's explanations are generally more comprehensive and detailed than human annotations and text-davinci-003's explanations. However, it falls behind text-davinci-003 in solving mathematical problems, probably related to the model's size.</td>\n",
              "      <td>3.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>288</th>\n",
              "      <td>Can explicit articulation of the thought process in language elicit high-quality answers from LLMs?\\n\\n</td>\n",
              "      <td>Yes, recent research has shown that explicitly articulating the thought process in language can elicit high-quality answers from LLMs. Examples include CoT (Kojima et al., 2022; Wei et al., 2022), ToT (Yao et al., 2023), and ReAct (Yao et al., 2022). These findings resemble human thinking and suggest that LLMs can benefit from drawing from the human thinking process to facilitate more effective and efficient AI.</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>What is the difference between GPT-12B and Codex in terms of their performance on the HumanEval dataset?\\n\\n</td>\n",
              "      <td>Codex outperforms GPT-12B in generating standalone Python functions from docstrings on the HumanEval dataset. When a single sample is generated for each problem, Codex solves 28.8% of the problems, while GPT-12B solves no problems. However, when generating 100 samples per problem and selecting the sample with the highest mean log-probability, Codex solves 44.5% of the problems, while GPT-12B solves no problems. This suggests that Codex is better suited for generating code than GPT-12B.</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>Given a complex problem, how can we facilitate its solution by breaking it down into a series of simpler subproblems and solving them in sequence?\\n\\n</td>\n",
              "      <td>Least-to-most prompting is a novel prompting strategy that breaks down a complex problem into a series of simpler subproblems and solves them in sequence. The key idea in this strategy is to use the answers to previously solved subproblems to facilitate the solution of each subsequent subproblem. Experimental results have shown that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts, and can achieve high accuracy on tasks related to symbolic manipulation, compositional generalization, and math reasoning with just a small number of exemplars.</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>What types of ice cream treats are available on the Ice Cream Parlor's menu?\\n</td>\n",
              "      <td>The Ice Cream Parlor's menu includes house-made ice cream sandwiches, kids' cones, Oldman's Sweet Ice Cream based Mini Six Peanut Butter Cups, and brand new Oreos Made from Oakwood.</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>In code generation, what is the advantage of using functional correctness over match-based metrics?\\n\\n</td>\n",
              "      <td>Functional correctness is advantageous in code generation because it accounts for the large and complex space of programs functionally equivalent to a reference solution, whereas match-based metrics are unable to do so. Additionally, functional correctness is used by human developers to judge code, making it a more convincing metric to evaluate code generation.</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>181 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                              question  \\\n",
              "0                                                                     Can a large language model with parallel decoding be designed to think more like a human for answer quality?\\n\\n   \n",
              "1                            Does the dataset augmentation with samples from the model improve the performance of the scratchpad model, while it harms the direct execution model?\\n\\n   \n",
              "3    Given the context, how does the Mini Crosswords algorithm propose and aggregate thoughts in a priority queue during a depth-first search (DFS) for solving crossword puzzles?\\n\\n   \n",
              "4                                                                                                       How can dialogue models be improved to better align with human judgements?\\n\\n   \n",
              "5                     In the context of TeacherLM-7.1B, what are the characteristics and deficiencies of the generation content compared to human annotation and text-davinci-003?\\n\\n   \n",
              "..                                                                                                                                                                                 ...   \n",
              "288                                                                            Can explicit articulation of the thought process in language elicit high-quality answers from LLMs?\\n\\n   \n",
              "291                                                                       What is the difference between GPT-12B and Codex in terms of their performance on the HumanEval dataset?\\n\\n   \n",
              "293                             Given a complex problem, how can we facilitate its solution by breaking it down into a series of simpler subproblems and solving them in sequence?\\n\\n   \n",
              "296                                                                                                     What types of ice cream treats are available on the Ice Cream Parlor's menu?\\n   \n",
              "299                                                                            In code generation, what is the advantage of using functional correctness over match-based metrics?\\n\\n   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            answer  \\\n",
              "0               Yes, the proposed Skeleton-of-Thought (SoT) method in the context is an attempt to optimize the inference efficiency of large language models (LLMs) for data-centric optimization, which can potentially improve the answer quality on several question categories. This is achieved by guiding the LLMs to generate the skeleton of the answer first and then conducting parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. The method is an initial attempt at data-centric optimization for inference efficiency and underscores the potential of pushing LLMs to think more like a human for answer quality.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Yes, the dataset augmentation with samples from the model improves the performance of the scratchpad model, while it harms the direct execution model.   \n",
              "3    The Mini Crosswords algorithm proposes and aggregates thoughts in a priority queue during a depth-first search (DFS) for solving crossword puzzles by first generating all possible thoughts for each given clue. These thoughts are then evaluated based on the possibility of filling in each remaining word clue, and pruned if any remaining clue is deemed not possible to fill by the LM. The remaining thoughts are then sorted in a priority queue based on their evaluation scores, and the algorithm explores the next promising thought for clue in the priority queue. This process continues until the deepest explored state is rendered into the final output.   \n",
              "4                                                                                                                                                                                                                                           Dialogue models can be improved to better align with human judgements by incorporating targeted human judgements into the training process. This can be done through methods such as active learning and human-in-the-loop approaches. Additionally, improving the quality of the training data and using more advanced techniques such as transfer learning and low-rank hypercomplex adapter layers can also help improve alignment.   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                        TeacherLM-7.1B's explanations are generally more comprehensive and detailed than human annotations and text-davinci-003's explanations. However, it falls behind text-davinci-003 in solving mathematical problems, probably related to the model's size.   \n",
              "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ...   \n",
              "288                                                                                                                                                                                                                                                Yes, recent research has shown that explicitly articulating the thought process in language can elicit high-quality answers from LLMs. Examples include CoT (Kojima et al., 2022; Wei et al., 2022), ToT (Yao et al., 2023), and ReAct (Yao et al., 2022). These findings resemble human thinking and suggest that LLMs can benefit from drawing from the human thinking process to facilitate more effective and efficient AI.   \n",
              "291                                                                                                                                                                     Codex outperforms GPT-12B in generating standalone Python functions from docstrings on the HumanEval dataset. When a single sample is generated for each problem, Codex solves 28.8% of the problems, while GPT-12B solves no problems. However, when generating 100 samples per problem and selecting the sample with the highest mean log-probability, Codex solves 44.5% of the problems, while GPT-12B solves no problems. This suggests that Codex is better suited for generating code than GPT-12B.   \n",
              "293                                                  Least-to-most prompting is a novel prompting strategy that breaks down a complex problem into a series of simpler subproblems and solves them in sequence. The key idea in this strategy is to use the answers to previously solved subproblems to facilitate the solution of each subsequent subproblem. Experimental results have shown that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts, and can achieve high accuracy on tasks related to symbolic manipulation, compositional generalization, and math reasoning with just a small number of exemplars.   \n",
              "296                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The Ice Cream Parlor's menu includes house-made ice cream sandwiches, kids' cones, Oldman's Sweet Ice Cream based Mini Six Peanut Butter Cups, and brand new Oreos Made from Oakwood.   \n",
              "299                                                                                                                                                                                                                                                                                                    Functional correctness is advantageous in code generation because it accounts for the large and complex space of programs functionally equivalent to a reference solution, whereas match-based metrics are unable to do so. Additionally, functional correctness is used by human developers to judge code, making it a more convincing metric to evaluate code generation.   \n",
              "\n",
              "     groundedness_score  relevance_score  standalone_score  \n",
              "0                   3.0              3.0               3.0  \n",
              "1                   3.0              3.0               3.0  \n",
              "3                   4.5              4.0               4.5  \n",
              "4                   4.5              4.0               4.5  \n",
              "5                   3.5              4.0               3.5  \n",
              "..                  ...              ...               ...  \n",
              "288                 4.0              4.0               4.0  \n",
              "291                 5.0              4.0               5.0  \n",
              "293                 5.0              4.0               5.0  \n",
              "296                 3.0              4.0               3.0  \n",
              "299                 4.5              4.0               4.5  \n",
              "\n",
              "[181 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "generated_questions = generated_questions.loc[\n",
        "    (generated_questions[\"groundedness_score\"] >= 3.0)\n",
        "    & (generated_questions[\"relevance_score\"] >= 3.0)\n",
        "    & (generated_questions[\"standalone_score\"] >= 3.0)\n",
        "]\n",
        "print(\"============================================\")\n",
        "print(\"Final evaluation dataset:\")\n",
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "\n",
        "# eval_dataset = datasets.Dataset.from_pandas(\n",
        "#     generated_questions, split=\"train\", preserve_index=False\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "generated_questions.to_csv(\"./data/generated_questions_filtered.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "#generated_questions.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaOMZyu69jVO"
      },
      "source": [
        "- Go through the 181 rows remaining post dropping missing vals and missing value imputation visually, keep the better 120ish questions\n",
        "    - Dropped questions that were off-target for learning about LLMs, relied on the reference section, or mentioned the context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_dataset = pd.read_csv(\"../data/generated_questions_filtered_w_human.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(121, 10)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Q3RRz4W79jVO"
      },
      "outputs": [],
      "source": [
        "#eval_dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5s19uTd9jVO"
      },
      "source": [
        "# 2. Build our RAG System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-mET8Dy9jVO"
      },
      "source": [
        "### 2.1. Preprocessing documents to build our vector database\n",
        "\n",
        "- In this part, __we split the documents from our knowledge base into smaller chunks__: these will be the snippets that are picked by the Retriever, to then be ingested by the Reader LLM as supporting elements for its answer.\n",
        "- The goal is to build semantically relevant snippets: not too small to be sufficient for supporting an answer, and not too large too avoid diluting individual ideas.\n",
        "\n",
        "Many options exist for text splitting:\n",
        "- split every `n` words / characters, but this has the risk of cutting in half paragraphs or even sentences\n",
        "- split after `n` words / character, but only on sentence boundaries\n",
        "- **recursive split** tries to preserve even more of the document structure, by processing it tree-like way, splitting first on the largest units (chapters) then recursively splitting on smaller units (paragraphs, sentences).\n",
        "\n",
        "To learn more about chunking, I recommend you read [this great notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb) by Greg Kamradt.\n",
        "\n",
        "[This space](https://huggingface.co/spaces/m-ric/chunk_visualizer) lets you visualize how different splitting options affect the chunks you get.\n",
        "\n",
        "> In the following, we use Langchain's `RecursiveCharacterTextSplitter`.\n",
        "\n",
        "ğŸ’¡ _To measure chunk length in our Text Splitter, our length function will not be the count of characters, but the count of tokens in the tokenized text: indeed, for subsequent embedder that processes token, measuring length in tokens is more relevant and empirically performs better._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "H4fhm55Q9jVO"
      },
      "outputs": [],
      "source": [
        "# from langchain.docstore.document import Document as LangchainDocument\n",
        "\n",
        "# RAW_KNOWLEDGE_BASE = [\n",
        "#     LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
        "#     for doc in tqdm(eval_dataset)\n",
        "# ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "sz9Jw2_q9jVO"
      },
      "outputs": [],
      "source": [
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "# def split_documents(\n",
        "#     chunk_size: int,\n",
        "#     knowledge_base: List[LangchainDocument],\n",
        "#     tokenizer_name: str,\n",
        "# ) -> List[LangchainDocument]:\n",
        "#     \"\"\"\n",
        "#     Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
        "#     \"\"\"\n",
        "#     text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "#         AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "#         chunk_size=chunk_size,\n",
        "#         chunk_overlap=int(chunk_size / 10),\n",
        "#         add_start_index=True,\n",
        "#         strip_whitespace=True,\n",
        "#         separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        "#     )\n",
        "\n",
        "#     docs_processed = []\n",
        "#     for doc in knowledge_base:\n",
        "#         docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "#     # Remove duplicates\n",
        "#     unique_texts = {}\n",
        "#     docs_processed_unique = []\n",
        "#     for doc in docs_processed:\n",
        "#         if doc.page_content not in unique_texts:\n",
        "#             unique_texts[doc.page_content] = True\n",
        "#             docs_processed_unique.append(doc)\n",
        "\n",
        "#     return docs_processed_unique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzBYfNG79jVO"
      },
      "source": [
        "### 2.2. Retriever - embeddings ğŸ—‚ï¸\n",
        "The __retriever acts like an internal search engine__: given the user query, it returns the most relevant documents from your knowledge base.\n",
        "\n",
        "> For the knowledge base, we use Langchain vector databases since __it offers a convenient [FAISS](https://github.com/facebookresearch/faiss) index and allows us to keep document metadata throughout the processing__.\n",
        "\n",
        "ğŸ› ï¸ __Options included:__\n",
        "\n",
        "- Tune the chunking method:\n",
        "    - Size of the chunks\n",
        "    - Method: split on different separators, use [semantic chunking](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker)...\n",
        "- Change the embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "LqJlIDZR9jVO"
      },
      "outputs": [],
      "source": [
        "# from langchain.vectorstores import FAISS\n",
        "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "# from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "# import os\n",
        "\n",
        "\n",
        "# def load_embeddings(\n",
        "#     langchain_docs: List[LangchainDocument],\n",
        "#     chunk_size: int,\n",
        "#     embedding_model_name: Optional[str] = \"thenlper/gte-small\",\n",
        "# ) -> FAISS:\n",
        "#     \"\"\"\n",
        "#     Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
        "\n",
        "#     Args:\n",
        "#         langchain_docs: list of documents\n",
        "#         chunk_size: size of the chunks to split the documents into\n",
        "#         embedding_model_name: name of the embedding model to use\n",
        "\n",
        "#     Returns:\n",
        "#         FAISS index\n",
        "#     \"\"\"\n",
        "#     # load embedding_model\n",
        "#     embedding_model = HuggingFaceEmbeddings(\n",
        "#         model_name=embedding_model_name,\n",
        "#         multi_process=True,\n",
        "#         model_kwargs={\"device\": \"cuda\"},\n",
        "#         encode_kwargs={\n",
        "#             \"normalize_embeddings\": True\n",
        "#         },  # set True to compute cosine similarity\n",
        "#     )\n",
        "\n",
        "#     # Check if embeddings already exist on disk\n",
        "#     index_name = (\n",
        "#         f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
        "#     )\n",
        "#     index_folder_path = f\"./data/indexes/{index_name}/\"\n",
        "#     if os.path.isdir(index_folder_path):\n",
        "#         return FAISS.load_local(\n",
        "#             index_folder_path,\n",
        "#             embedding_model,\n",
        "#             distance_strategy=DistanceStrategy.COSINE,\n",
        "#         )\n",
        "\n",
        "#     else:\n",
        "#         print(\"Index not found, generating it...\")\n",
        "#         docs_processed = split_documents(\n",
        "#             chunk_size,\n",
        "#             langchain_docs,\n",
        "#             embedding_model_name,\n",
        "#         )\n",
        "#         knowledge_index = FAISS.from_documents(\n",
        "#             docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
        "#         )\n",
        "#         knowledge_index.save_local(index_folder_path)\n",
        "#         return knowledge_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6y1mQJX9jVO"
      },
      "source": [
        "### 2.3. Reader - LLM ğŸ’¬\n",
        "\n",
        "In this part, the __LLM Reader reads the retrieved documents to formulate its answer.__\n",
        "\n",
        "ğŸ› ï¸ Here we tried the following options to improve results:\n",
        "- Switch reranking on/off\n",
        "- Change the reader model\n",
        "\n",
        "TODO: Already have Mixtral, use it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings import CacheBackedEmbeddings, HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.storage import LocalFileStore\n",
        "store = LocalFileStore(\"./cache/\")\n",
        "\n",
        "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "core_embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=embed_model_id\n",
        ")\n",
        "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    core_embeddings_model, store, namespace=embed_model_id\n",
        ")\n",
        "\n",
        "vector_store = FAISS.load_local('../rag_index_dir', embedder,allow_dangerous_deserialization=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9PdpuWyP9jVP"
      },
      "outputs": [],
      "source": [
        "RAG_PROMPT_TEMPLATE = \"\"\"\n",
        "<|system|>\n",
        "Using the information contained in the context,\n",
        "give a comprehensive answer to the question.\n",
        "Respond only to the question asked, response should be concise and relevant to the question.\n",
        "Provide the number of the source document when relevant.\n",
        "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
        "<|user|>\n",
        "Context:\n",
        "{context}\n",
        "---\n",
        "Now here is the question you need to answer.\n",
        "\n",
        "Question: {question}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n"
          ]
        }
      ],
      "source": [
        "from exllamav2 import *\n",
        "from exllamav2.generator import *\n",
        "import sys, torch\n",
        "\n",
        "\n",
        "reader_config = ExLlamaV2Config()\n",
        "reader_config.model_dir = \"/home/mainuser/Desktop/LLMs/ZephyrInference\"\n",
        "#reader_config.model_dir = '/home/mainuser/Desktop/LLMs/Mixtral4bit'\n",
        "reader_config.prepare()\n",
        "\n",
        "reader_model = ExLlamaV2(reader_config)\n",
        "cache = ExLlamaV2Cache(reader_model, lazy = True)\n",
        "\n",
        "print(\"Loading model...\")\n",
        "reader_model.load_autosplit(cache)\n",
        "\n",
        "reader_tokenizer = ExLlamaV2Tokenizer(reader_config)\n",
        "reader_llm = ExLlamaV2StreamingGenerator(reader_model, cache, reader_tokenizer)\n",
        "#reader_llm.set_stop_conditions([reader_tokenizer.eos_token_id])\n",
        "reader_settings = ExLlamaV2Sampler.Settings()\n",
        "reader_settings.temperature = 0.85\n",
        "reader_settings.top_k = 30\n",
        "reader_settings.top_p = 0.8\n",
        "reader_settings.token_repetition_penalty = 1.03"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "#os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'xxx' # added to .bashrc, should be good on next restart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Retrieving documents...\n"
          ]
        }
      ],
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "from typing import Optional, List, Tuple\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    generator: ExLlamaV2StreamingGenerator,\n",
        "   # tokenizer: ExLlamaV2Tokenizer,\n",
        "    settings:ExLlamaV2Sampler.Settings,\n",
        "    max_new_tokens = 512,\n",
        "    knowledge_index: FAISS = vector_store,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 10, #30,\n",
        "    #num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    # Gather documents with retriever\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    embedding_vector = core_embeddings_model.embed_query(question)\n",
        "    relevant_docs = knowledge_index.similarity_search_by_vector(embedding_vector, k = num_retrieved_docs)#num_retrieved_docs)\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
        "\n",
        "    # Optionally rerank results\n",
        "    # if reranker:\n",
        "    #     print(\"=> Reranking documents...\")\n",
        "    #     relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "    #     #print(f\"Type is : {type(relevant_docs[0])}\")\n",
        "    #     print(dir(relevant_docs[0]))\n",
        "    #     relevant_docs = [doc['page_content'] for doc in relevant_docs]\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_retrieved_docs]\n",
        "\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
        "\n",
        "   \n",
        "    generator.warmup()\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    answer = generator.generate_simple(final_prompt, \n",
        "    settings, max_new_tokens, seed = 1234)\n",
        "    # print(answer)\n",
        "    return answer,relevant_docs\n",
        "\n",
        "\n",
        "answer, relevant_docs = answer_with_rag(question=\"What is the difference between RAG and self-RAG?\", generator=reader_llm,settings=reader_settings,max_new_tokens=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n<|system|>\\nUsing the information contained in the context,\\ngive a comprehensive answer to the question.\\nRespond only to the question asked, response should be concise and relevant to the question.\\nProvide the number of the source document when relevant.\\nIf the answer cannot be deduced from the context, do not give an answer.</s>\\n<|user|>\\nContext:\\n\\nExtracted documents:\\nDocument 0:::\\nRetrieval Threshold0.60.81.0AccuracyPopQA0.00.51.0\\nFrequency\\n0.250.500.751.00\\nFrequency\\n (c) Retrieval\\nFigure 3: Analysis on SELF-RAG:(a)Ablation studies for key components of SELF-RAGtraining\\nand inference based on our 7B model. (b) Effects of soft weights on ASQA citation precision and\\nMauve (fluency). (c) Retrieval frequency andnormalized accuracy on PubHealth and PopQA.\\nprecisely grounded yet shorter outputs. Llama2-FT 7B, which is the baseline LM trained on the same\\ninstruction-output pairs as SELF-RAGwithout retrieval or self-reflection and is retrieval-augmented\\nat test time only, lags behind S ELF-RAG. This result indicates S ELF-RAGgains are not solely from\\ntraining data and demonstrate the effectiveness of S ELF-RAGframework.\\n5.2 A NALYSIS\\nAblation studies. We conduct a set of ablations of our framework to identify which factors play\\nkey roles. We evaluate two model variants trained differently than our model: No Retriever trains an\\nLM using the standard instruction-following method given instruction-output pairs, without retrieved\\npassages; No Critic trains an LM trained with input-output pairs that are always augmented with the\\ntop one retrieved document without reflection tokens. This is similar to SAIL (Luo et al., 2023), and\\nwe use our instruction-output data instead of using the Alpaca dataset (Dubois et al., 2023), as in\\nSAIL. We also conduct ablation on our inference-time algorithm, including No retrieval disables\\nretrieval during inference; Hard constraints indicates the model performance that retrieves when\\nRetrieve =Yes instead of using the adaptive threshold; Retrieve top 1 always retrieves and uses the\\ntop one document only, similar to standard RAG approaches; Remove ISSUPindicates the model\\nperformance that removes ISSUPscore only during critique-guided beam search in Eq. 4. In this\\nablation experiment, we use a training instance size of 50k for a more efficient exploration of trainingDocument 1:::\\noutput is supported by the cited evidence). Human annotators find SELF-RAGanswers are often\\nplausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is\\nconsistent with Menick et al. (2022). Human annotators also find ISRELand ISSUPreflection token\\npredictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated\\nexamples and explanations on assessments.\\n6 C ONCLUSION\\nThis work introduces SELF-RAG, a new framework to enhance the quality and factuality of LLMs\\nthrough retrieval on demand and self-reflection. SELF-RAGtrains an LM to learn to retrieve, generate,\\nand critique text passages and its own generation by predicting the next tokens from its original\\nvocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables\\nthe tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on\\nsix tasks using multiple metrics demonstrate that SELF-RAGsignificantly outperforms LLMs with\\nmore parameters or with conventional retrieval-augmented generation approaches.\\n10Preprint.\\nETHICAL CONCERNS\\nThis work aims to improve the factuality of LLM outputs, the lack of which continues to cause nu-\\nmerous real-world problems (e.g., spread of misinformation and provision of incorrect and dangerous\\nadvice). While our method shows significant improvements in terms of performance, factuality, and\\ncitation accuracy, it can still generate outputs that are not fully supported by the citations. We hope\\nthat explicit self-reflection and fine-grained attribution may help users verify factual errors in the\\nmodel outputs.\\nACKNOWLEDGMENTS\\nWe thank Sewon Min, Scott Wen-tau Yih, Sean Welleck, and Kawin Ethayarajh for fruitful discussions\\nin the early stages of this work. We thank Sewon Min, Joongwon (Daniel) Kim, and Sandy Kaplan\\nfor valuable feedback on the paper, and Tianyu Gao and Weijia Shi for their help on evaluations.Document 2:::\\nSELF-RAGis a framework that enhances the quality and factuality of an LLM through retrieval and\\nself-reflection, without sacrificing LLMâ€™s original creativity and versatility. Our end-to-end training\\nlets an LM Mgenerate text informed by retrieved passages, if needed, and criticize the output by\\nlearning to generate special tokens. These reflection tokens (Table 1) signal the need for retrieval\\nor confirm the outputâ€™s relevance, support, or completeness. In contrast, common RAG approaches\\nretrieve passages indiscriminately, without ensuring complete support from cited sources.\\n3.1 P ROBLEM FORMALIZATION AND OVERVIEW\\nFormally, given input x, we train Mto sequentially generate textual outputs yconsisting of multiple\\nsegments y= [y1, . . . , y T], where ytindicates a sequence of tokens for the t-th segment.3Generated\\ntokens in ytinclude text from the original vocabulary as well as the reflection tokens (Table 1).\\n2All work is arXived within a week of this preprint.\\n3In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any\\nsegment unit (i.e., sub-sentence).\\n3Preprint.\\nType Input Output Definitions\\nRetrieve x/x, y {yes, no, continue } Decides when to retrieve with R\\nISREL x, d {relevant , irrelevant } dprovides useful information to solve x.\\nISSUP x, d, y {fully supported , partially\\nsupported, no support }All of the verification-worthy statement in y\\nis supported by d.\\nISUSE x, y {5, 4, 3, 2, 1 } yis a useful response to x.\\nTable 1: Four types of reflection tokens used in SELF-RAG. Each type uses several tokens to represent\\nits output values. The bottom three rows are three types of Critique tokens, and the bold text indicates\\nthe most desirable critique tokens. x, y, d indicate input, output, and a relevant passage, respectively.\\nAlgorithm 1 SELF-RAGInference\\nRequire: Generator LM M, Retriever R, Large-scale passage collections {d1, . . . , d N}Document 3:::\\ncalls a retriever model on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple\\nretrieved passages, evaluating their relevance and then generating corresponding task outputs (Step\\n2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms\\nof factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which\\n1Our code and trained models are available at https://selfrag.github.io/ .\\n1arXiv:2310.11511v1  [cs.CL]  17 Oct 2023Preprint.\\nStep 1: Retrieve K documentsCalifornia was named after a ï¬ctional island in a Spanish book. Prompt How did US states get their names? \\nUS states got their names from a variety of sources. Eleven states are named after an individual person (e.g, California was named after Christopher Columbus). Some states including Texas and Utah, are named after Native American tribe.\\nRetrieval-Augmented Generation (RAG)Ours: Self-reï¬‚ective Retrieval-Augmented Generation (Self-RAG) \\nPopular names by states. In Texas, Emma is a popular baby name. Of the ï¬fty states, eleven are named after an individual person. \\nPrompt How did US states get their names? + Step 2: Prompt LM with K docs and generateRetriever\\nLM\\nPrompt How did US states get their names? US states got their names from a variety of sources. RetrieveStep 1: Retrieve on demand  \\nPrompt +  \\n11 of 50 state namesRelevant\\nStep 2: Generate segment in parallel \\ncome from persons.SupportedIrrelevantTexas is namedafter a Native American tribe. Step 3: Critique outputs and select best segmentorigins in a 16th-century novel Las Sergas de EsplandiÃ¡n. California\\'s name has itsRelevantPartially\\nUS states got their names from a variety of sources. 11 of 50 states names are come from persons.    26 states are named after Native Americans, including Utah. \\nPrompt: Write an essay of your best summer vacation\\nPrompt: Write an essay of your best summer vacationDocument 4:::\\nwork for this task (and many others) when compiled appropriately.\\nOur baseline RAG program is the one given in Section 3.2 as a simple example of RAG with a\\ndspy.ChainOfThought layer. We will see that this program does not excel at HotPotQA, and this\\nmotivates us to evaluate two multi-hop programs.\\nTo that end, we first test ReAct (Yao et al., 2022), a multi-step agent for tool use, which is imple-\\nmented as a built-in module in DSPy. In the simplest case, a ReAct module for a particular signature\\ncan be declared as follows in DSPy:\\n1react = dspy.ReAct(\"question -> answer\", tools=[dspy.Retrieve(k=1)], max_iters=5)\\nWe also test the following custom program, which simulates the information flow in Baleen (Khattab\\net al., 2021a) and IRRR (Qi et al., 2020) and has similarities to IRCoT (Trivedi et al., 2022).\\n1class BasicMultiHop(dspy.Module):\\n2def __init__(self, passages_per_hop):\\n3 self.retrieve = dspy.Retrieve(k=passages_per_hop)\\n4 self.generate_query = dspy.ChainOfThought(\"context, question -> search_query\")\\n5 self.generate_answer = dspy.ChainOfThought(\"context, question -> answer\")\\n6\\n7def forward(self, question):\\n8 context = []\\n9\\n10 for hop in range(2):\\n11 query = self.generate_query(context=context, question=question).search_query\\n12 context += self.retrieve(query).passages\\n13\\n14 return self.generate_answer(context=context, question=question)\\n15\\n16multihop = BasicMultiHop(passages_per_hop=3)\\nCompiling For compilers, we continue to use the ones that we used for GSM8K (see Sec 6). We\\nalso consider two compositions of our teleprompters. For ReAct, we consider bootstrapping with\\nBootstrapFewShotWithRandomSearch starting from an earlier bootstrap of the ReAct program.\\nFor the simple multihop program, we also consider fine-tuning with T5-Large starting from the\\nearlier bootstrap of that program.\\n1multihop_t5 = dspy.BootstrapFinetune(metric=answer_exact_match).compile(program,\\nteacher=bootstrap, trainset=trainset, target=â€™t5-largeâ€™)Document 5:::\\nPreprint.\\nSELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND\\nCRITIQUE THROUGH SELF-REFLECTION\\nAkari Asaiâ€ , Zeqiu Wuâ€ , Yizhong Wangâ€ Â§, Avirup Silâ€¡, Hannaneh Hajishirziâ€ Â§\\nâ€ University of WashingtonÂ§Allen Institute for AIâ€¡IBM Research AI\\n{akari,zeqiuwu,yizhongw,hannaneh }@cs.washington.edu ,avi@us.ibm.com\\nABSTRACT\\nDespite their remarkable capabilities, large language models (LLMs) often produce\\nresponses containing factual inaccuracies due to their sole reliance on the paramet-\\nric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad\\nhoc approach that augments LMs with retrieval of relevant knowledge, decreases\\nsuch issues. However, indiscriminately retrieving and incorporating a fixed number\\nof retrieved passages, regardless of whether retrieval is necessary, or passages are\\nrelevant, diminishes LM versatility or can lead to unhelpful response generation.\\nWe introduce a new framework called Self-Reflective Retrieval-Augmented Gen-\\neration ( SELF-RAG)that enhances an LMâ€™s quality and factuality through retrieval\\nand self-reflection. Our framework trains a single arbitrary LM that adaptively\\nretrieves passages on-demand, and generates and reflects on retrieved passages\\nand its own generations using special tokens, called reflection tokens. Generating\\nreflection tokens makes the LM controllable during the inference phase, enabling it\\nto tailor its behavior to diverse task requirements. Experiments show that SELF-\\nRAG(7B and 13B parameters) significantly outperforms state-of-the-art LLMs\\nand retrieval-augmented models on a diverse set of tasks. Specifically, SELF-RAG\\noutperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\\nreasoning and fact verification tasks, and it shows significant gains in improving\\nfactuality and citation accuracy for long-form generations relative to these models.1\\n1 I NTRODUCTION\\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)Document 6:::\\nEffects of training data size. We conduct an analysis of how the data scale affects the modelâ€™s\\nperformance. In particular, we randomly sample 5k, 10k, 20k, and 50k instances from our original\\n150k training instances, and fine-tune four SELF-RAG 7Bvariants on those subsets. Then, we compare\\nthe model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF-\\nRAGtrained on the full 150k instances. We also evaluate Figures 4a, 4b and 4c shows the modelsâ€™\\nperformance trained on different amount of data. Across all datasets, increasing data size often shows\\nupward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do\\nnot observed such significant improvements on Llama2-FT 7Bwhen increasing the training data from\\n50k to 150k. These results also indicate that further expanding the training data of SELF-RAGmay\\nlead to further improvements, although in this work we limit our training data size to 150k.\\nHuman evaluations. We conduct small human evaluations on SELF-RAGoutputs, as well as the\\nreliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio\\nresults. Following Menick et al. (2022), human annotators evaluate S&P , which indicates whether\\nthe model output is plausible (i.e., the output is a reasonable and on-topic response to the question\\nas if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to\\nverify the validity of the answer). For S&P, we do not consider the instances where SELF-RAG\\npredicts irrelevant orno support . We then ask our annotators whether the model-predicted\\nreflection tokens about ISRELand ISSUPmatch their inspections (e.g., whether the fully supported\\noutput is supported by the cited evidence). Human annotators find SELF-RAGanswers are often\\nplausible and supported by relevant passages with higher S&P scores on short-form PopQA, which isDocument 7:::\\nAlgorithm 1 SELF-RAGInference\\nRequire: Generator LM M, Retriever R, Large-scale passage collections {d1, . . . , d N}\\n1:Input: input prompt xand preceding generation y<t,Output: next output segment yt\\n2:Mpredicts Retrieve given (x, y<t)\\n3:ifRetrieve ==Yes then\\n4: Retrieve relevant text passages DusingRgiven (x, ytâˆ’1) â–·Retrieve\\n5: Mpredicts ISRELgiven x, dandytgiven x, d, y <tfor each dâˆˆD â–·Generate\\n6: Mpredicts ISSUPand ISUSEgiven x, yt, dfor each dâˆˆD â–·Critique\\n7: Rank ytbased on ISREL,ISSUP,ISUSE â–·Detailed in Section 3.3\\n8:else if Retrieve ==Nothen\\n9: Mgenpredicts ytgiven x â–· Generate\\n10: Mgenpredicts ISUSEgiven x, yt â–·Critique\\nInference overview. Figure 1 and Algorithm 1 present an overview of S ELF-RAGat inference. For\\nevery xand preceding generation y<t, the model decodes a retrieval token to evaluate the utility\\nof retrieval. If retrieval is not required, the model predicts the next output segment, as it does in a\\nstandard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved\\npassageâ€™s relevance, the next response segment, and a critique token to evaluate if the information in\\nthe response segment is supported by the passage. Finally, a new critique token evaluates the overall\\nutility of the response.4To generate each segment, SELF-RAGprocesses multiple passages in parallel\\nand uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control\\n(Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages\\nd1is selected at the first time step since d2does not provide direct evidence ( ISRELis Irrelevant)\\nandd3output is only partially supported while d1are fully supported.\\nTraining overview. SELF-RAGenables an arbitrary LM to generate text with reflection tokens\\nby unifying them as next token predictions from the expanded model vocabulary (i.e., the originalDocument 8:::\\n1 I NTRODUCTION\\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\\ndespite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\\n(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\\nwith relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\\n2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\\nunnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\\nretrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\\nthe output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\\nthe models are not explicitly trained to leverage and follow facts from provided passages. This\\nwork introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an\\nLLMâ€™s generation quality, including its factual accuracy without hurting its versatility, via on-demand\\nretrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\\nits own generation process given a task input by generating both task output and intermittent special\\ntokens (i.e., reflection tokens ). Reflection tokens are categorized into retrieval andcritique tokens to\\nindicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\\ngiven an input prompt and preceding generations, SELF-RAGfirst determines if augmenting the\\ncontinued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that\\ncalls a retriever model on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple\\nretrieved passages, evaluating their relevance and then generating corresponding task outputs (StepDocument 9:::\\nPrompt: Write an essay of your best summer vacation\\nPrompt: Write an essay of your best summer vacation\\nNo RetrievalMy best summer vacation is when my family and I embarked on a road trip along â€¦My bestâ€¦ \\n>Repeat.â€¦\\nNo information in passagesContradictory>Prompt +  \\nPrompt +  \\nRetrieve\\nFigure 1: Overview of SELF-RAG.SELF-RAGlearns to retrieve, critique, and generate text passages\\nto enhance overall generation quality, factuality, and verifiability.\\nconsistently retrieves a fixed number of documents for generation regardless of the retrieval necessity\\n(e.g., the bottom figure example does not require factual knowledge) and never second visits the\\ngeneration quality. Moreover, SELF-RAGprovides citations for each segment with its self-assessment\\nof whether the output is supported by the passage, leading to easier fact verification.\\nSELF-RAGtrains an arbitrary LM to generate text with reflection tokens by unifying them as the\\nnext token prediction from the expanded model vocabulary. We train our generator LM on a diverse\\ncollection of text interleaved with reflection tokens and retrieved passages. Reflection tokens, inspired\\nby reward models used in reinforcement learning (Ziegler et al., 2019; Ouyang et al., 2022), are\\ninserted offline into the original corpus by a trained critic model. This eliminates the need to host a\\ncritic model during training, reducing overhead. The critic model, in part, is supervised on a dataset\\nof input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e.,\\nGPT-4; OpenAI 2023). While we draw inspiration from studies that use control tokens to start and\\nguide text generation (Lu et al., 2022; Keskar et al., 2019), our trained LM uses critique tokens to\\nassess its own predictions after each generated segment as an integral part of the generation output.\\nSELF-RAGfurther enables a customizable decoding algorithm to satisfy hard or soft constraints,\\n---\\nNow here is the question you need to answer.\\n\\nQuestion: What is the difference between RAG and self-RAG?\\n</s>\\n<|assistant|>\\nRAG (Retrieval-Augmented Generation) is an ad hoc approach that augments large language models (LLMs) with the retrieval of relevant knowledge to decrease factual errors. However, RAG indiscriminately retrieves and incorporates a fixed number of retrieved passages, regardless of whether retrieval is necessary or not, which diminishes LLM versatility or can lead to unhelpful response generation. On the other hand, Self-Reflective Retrieval-Augmented Generation (SELF-RAG) enhances an LLM\\'s quality and factuality through retrieval and self-reflection. SELF-RAG trains a single arbitrary LLM that adaptively retrieves passages on-demand and generates and reflects on retrieved passages and its own generations using special tokens called reflection tokens. Generating reflection tokens makes the LLM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. SELF-RAG shows significant gains in improving factuality and citation accuracy for long-form generations relative to RAG and LLMs with more parameters or with conventional retrieval-augmented generation approaches.'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- OK, Zephyr seems to work well, under 4s/question with exl2.  Will try to setup reranker, then onto generating questions and relevant docs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Retrieving documents...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 25.24it/s]\n"
          ]
        }
      ],
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "from typing import Optional, List, Tuple\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    generator: ExLlamaV2StreamingGenerator,\n",
        "   # tokenizer: ExLlamaV2Tokenizer,\n",
        "    settings:ExLlamaV2Sampler.Settings,\n",
        "    max_new_tokens = 512,\n",
        "    knowledge_index: FAISS = vector_store,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 10, #30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    # Gather documents with retriever\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    embedding_vector = core_embeddings_model.embed_query(question)\n",
        "    relevant_docs = knowledge_index.similarity_search_by_vector(embedding_vector, k = num_retrieved_docs)#num_retrieved_docs)\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
        "\n",
        "\n",
        "    if reranker:\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_retrieved_docs]\n",
        "\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
        "\n",
        "   \n",
        "    generator.warmup()\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    answer = generator.generate_simple(final_prompt, \n",
        "    settings, max_new_tokens, seed = 1234)\n",
        "    return answer,relevant_docs\n",
        "\n",
        "\n",
        "answer, relevant_docs = answer_with_rag(question=\"What is the difference between RAG and self-RAG?\", generator=reader_llm,settings=reader_settings,max_new_tokens=512,reranker = RERANKER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n<|system|>\\nUsing the information contained in the context,\\ngive a comprehensive answer to the question.\\nRespond only to the question asked, response should be concise and relevant to the question.\\nProvide the number of the source document when relevant.\\nIf the answer cannot be deduced from the context, do not give an answer.</s>\\n<|user|>\\nContext:\\n\\nExtracted documents:\\nDocument 0:::\\nSELF-RAGis a framework that enhances the quality and factuality of an LLM through retrieval and\\nself-reflection, without sacrificing LLMâ€™s original creativity and versatility. Our end-to-end training\\nlets an LM Mgenerate text informed by retrieved passages, if needed, and criticize the output by\\nlearning to generate special tokens. These reflection tokens (Table 1) signal the need for retrieval\\nor confirm the outputâ€™s relevance, support, or completeness. In contrast, common RAG approaches\\nretrieve passages indiscriminately, without ensuring complete support from cited sources.\\n3.1 P ROBLEM FORMALIZATION AND OVERVIEW\\nFormally, given input x, we train Mto sequentially generate textual outputs yconsisting of multiple\\nsegments y= [y1, . . . , y T], where ytindicates a sequence of tokens for the t-th segment.3Generated\\ntokens in ytinclude text from the original vocabulary as well as the reflection tokens (Table 1).\\n2All work is arXived within a week of this preprint.\\n3In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any\\nsegment unit (i.e., sub-sentence).\\n3Preprint.\\nType Input Output Definitions\\nRetrieve x/x, y {yes, no, continue } Decides when to retrieve with R\\nISREL x, d {relevant , irrelevant } dprovides useful information to solve x.\\nISSUP x, d, y {fully supported , partially\\nsupported, no support }All of the verification-worthy statement in y\\nis supported by d.\\nISUSE x, y {5, 4, 3, 2, 1 } yis a useful response to x.\\nTable 1: Four types of reflection tokens used in SELF-RAG. Each type uses several tokens to represent\\nits output values. The bottom three rows are three types of Critique tokens, and the bold text indicates\\nthe most desirable critique tokens. x, y, d indicate input, output, and a relevant passage, respectively.\\nAlgorithm 1 SELF-RAGInference\\nRequire: Generator LM M, Retriever R, Large-scale passage collections {d1, . . . , d N}Document 1:::\\nPreprint.\\nSELF-RAG: LEARNING TO RETRIEVE , GENERATE ,AND\\nCRITIQUE THROUGH SELF-REFLECTION\\nAkari Asaiâ€ , Zeqiu Wuâ€ , Yizhong Wangâ€ Â§, Avirup Silâ€¡, Hannaneh Hajishirziâ€ Â§\\nâ€ University of WashingtonÂ§Allen Institute for AIâ€¡IBM Research AI\\n{akari,zeqiuwu,yizhongw,hannaneh }@cs.washington.edu ,avi@us.ibm.com\\nABSTRACT\\nDespite their remarkable capabilities, large language models (LLMs) often produce\\nresponses containing factual inaccuracies due to their sole reliance on the paramet-\\nric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad\\nhoc approach that augments LMs with retrieval of relevant knowledge, decreases\\nsuch issues. However, indiscriminately retrieving and incorporating a fixed number\\nof retrieved passages, regardless of whether retrieval is necessary, or passages are\\nrelevant, diminishes LM versatility or can lead to unhelpful response generation.\\nWe introduce a new framework called Self-Reflective Retrieval-Augmented Gen-\\neration ( SELF-RAG)that enhances an LMâ€™s quality and factuality through retrieval\\nand self-reflection. Our framework trains a single arbitrary LM that adaptively\\nretrieves passages on-demand, and generates and reflects on retrieved passages\\nand its own generations using special tokens, called reflection tokens. Generating\\nreflection tokens makes the LM controllable during the inference phase, enabling it\\nto tailor its behavior to diverse task requirements. Experiments show that SELF-\\nRAG(7B and 13B parameters) significantly outperforms state-of-the-art LLMs\\nand retrieval-augmented models on a diverse set of tasks. Specifically, SELF-RAG\\noutperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\\nreasoning and fact verification tasks, and it shows significant gains in improving\\nfactuality and citation accuracy for long-form generations relative to these models.1\\n1 I NTRODUCTION\\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)Document 2:::\\ncalls a retriever model on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple\\nretrieved passages, evaluating their relevance and then generating corresponding task outputs (Step\\n2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms\\nof factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which\\n1Our code and trained models are available at https://selfrag.github.io/ .\\n1arXiv:2310.11511v1  [cs.CL]  17 Oct 2023Preprint.\\nStep 1: Retrieve K documentsCalifornia was named after a ï¬ctional island in a Spanish book. Prompt How did US states get their names? \\nUS states got their names from a variety of sources. Eleven states are named after an individual person (e.g, California was named after Christopher Columbus). Some states including Texas and Utah, are named after Native American tribe.\\nRetrieval-Augmented Generation (RAG)Ours: Self-reï¬‚ective Retrieval-Augmented Generation (Self-RAG) \\nPopular names by states. In Texas, Emma is a popular baby name. Of the ï¬fty states, eleven are named after an individual person. \\nPrompt How did US states get their names? + Step 2: Prompt LM with K docs and generateRetriever\\nLM\\nPrompt How did US states get their names? US states got their names from a variety of sources. RetrieveStep 1: Retrieve on demand  \\nPrompt +  \\n11 of 50 state namesRelevant\\nStep 2: Generate segment in parallel \\ncome from persons.SupportedIrrelevantTexas is namedafter a Native American tribe. Step 3: Critique outputs and select best segmentorigins in a 16th-century novel Las Sergas de EsplandiÃ¡n. California's name has itsRelevantPartially\\nUS states got their names from a variety of sources. 11 of 50 states names are come from persons.    26 states are named after Native Americans, including Utah. \\nPrompt: Write an essay of your best summer vacation\\nPrompt: Write an essay of your best summer vacationDocument 3:::\\n1 I NTRODUCTION\\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\\ndespite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\\n(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\\nwith relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\\n2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\\nunnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\\nretrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\\nthe output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\\nthe models are not explicitly trained to leverage and follow facts from provided passages. This\\nwork introduces Self-Reflective Retrieval-augmented Generation ( SELF-RAG)to improve an\\nLLMâ€™s generation quality, including its factual accuracy without hurting its versatility, via on-demand\\nretrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\\nits own generation process given a task input by generating both task output and intermittent special\\ntokens (i.e., reflection tokens ). Reflection tokens are categorized into retrieval andcritique tokens to\\nindicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\\ngiven an input prompt and preceding generations, SELF-RAGfirst determines if augmenting the\\ncontinued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that\\ncalls a retriever model on demand (Step 1). Subsequently, SELF-RAGconcurrently processes multiple\\nretrieved passages, evaluating their relevance and then generating corresponding task outputs (StepDocument 4:::\\nPrompt: Write an essay of your best summer vacation\\nPrompt: Write an essay of your best summer vacation\\nNo RetrievalMy best summer vacation is when my family and I embarked on a road trip along â€¦My bestâ€¦ \\n>Repeat.â€¦\\nNo information in passagesContradictory>Prompt +  \\nPrompt +  \\nRetrieve\\nFigure 1: Overview of SELF-RAG.SELF-RAGlearns to retrieve, critique, and generate text passages\\nto enhance overall generation quality, factuality, and verifiability.\\nconsistently retrieves a fixed number of documents for generation regardless of the retrieval necessity\\n(e.g., the bottom figure example does not require factual knowledge) and never second visits the\\ngeneration quality. Moreover, SELF-RAGprovides citations for each segment with its self-assessment\\nof whether the output is supported by the passage, leading to easier fact verification.\\nSELF-RAGtrains an arbitrary LM to generate text with reflection tokens by unifying them as the\\nnext token prediction from the expanded model vocabulary. We train our generator LM on a diverse\\ncollection of text interleaved with reflection tokens and retrieved passages. Reflection tokens, inspired\\nby reward models used in reinforcement learning (Ziegler et al., 2019; Ouyang et al., 2022), are\\ninserted offline into the original corpus by a trained critic model. This eliminates the need to host a\\ncritic model during training, reducing overhead. The critic model, in part, is supervised on a dataset\\nof input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e.,\\nGPT-4; OpenAI 2023). While we draw inspiration from studies that use control tokens to start and\\nguide text generation (Lu et al., 2022; Keskar et al., 2019), our trained LM uses critique tokens to\\nassess its own predictions after each generated segment as an integral part of the generation output.\\nSELF-RAGfurther enables a customizable decoding algorithm to satisfy hard or soft constraints,\\n---\\nNow here is the question you need to answer.\\n\\nQuestion: What is the difference between RAG and self-RAG?\\n</s>\\n<|assistant|>\\nRAG (Retrieval-Augmented Generation) is an ad hoc approach that augments large language models (LLMs) with the retrieval of relevant knowledge to decrease factual errors in knowledge-intensive tasks. However, RAG indiscriminately retrieves and incorporates a fixed number of retrieved passages without ensuring complete support from cited sources, which can hinder LLM versatility or lead to unhelpful response generation. In contrast, Self-Reflective Retrieval-Augmented Generation (SELF-RAG) enhances an LLM's quality and factuality through retrieval and self-reflection. SELF-RAG trains a single arbitrary LLM to adaptively retrieve passages on demand, generate and reflect on retrieved passages and its own generations using special tokens called reflection tokens. SELF-RAG concurrently processes multiple retrieved passages, evaluates their relevance, and generates corresponding task outputs while generating critique tokens to criticize its own output and choose the best one in terms of factuality and overall quality. SELF-RAG also provides citations for each segment with its self-assessment of whether the output is supported by the passage, leading to easier fact verification. In summary, while RAG retrieves passages indiscriminately, SELF-RAG learns to retrieve, critique, and generate text passages to enhance overall generation quality, factuality, and verifiability.\""
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "QZ62CbcZ9jVP"
      },
      "outputs": [],
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "from langchain_core.vectorstores import VectorStore\n",
        "from langchain_core.language_models.llms import LLM\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: LLM,\n",
        "    knowledge_index: VectorStore,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 7,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
        "    # Gather documents with retriever\n",
        "    relevant_docs = knowledge_index.similarity_search(\n",
        "        query=question, k=num_retrieved_docs\n",
        "    )\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
        "\n",
        "    # Optionally rerank results\n",
        "    if reranker:\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_docs_final]\n",
        "\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join(\n",
        "        [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)]\n",
        "    )\n",
        "\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    # Redact an answer\n",
        "    answer = llm(final_prompt)\n",
        "\n",
        "    return answer, relevant_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiygbqfT9jVP"
      },
      "source": [
        "# 3. Benchmarking the RAG system\n",
        "\n",
        "The RAG system and the evaluation datasets are now ready. The last step is to judge the RAG system's output on this evlauation dataset.\n",
        "\n",
        "To this end, __we setup a judge agent__. âš–ï¸ğŸ¤–\n",
        "\n",
        "Out of [the different RAG evaluation metrics](https://docs.ragas.io/en/latest/concepts/metrics/index.html), we choose to focus only on faithfulness since it the best end-to-end metric of our system's performance.\n",
        "\n",
        "> We use GPT4 as a judge for its empirically good performance, but you could try with other models such as [kaist-ai/prometheus-13b-v1.0](https://huggingface.co/kaist-ai/prometheus-13b-v1.0) or [BAAI/JudgeLM-33B-v1.0](https://huggingface.co/BAAI/JudgeLM-33B-v1.0).\n",
        "\n",
        "ğŸ’¡ _In the evaluation prompt, we give a detailed description each metric on the scale 1-5, as is done in [Prometheus's prompt template](https://huggingface.co/kaist-ai/prometheus-13b-v1.0): this helps the model ground its metric precisely. If instead you give the judge LLM a vague scale to work with, the outputs will not be consistent enough between different examples._\n",
        "\n",
        "ğŸ’¡ _Again, prompting the LLM to output rationale before giving its final score gives it more tokens to help it formalize and elaborate a judgement._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'kaist-ai/prometheus-13b-v1.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrlMh_ZI9jVP"
      },
      "outputs": [],
      "source": [
        "def run_rag_tests(\n",
        "    eval_dataset: datasets.Dataset,\n",
        "    llm: BaseChatModel,\n",
        "    knowledge_index: VectorStore,\n",
        "    output_file: str,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    verbose: Optional[bool] = True,\n",
        "    test_settings: Optional[str] = None,  # To document the test settings used\n",
        "):\n",
        "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
        "    try:  # load previous generations if they exist\n",
        "        with open(output_file, \"r\") as f:\n",
        "            outputs = json.load(f)\n",
        "    except:\n",
        "        outputs = []\n",
        "\n",
        "    for example in tqdm(eval_dataset):\n",
        "        question = example[\"question\"]\n",
        "        if question in [output[\"question\"] for output in outputs]:\n",
        "            continue\n",
        "\n",
        "        answer, relevant_docs = answer_with_rag(\n",
        "            question, llm, knowledge_index, reranker=reranker\n",
        "        )\n",
        "        if verbose:\n",
        "            print(\"=======================================================\")\n",
        "            print(f\"Question: {question}\")\n",
        "            print(f\"Answer: {answer}\")\n",
        "            print(f'True answer: {example[\"answer\"]}')\n",
        "        result = {\n",
        "            \"question\": question,\n",
        "            \"true_answer\": example[\"answer\"],\n",
        "            \"source_doc\": example[\"source_doc\"],\n",
        "            \"generated_answer\": answer,\n",
        "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
        "        }\n",
        "        if test_settings:\n",
        "            result[\"test_settings\"] = test_settings\n",
        "        outputs.append(result)\n",
        "\n",
        "        with open(output_file, \"w\") as f:\n",
        "            json.dump(outputs, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ae-3KWzK9jVP"
      },
      "outputs": [],
      "source": [
        "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
        "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
        "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
        "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
        "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
        "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
        "\n",
        "###The instruction to evaluate:\n",
        "{instruction}\n",
        "\n",
        "###Response to evaluate:\n",
        "{response}\n",
        "\n",
        "###Reference Answer (Score 5):\n",
        "{reference_answer}\n",
        "\n",
        "###Score Rubrics:\n",
        "[Is the response correct, accurate, and factual based on the reference answer?]\n",
        "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
        "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
        "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
        "Score 4: The response is mostly correct, accurate, and factual.\n",
        "Score 5: The response is completely correct, accurate, and factual.\n",
        "\n",
        "###Feedback:\"\"\"\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import SystemMessage\n",
        "\n",
        "\n",
        "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
        "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "ia9Mvn859jVP"
      },
      "outputs": [],
      "source": [
        "#from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "#eval_chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
        "eval_chat_repo = 'kaist-ai/prometheus-13b-v1.0'\n",
        "evaluator_name = \"PROMETHEUS-13B\"\n",
        "\n",
        "repo_id = eval_chat_repo\n",
        "READER_MODEL_NAME = \"kaist-ai/prometheus-13b-v1.0\"\n",
        "\n",
        "eval_chat_model = HuggingFaceHub(\n",
        "    repo_id=repo_id,\n",
        "    task=\"text-generation\",\n",
        "    model_kwargs={\n",
        "        \"max_new_tokens\": 512,\n",
        "        \"top_k\": 30,\n",
        "        \"temperature\": 0.1,\n",
        "        \"repetition_penalty\": 1.03,\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_answers(\n",
        "    answer_path: str,\n",
        "    eval_chat_model,#BaseChatModel,\n",
        "    evaluator_name: str,\n",
        "    evaluation_prompt_template #: ChatPromptTemplate,\n",
        ") -> None:\n",
        "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
        "    answers = []\n",
        "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
        "        answers = json.load(open(answer_path, \"r\"))\n",
        "\n",
        "    for experiment in tqdm(answers):\n",
        "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
        "            continue\n",
        "\n",
        "        eval_prompt = evaluation_prompt_template.format_messages(\n",
        "            instruction=experiment[\"question\"],\n",
        "            response=experiment[\"generated_answer\"],\n",
        "            reference_answer=experiment[\"true_answer\"],\n",
        "        )\n",
        "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
        "        feedback, score = [\n",
        "            item.strip() for item in eval_result.content.split(\"[RESULT]\")\n",
        "        ]\n",
        "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
        "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
        "\n",
        "        with open(answer_path, \"w\") as f:\n",
        "            json.dump(answers, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXH-szLe9jVP"
      },
      "source": [
        "ğŸš€ Let's run the tests and evaluate answers!ğŸ‘‡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW2nnvUT9jVQ"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"./output\"):\n",
        "    os.mkdir(\"./output\")\n",
        "\n",
        "for chunk_size in [200]:  # Add other chunk sizes (in tokens) as needed\n",
        "    for embeddings in [\"thenlper/gte-small\"]:  # Add other embeddings as needed\n",
        "        for rerank in [True, False]:\n",
        "            settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n",
        "            output_file_name = f\"./output/rag_{settings_name}.json\"\n",
        "\n",
        "            print(f\"Running evaluation for {settings_name}:\")\n",
        "\n",
        "            print(\"Loading knowledge base embeddings...\")\n",
        "            knowledge_index = load_embeddings(\n",
        "                RAW_KNOWLEDGE_BASE,\n",
        "                chunk_size=chunk_size,\n",
        "                embedding_model_name=embeddings,\n",
        "            )\n",
        "\n",
        "            print(\"Running RAG...\")\n",
        "            reranker = (\n",
        "                RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "                if rerank\n",
        "                else None\n",
        "            )\n",
        "            run_rag_tests(\n",
        "                eval_dataset=eval_dataset,\n",
        "                llm=READER_LLM,\n",
        "                knowledge_index=knowledge_index,\n",
        "                output_file=output_file_name,\n",
        "                reranker=reranker,\n",
        "                verbose=False,\n",
        "                test_settings=settings_name,\n",
        "            )\n",
        "\n",
        "            print(\"Running evaluation...\")\n",
        "            evaluate_answers(\n",
        "                output_file_name,\n",
        "                eval_chat_model,\n",
        "                evaluator_name,\n",
        "                evaluation_prompt_template,\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tytXV5-h9jVT"
      },
      "source": [
        "### Inspect results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4YDSfmr9jVT"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "outputs = []\n",
        "for file in glob.glob(\"./output/*.json\"):\n",
        "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
        "    output[\"settings\"] = file\n",
        "    outputs.append(output)\n",
        "result = pd.concat(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdkXMNvS9jVT"
      },
      "outputs": [],
      "source": [
        "result[\"eval_score_GPT4\"] = result[\"eval_score_GPT4\"].apply(\n",
        "    lambda x: int(x) if isinstance(x, str) else 1\n",
        ")\n",
        "result[\"eval_score_GPT4\"] = (result[\"eval_score_GPT4\"] - 1) / 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgxBpid29jVT",
        "outputId": "9a3bcf32-4b0c-4df1-c76c-3ebbca82929d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "settings\n",
              "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta.json       0.884328\n",
              "./output/rag_chunk:200_embeddings:BAAI~bge-base-en-v1.5_rerank:False_reader-model:zephyr-7b-beta.json    0.906716\n",
              "./output/rag_chunk:200_embeddings:BAAI~bge-base-en-v1.5_rerank:True_reader-model:zephyr-7b-beta.json     0.906716\n",
              "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mixtral.json               0.906716\n",
              "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:zephyr-7b-beta.json        0.921642\n",
              "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mixtral0.json              0.947761\n",
              "Name: eval_score_GPT4, dtype: float64"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "average_scores = result.groupby(\"settings\")[\"eval_score_GPT4\"].mean()\n",
        "average_scores.sort_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSPH9DYI9jVT"
      },
      "source": [
        "## Example results\n",
        "\n",
        "Let us load the results that I obtained by tweaking the different options available in this notebook.\n",
        "For more detail on why these options could work on not, see the notebook on [advanced_RAG](advanced_rag).\n",
        "\n",
        "As you can see in the graph below, some tweaks do not bring any improvement, some give huge performance boosts.\n",
        "\n",
        "â¡ï¸ ___There is no single good recipe: you should try several different directions when tuning your RAG systems.___\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVOxatv99jVT"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "scores = datasets.load_dataset(\"m-ric/rag_scores_cookbook\", split=\"train\")\n",
        "scores = pd.Series(scores[\"score\"], index=scores[\"settings\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqK0Dg2Q9jVT"
      },
      "outputs": [],
      "source": [
        "fig = px.bar(\n",
        "    scores,\n",
        "    color=scores,\n",
        "    labels={\n",
        "        \"value\": \"Accuracy\",\n",
        "        \"settings\": \"Configuration\",\n",
        "    },\n",
        "    color_continuous_scale=\"bluered\",\n",
        ")\n",
        "fig.update_layout(w\n",
        "    width=1000,\n",
        "    height=600,\n",
        "    barmode=\"group\",\n",
        "    yaxis_range=[0, 100],\n",
        "    title=\"<b>Accuracy of different RAG configurations</b>\",\n",
        "    xaxis_title=\"RAG settings\",\n",
        "    font=dict(size=15),\n",
        ")\n",
        "fig.layout.yaxis.ticksuffix = \"%\"\n",
        "fig.update_coloraxes(showscale=False)\n",
        "fig.update_traces(texttemplate=\"%{y:.1f}\", textposition=\"outside\")\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
